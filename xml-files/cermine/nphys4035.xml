<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <article-id pub-id-type="doi">10.1038/NPHYS4035</article-id>
      <title-group>
        <article-title>Machine learning phases of matter</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Juan Carrasquilla</string-name>
          <email>jcarrasquilla@perimeterinstitute.ca</email>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Roger G. Melko</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Department of Physics and Astronomy, University of Waterloo</institution>
          ,
          <addr-line>Ontario N2L 3G1</addr-line>
          ,
          <country country="CA">Canada</country>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>Perimeter Institute for Theoretical Physics</institution>
          ,
          <addr-line>Waterloo, Ontario N2L 2Y5</addr-line>
          ,
          <country country="CA">Canada</country>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2017</year>
      </pub-date>
      <volume>13</volume>
      <fpage>431</fpage>
      <lpage>434</lpage>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>-</title>
      <p>Condensed-matter physics is the study of the collective
behaviour of infinitely complex assemblies of electrons, nuclei,
magnetic moments, atoms or qubits1. This complexity is
reflected in the size of the state space, which grows
exponentially with the number of particles, reminiscent of the
‘curse of dimensionality’ commonly encountered in machine
learning2. Despite this curse, the machine learning community
has developed techniques with remarkable abilities to
recognize, classify, and characterize complex sets of data. Here, we
show that modern machine learning architectures, such as fully
connected and convolutional neural networks3, can identify
phases and phase transitions in a variety of condensed-matter
Hamiltonians. Readily programmable through modern
software libraries4,5, neural networks can be trained to detect
multiple types of order parameter, as well as highly non-trivial states
with no conventional order, directly from raw state
configurations sampled with Monte Carlo6,7.</p>
      <p>Conventionally, the study of phases in condensed-matter systems
is performed with the help of tools that have been carefully designed
to elucidate the underlying physical structures of various states.
Among the most powerful are Monte Carlo simulations, which
consist of two steps: a stochastic importance sampling over state
space, and the evaluation of estimators for physical quantities
calculated from these samples7. These estimators are constructed
on the basis of a variety of physical motivations; for example, the
availability of an experimental measure such as a specific heat; or,
the encoding of a theoretical device such as an order parameter1.
However, some technologically important states of matter, such
as topologically ordered states1,8, can not be straightforwardly
identified with standard estimators9,10.</p>
      <p>Machine learning, already explored as a tool in
condensedmatter research11 16, provides a complementary paradigm to the
above approach. The ability of modern machine learning techniques
to classify, identify, or interpret massive data sets such as images
foreshadows their suitability to provide physicists with similar
analyses on the exponentially large data sets embodied in the state
space of condensed-matter systems. We first demonstrate this on
the prototypical example of the square-lattice ferromagnetic Ising
mizoDdel, 1HsoD thaJtPfohriji Niz lajzt.ticWeesisteets, JthDe1staanted stphaeceIsiisngofvsairzieab2leNs.
Standard Monte Carlo techniques provide samples of configurations
for any temperature T , weighted by the Boltzmann distribution.
The existence of a well-understood phase transition at temperature
Tc (ref. 17), between a high-temperature paramagnetic phase and
a low-temperature ferromagnetic phase, allows us the opportunity
to attempt to classify the two di erent types of configurations
without the use of Monte Carlo estimators. Instead, we construct
a fully connected feed-forward neural network, implemented
with TensorFlow4, to perform supervised learning directly on the
thermalized and uncorrelated raw configurations sampled by a
Monte Carlo simulation. As illustrated in Fig. 1a, the neural network
is composed of an input layer with values determined by the spin
configurations, a 100-unit hidden layer of sigmoid neurons, and
an analogous output layer. When trained on a broad range of data
at temperatures above and below Tc, the neural network is able
to correctly classify data in a test set. Finite-size scaling is capable
of systematically narrowing in on the thermodynamic value of Tc
in a way analogous to measurements of the magnetization: a data
collapse of the output layer (Fig. 1b) leads to an estimate of the
critical exponent ' 1.0 0.2, while a size scaling of the crossing
temperature T =J estimates Tc=J ' 2.266 0.002 (Fig. 1c). One
can understand the training of the network through a simple toy
model involving a hidden layer of only three analytically `trained'
perceptrons, representing the possible combinations of high- and
low-temperature magnetic states exclusively on the basis of their
magnetization. Similarly, our 100-unit neural network relies on the
magnetization of the configurations in the classification task. Details
about the toy model, the 100-unit neural network, as well as a
lowdimensional visualization of the training data, which may be used
as a preprocessing step to generate the labels if they are not available
a priori, are discussed in the Supplementary Figs 1, 2, and 4. We
note that in a recent development, a closely related
neural-networkbased approach allows for the determination of critical points using
a confusion scheme18, which works even in the absence of labels.
Finally, we mention that similar success rates occur if the model
is modified to have antiferromagnetic couplings, H D J Phiji iz jz ,
illustrating that the neural network is not only useful in identifying a
global spin polarization, but an order parameter with other ordering
wavevectors (here q D . , /).</p>
      <p>The power of neural networks lies in their ability to generalize
to tasks beyond their original design. For example, what if one
was presented with a data set of configurations from an Ising
Hamiltonian where the lattice structure (and therefore its Tc)
is not known? We illustrate this scenario by taking our above
feed-forward neural network, already trained on configurations
for the square-lattice ferromagnetic Ising model, and provide it
a test set produced by Monte Carlo simulations of the triangular
lattice ferromagnetic Ising Hamiltonian. In Fig. 1d f we present the
averaged output layer versus T , the corresponding data collapse,
and a size scaling of T =J , allowing us to successfully estimate the
critical parameters Tc=J D 3.65 0.01 and ' 1.0 0.3 consistent
with the exact values19.</p>
      <p>We now turn to the application of such techniques to problems
of greater interest in modern condensed matter, such as disordered
or topological phases, where no conventional order parameter
exists. Coulomb phases, for example, are states of frustrated
lattice models where local energetic constraints imposed by the
Hamiltonian lead to extensively degenerate classical ground states.
We consider a two-dimensional square-ice Hamiltonian given by
H D J Pv Qv2, where the charge Qv D Pi2v iz is the sum over the
Ising variables located in the lattice bonds incident on vertex v,
as shown in Fig. 2. In a conventional approach, the ground states
a
d
1.0
0.8
and the high-temperature states are distinguished by their spin spin
correlation functions: power-law decay at T D 0, and exponential
decay at T D 1. Instead we feed raw Monte Carlo configurations
to train a neural network (Fig. 1a) to distinguish ground states
from high-temperature states (Fig. 2a,b). For a square-ice system
with N D 2 16 16 spins, we find that a neural network with
100 hidden units successfully distinguishes the states with a 99%
accuracy. The network does so solely based on spin configurations,
with no information about the underlying lattice a feat di cult for
the human eye, even if supplemented with a layout of the underlying
Hamiltonian locality.</p>
      <p>We now examine an Ising lattice gauge theory, the prototypical
example of a topological phase of matter, without an order
parameter at T D 0 (refs 8,20). The Hamiltonian is H D J Pp Qi2p iz ,
where the Ising spins live on the bonds of a two-dimensional square
lattice with plaquettes p (see Fig. 2c). The ground state is again a
degenerate manifold8,21 with exponentially decaying spin spin
correlations. As in the square-ice model, we attempt to use the neural
network in Fig. 1a to classify the high- and low-temperature states,
but find that the training fails to classify the test sets to an accuracy
of over 50% equivalent to simply guessing. Instead, we employ a
convolutional neural network (CNN)3,22 which readily takes
advantage of the two-dimensional structure as well as the translational
invariance of the model. We optimize the CNN in Fig. 2d using
Monte Carlo configurations from the Ising gauge theory at T D 0
and T D 1. The CNN discriminates high-temperature from ground
states with an accuracy of 100% in spite of the lack of an order
parameter or qualitative di erences in the spin spin correlations.
432
We find that the discriminative power of the CNN relies on the
detection of satisfied local energetic constraints of the theory, namely
whether Qi2p iz is either C1 (satisfied) or 1 (unsatisfied) on
each plaquette of the system (see the Supplementary Fig. 5). We
construct an analytical model to explicitly exploit the presence of
local constraints in the classification task, which discriminates our
test sets with an accuracy of 100% (see Supplementary Fig. 6).</p>
      <p>Notice that, because there is no finite-temperature phase
transition in the Ising gauge theory, we have restricted our analysis
to temperatures T D 0 and T D 1, only. However, in finite systems,
violations of the local constraints are strongly suppressed, and the
system is expected to slowly cross over to the high-temperature
phase. The crossover temperature T happens as the number of
itmheprlmyianlglyTex=cJited1=dlenfepctNs (reNf. 2ex3p)..As2tJhe/pirsesoefntcheeoofrlodcearl odfefoenctes,
is the mechanism through which the CNN decides whether a
system is in its ground state or not, we expect that it will be
able to detect the crossover temperature in a test set at small but
finite temperatures. In Fig. 3 we present the results of the output
neurons of our analytical model for di erent system sizes averaged
over test sets at di erent temperatures. We estimate the inverse
crossover temperature J based on the crossing point of the
lowand high-temperature output neurons. As expected theoretically,
this depends on the system size, and as shown in the inset in Fig. 3,
a clear logarithmic crossover is apparent. This result showcases the
ability of the CNN to detect not only phase transitions, but also
non-trivial crossovers between topological phases and their
hightemperature counterparts.</p>
    </sec>
    <sec id="sec-2">
      <title>High-temperature state</title>
      <p>b</p>
    </sec>
    <sec id="sec-3">
      <title>Ising square-ice ground state</title>
      <p>c</p>
    </sec>
    <sec id="sec-4">
      <title>Ising lattice gauge theory</title>
      <p>v</p>
    </sec>
    <sec id="sec-5">
      <title>Fully connected layer (64)</title>
      <p>p
Softmax
a
0.8
re 0.6
y
a
l
t
u
p
t
uO0.4
0.2
0.0
−1
3.5
3.0
2.5
∗J
β 2.0
1.5
1.0100
2×2 maps
(64 per sublattice)</p>
    </sec>
    <sec id="sec-6">
      <title>Dropout regularization</title>
      <p>A final implementation of our approach on a system of
noninteracting spinless fermions subject to a quasi-periodic
potential24 demonstrates that neural networks can distinguish metallic
from Anderson localized phases, and can be used to study
the localization transition between them (see the Supplementary
Figs 3 and 4).</p>
      <p>We have shown that neural network technology, developed
for applications such as computer vision and natural language
processing, can be used to encode phases of matter and discriminate
phase transitions in correlated many-body systems. In particular,
we have argued that neural networks encode information about
conventional ordered phases by learning the order parameter of the
phase, without knowledge of the energy or locality conditions of
the Hamiltonian. Furthermore, we have shown that neural networks
can encode basic information about unconventional phases such
as the ones present in the square-ice model and the Ising lattice
gauge theory, as well as Anderson localized phases. These results
indicate that neural networks have the potential to represent ground
state wavefunctions. For instance, ground states of the toric code1,8
can be represented by convolutional neural networks akin to the
one in Fig. 2d (see Supplementary Fig. 6 and Supplementary
Table 1). We thus anticipate their use in the field of quantum
technology25, such as quantum error correction protocols26, and
quantum state tomography27. As in all other areas of `big data',
we are already witnessing the rapid adoption of machine learning
techniques as a basic research tool in condensed matter and
statistical physics.</p>
      <p>Data availability. The data that support the plots within this paper
and other findings of this study are available from the corresponding
author upon request.</p>
      <p>Received 27 June 2016; accepted 11 January 2017;
published online 13 February 2017</p>
      <sec id="sec-6-1">
        <title>Acknowledgements</title>
        <p>We would like to thank G. Baskaran, C. Castelnovo, A. Chandran, L. E. Hayward Sierens,
B. Kulchytskyy, D. Schwab, M. Stoudenmire, G. Torlai, G. Vidal and Y. Wan for
discussions and encouragement. We thank A. Del Maestro for a careful reading of the
manuscript. This research was supported by NSERC of Canada, the Perimeter Institute
for Theoretical Physics, the John Templeton Foundation, and the Shared Hierarchical
Academic Research Computing Network (SHARCNET). R.G.M. acknowledges support
from a Canada Research Chair. Research at Perimeter Institute is supported through
Industry Canada and by the Province of Ontario through the Ministry of Research
&amp; Innovation.</p>
      </sec>
      <sec id="sec-6-2">
        <title>Author contributions</title>
        <sec id="sec-6-2-1">
          <title>All authors contributed significantly to this work.</title>
        </sec>
      </sec>
      <sec id="sec-6-3">
        <title>Additional information</title>
        <p>Supplementary information is available in the online version of the paper. Reprints and
permissions information is available online at www.nature.com/reprints.
Correspondence and requests for materials should be addressed to J.C.</p>
      </sec>
      <sec id="sec-6-4">
        <title>Competing financial interests</title>
        <sec id="sec-6-4-1">
          <title>The authors declare no competing financial interests.</title>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          1.
          <string-name>
            <surname>Wen</surname>
            ,
            <given-names>X.</given-names>
          </string-name>
          <article-title>Quantum Field Theory of Many-Body Systems: From the Origin of Sound to an Origin of Light and Electrons (Oxford Graduate Texts</article-title>
          , OUP Oxford,
          <year>2004</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          2.
          <string-name>
            <surname>Bellman</surname>
            ,
            <given-names>R.</given-names>
          </string-name>
          <article-title>Dynamic Programming 1st edn</article-title>
          (Princeton Univ. Press,
          <year>1957</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          3.
          <string-name>
            <surname>Goodfellow</surname>
            ,
            <given-names>I.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bengio</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Courville</surname>
            ,
            <given-names>A. Deep Learning</given-names>
          </string-name>
          (MIT Press,
          <year>2016</year>
          ); http://www.deeplearningbook.org
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          4.
          <string-name>
            <surname>Abadi</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          et al.
          <source>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</source>
          (
          <year>2015</year>
          ); http://tensorflow.org
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          5.
          <string-name>
            <surname>Bergstra</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          et al.
          <article-title>Theano: a CPU and GPU math expression compiler</article-title>
          .
          <source>Proc. Python Sci. Comput. Conf. (SciPy)</source>
          (
          <year>2010</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          6.
          <string-name>
            <surname>Avella</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Mancini</surname>
            ,
            <given-names>F.</given-names>
          </string-name>
          <string-name>
            <surname>Strongly Correlated</surname>
            <given-names>Systems</given-names>
          </string-name>
          : Numerical Methods (Springer Series in Solid-State
          <string-name>
            <surname>Sciences</surname>
          </string-name>
          ,
          <year>2013</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          7.
          <string-name>
            <surname>Sandvik</surname>
            ,
            <given-names>A. W.</given-names>
          </string-name>
          <article-title>Computational studies of quantum spin systems</article-title>
          .
          <source>AIP Conf. Proc. 1297</source>
          ,
          <fpage>135</fpage>
          <lpage>338</lpage>
          (
          <year>2010</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          8.
          <string-name>
            <surname>Kitaev</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          <article-title>Fault-tolerant quantum computation by anyons</article-title>
          .
          <source>Ann. Phys. 303</source>
          ,
          <fpage>2</fpage>
          <lpage>30</lpage>
          (
          <year>2003</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          9.
          <string-name>
            <surname>Levin</surname>
            ,
            <given-names>M.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Wen</surname>
            ,
            <given-names>X.-G.</given-names>
          </string-name>
          <article-title>Detecting topological order in a ground state wave function</article-title>
          .
          <source>Phys. Rev. Lett</source>
          .
          <volume>96</volume>
          ,
          <issue>110405</issue>
          (
          <year>2006</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          10.
          <string-name>
            <surname>Kitaev</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Preskill</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          <article-title>Topological entanglement entropy</article-title>
          .
          <source>Phys. Rev. Lett</source>
          .
          <volume>96</volume>
          ,
          <issue>110404</issue>
          (
          <year>2006</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          11.
          <string-name>
            <surname>Arsenault</surname>
            ,
            <given-names>L.-F.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Lopez-Bezanilla</surname>
            ,
            <given-names>A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>von Lilienfeld</surname>
            ,
            <given-names>O. A.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Millis</surname>
            ,
            <given-names>A. J.</given-names>
          </string-name>
          <article-title>Machine learning for many-body physics: the case of the Anderson impurity model</article-title>
          .
          <source>Phys. Rev. B</source>
          <volume>90</volume>
          ,
          <issue>155136</issue>
          (
          <year>2014</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          12.
          <string-name>
            <surname>Kusne</surname>
            ,
            <given-names>A. G.</given-names>
          </string-name>
          et al.
          <article-title>On-the-fly machine-learning for high-throughput experiments: search for rare-earth-free permanent magnets</article-title>
          .
          <source>Sci. Rep</source>
          .
          <volume>4</volume>
          ,
          <issue>6367</issue>
          (
          <year>2014</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          13.
          <string-name>
            <surname>Kalinin</surname>
            ,
            <given-names>S. V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sumpter</surname>
            ,
            <given-names>B. G.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Archibald</surname>
            ,
            <given-names>R. K.</given-names>
          </string-name>
          <article-title>Big-deep-smart data in imaging for guiding materials design</article-title>
          .
          <source>Nat. Mater</source>
          .
          <volume>14</volume>
          ,
          <fpage>973</fpage>
          <lpage>980</lpage>
          (
          <year>2015</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          14.
          <string-name>
            <surname>Ghiringhelli</surname>
            ,
            <given-names>L. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Vybiral</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Levchenko</surname>
            ,
            <given-names>S. V.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Draxl</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Sche er</surname>
          </string-name>
          , M.
          <article-title>Big data of materials science: critical role of the descriptor</article-title>
          .
          <source>Phys. Rev. Lett</source>
          .
          <volume>114</volume>
          ,
          <issue>105503</issue>
          (
          <year>2015</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          15.
          <string-name>
            <surname>Schoenholz</surname>
            ,
            <given-names>S. S.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Cubuk</surname>
            ,
            <given-names>E. D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Sussman</surname>
            ,
            <given-names>D. M.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kaxiras</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>A. J.</given-names>
          </string-name>
          <article-title>A structural approach to relaxation in glassy liquids</article-title>
          .
          <source>Nat. Phys</source>
          .
          <volume>12</volume>
          ,
          <fpage>469</fpage>
          <lpage>471</lpage>
          (
          <year>2016</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          16.
          <string-name>
            <surname>Mehta</surname>
            ,
            <given-names>P.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Schwab</surname>
            ,
            <given-names>D. J.</given-names>
          </string-name>
          <article-title>An exact mapping between the variational renormalization group and deep learning</article-title>
          . Preprint at http://arXiv.org/abs/1410.3831 (
          <year>2014</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          17.
          <string-name>
            <surname>Onsager</surname>
            ,
            <given-names>L. Crystal statistics. I.</given-names>
          </string-name>
          <article-title>A two-dimensional model with an order-disorder transition</article-title>
          .
          <source>Phys. Rev</source>
          .
          <volume>65</volume>
          ,
          <fpage>117</fpage>
          <lpage>149</lpage>
          (
          <year>1944</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref18">
        <mixed-citation>
          18.
          <string-name>
            <surname>van Nieuwenburg</surname>
            ,
            <given-names>E. P. L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Liu</surname>
            ,
            <given-names>Y.-H.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Huber</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          <article-title>Learning phase transitions by confusion</article-title>
          .
          <source>Nat. Phys</source>
          . http://dx.doi.org/10.1038/nphys4037 (
          <year>2017</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref19">
        <mixed-citation>
          19.
          <string-name>
            <surname>Newell</surname>
            ,
            <given-names>G. F.</given-names>
          </string-name>
          <article-title>Crystal statistics of a two-dimensional triangular Ising lattice</article-title>
          .
          <source>Phys. Rev</source>
          .
          <volume>79</volume>
          ,
          <fpage>876</fpage>
          <lpage>882</lpage>
          (
          <year>1950</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref20">
        <mixed-citation>
          20.
          <string-name>
            <surname>Kogut</surname>
            ,
            <given-names>J. B.</given-names>
          </string-name>
          <article-title>An introduction to lattice gauge theory and spin systems</article-title>
          .
          <source>Rev. Mod. Phys</source>
          .
          <volume>51</volume>
          ,
          <fpage>659</fpage>
          <lpage>713</lpage>
          (
          <year>1979</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref21">
        <mixed-citation>
          21.
          <string-name>
            <surname>Castelnovo</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Chamon</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          <article-title>Topological order and topological entropy in classical systems</article-title>
          .
          <source>Phys. Rev. B</source>
          <volume>76</volume>
          ,
          <issue>174416</issue>
          (
          <year>2007</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref22">
        <mixed-citation>
          22.
          <string-name>
            <surname>LeCun</surname>
          </string-name>
          , Y.,
          <string-name>
            <surname>Bottou</surname>
            ,
            <given-names>L.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bengio</surname>
            ,
            <given-names>Y.</given-names>
          </string-name>
          &amp;
          <article-title>Ha ner, P. Gradient-based learning applied to document recognition</article-title>
          .
          <source>IEEE Proc. 86</source>
          ,
          <fpage>2278</fpage>
          <lpage>2324</lpage>
          (
          <year>1998</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref23">
        <mixed-citation>
          23.
          <string-name>
            <surname>Castelnovo</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Chamon</surname>
            ,
            <given-names>C.</given-names>
          </string-name>
          <article-title>Entanglement and topological entropy of the toric code at finite temperature</article-title>
          .
          <source>Phys. Rev. B</source>
          <volume>76</volume>
          ,
          <issue>184442</issue>
          (
          <year>2007</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref24">
        <mixed-citation>
          24.
          <string-name>
            <surname>Aubry</surname>
            ,
            <given-names>S.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>André</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          <article-title>Analyticity breaking and anderson localization in incommensurate lattices</article-title>
          .
          <source>Ann. Isr. Phys. Soc</source>
          .
          <volume>3</volume>
          ,
          <issue>133</issue>
          (
          <year>1980</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref25">
        <mixed-citation>
          25.
          <string-name>
            <surname>Amin</surname>
            ,
            <given-names>M. H.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Andriyash</surname>
            ,
            <given-names>E.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Rolfe</surname>
            ,
            <given-names>J.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kulchytskyy</surname>
            ,
            <given-names>B.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Melko</surname>
            ,
            <given-names>R. Quantum</given-names>
          </string-name>
          <article-title>Boltzmann machine</article-title>
          . Preprint at http://arXiv.org/abs/1601.
          <year>02036</year>
          (
          <year>2016</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref26">
        <mixed-citation>
          26.
          <string-name>
            <surname>Torlai</surname>
            ,
            <given-names>G.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Melko</surname>
            ,
            <given-names>R. G.</given-names>
          </string-name>
          <article-title>A neural decoder for topological codes</article-title>
          . Preprint at http://arXiv.org/abs/1610.04238 (
          <year>2016</year>
          ).
        </mixed-citation>
      </ref>
      <ref id="ref27">
        <mixed-citation>
          27.
          <string-name>
            <surname>Landon-Cardinal</surname>
            ,
            <given-names>O.</given-names>
          </string-name>
          &amp;
          <string-name>
            <surname>Poulin</surname>
            ,
            <given-names>D.</given-names>
          </string-name>
          <article-title>Practical learning method for multi-scale entangled states</article-title>
          .
          <source>New J. Phys</source>
          .
          <volume>14</volume>
          ,
          <issue>085004</issue>
          (
          <year>2012</year>
          ).
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>

