Learning Disentangled Representations of Satellite Image Time Series
  In this paper, we investigate how to learn a suitable representation of
satellite image time series in an unsupervised manner by leveraging large
amounts of unlabeled data. Additionally , we aim to disentangle the
representation of time series into two representations: a shared representation
that captures the common information between the images of a time series and an
exclusive representation that contains the specific information of each image
of the time series. To address these issues, we propose a model that combines a
novel component called cross-domain autoencoders with the variational
autoencoder (VAE) and generative ad-versarial network (GAN) methods. In order
to learn disentangled representations of time series, our model learns the
multimodal image-to-image translation task. We train our model using satellite
image time series from the Sentinel-2 mission. Several experiments are carried
out to evaluate the obtained representations. We show that these disentangled
representations can be very useful to perform multiple tasks such as image
classification, image retrieval, image segmentation and change detection.
