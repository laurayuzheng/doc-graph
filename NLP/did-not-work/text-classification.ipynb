{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'similarity_tfidf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3d72349d5708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/laurazheng/Desktop/NASA Project/doc-graph/extracted_text/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'similarity_tfidf'"
     ]
    }
   ],
   "source": [
    "import glob, random, nltk, similarity_tfidf\n",
    "from nltk.corpus import movie_reviews\n",
    "directory = '/Users/laurazheng/Desktop/NASA Project/doc-graph/extracted_text/'\n",
    "text_files = glob.glob(directory + '*.txt')\n",
    "\n",
    "documents = [list(nltk.word_tokenize(open(t).read())) for t in text_files]\n",
    "random.shuffle(documents)\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(deep inferential spatial-temporal network for forecasting air pollution\\n  concentrations\\n  air pollution poses a serious threat to human health as well as economic\\ndevelopment around the world. to meet the increasing demand for accurate\\npredictions for air pollutions, we proposed a deep inferential spatial-temporal\\nnetwork to deal with the complicated non-linear spatial and temporal\\ncorrelations. we forecast three air pollutants (i.e., pm2.5, pm10 and o3) of\\nmonitoring stations over the next 48 hours, using a hybrid deep learning model\\nconsists of inferential predictor (inference for regions without air pollution\\nreadings), spatial predictor (capturing spatial correlations using cnn) and\\ntemporal predictor (capturing temporal relationship using sequence-to-sequence\\nmodel with simplified attention mechanism). our proposed model considers\\nhistorical air pollution records and historical meteorological data. we\\nevaluate our model on a large-scale dataset containing air pollution records of\\n35 monitoring stations and grid meteorological data in beijing, china. our\\nmodel outperforms other state-of-art methods in terms of smape and rmse.\\n)': False, \"contains(understanding rating behaviour and predicting ratings by identifying\\n  representative users\\n  online user reviews describing various products and services are now abundant\\non the web. while the information conveyed through review texts and ratings is\\neasily comprehensible, there is a wealth of hidden information in them that is\\nnot immediately obvious. in this study, we unlock this hidden value behind user\\nreviews to understand the various dimensions along which users rate products.\\nwe learn a set of users that represent each of these dimensions and use their\\nratings to predict product ratings. specifically, we work with restaurant\\nreviews to identify users whose ratings are influenced by dimensions like\\n'service', 'atmosphere' etc. in order to predict restaurant ratings and\\nunderstand the variation in rating behaviour across different cuisines. while\\nprevious approaches to obtaining product ratings require either a large number\\nof user ratings or a few review texts, we show that it is possible to predict\\nratings with few user ratings and no review text. our experiments show that our\\napproach outperforms other conventional methods by 16-27% in terms of rmse.\\n)\": False, 'contains(characterizing evaporation ducts within the marine atmospheric boundary\\n  layer using artificial neural networks\\n  we apply a multilayer perceptron machine learning (ml) regression approach to\\ninfer electromagnetic (em) duct heights within the marine atmospheric boundary\\nlayer (mabl) using sparsely sampled em propagation data obtained within a\\nbistatic context. this paper explains the rational behind the selection of the\\nml network architecture, along with other model hyperparameters, in an effort\\nto demystify the process of arriving at a useful ml model. the resulting speed\\nof our ml predictions of em duct heights, using sparse data measurements within\\nmabl, indicates the suitability of the proposed method for real-time\\napplications.\\n)': False, 'contains(deepcso: forecasting of combined sewer overflow at a citywide level\\n  using multi-task deep learning\\n  combined sewer overflow (cso) is a major problem to be addressed by many\\ncities. understanding the behavior of sewer system through proper urban\\nhydrological models is an effective method of enhancing sewer system\\nmanagement. conventional deterministic methods, which heavily rely on physical\\nprinciples, is inappropriate for real-time purpose due to their expensive\\ncomputation. on the other hand, data-driven methods have gained huge interests,\\nbut most studies only focus on modeling a single component of the sewer system\\nand supply information at a very abstract level. in this paper, we proposed the\\ndeepcso model, which aims at forecasting cso events from multiple cso\\nstructures simultaneously in near real time at a citywide level. the proposed\\nmodel provided an intermediate methodology that combines the flexibility of\\ndata-driven methods and the rich information contained in deterministic methods\\nwhile avoiding the drawbacks of these two methods. a comparison of the results\\ndemonstrated that the deep learning based multi-task model is superior to the\\ntraditional methods.\\n)': False, 'contains(the use of entropy to measure structural diversity\\n  in this paper entropy based methods are compared and used to measure\\nstructural diversity of an ensemble of 21 classifiers. this measure is mostly\\napplied in ecology, whereby species counts are used as a measure of diversity.\\nthe measures used were shannon entropy, simpsons and the berger parker\\ndiversity indexes. as the diversity indexes increased so did the accuracy of\\nthe ensemble. an ensemble dominated by classifiers with the same structure\\nproduced poor accuracy. uncertainty rule from information theory was also used\\nto further define diversity. genetic algorithms were used to find the optimal\\nensemble by using the diversity indices as the cost function. the method of\\nvoting was used to aggregate the decisions.\\n)': False, 'contains(a learning framework for an accurate prediction of rainfall rates\\n  the present work is aimed to examine the potential of advanced machine\\nlearning strategies to predict the monthly rainfall (precipitation) for the\\nindus basin, using climatological variables such as air temperature,\\ngeo-potential height, relative humidity and elevation. in this work, the focus\\nis on thirteen geographical locations, called index points, within the basin.\\narguably, not all of the hydrological components are relevant to the\\nprecipitation rate, and therefore, need to be filtered out, leading to a\\nlower-dimensional feature space. towards this goal, we adopted the gradient\\nboosting method to extract the most contributive features for precipitation\\nrate prediction. five state-of-the-art machine learning methods have then been\\ntrained where pearson correlation coefficient and mean absolute error have been\\nreported as the prediction performance criteria. the random forest regression\\nmodel outperformed the other regression models achieving the maximum pearson\\ncorrelation coefficient and minimum mean absolute error for most of the index\\npoints. our results suggest the relative humidity (for pressure levels of 300\\nmb and 150 mb, respectively), the u-direction wind (for pressure level of 700\\nmb), air temperature (for pressure levels of 150 mb and 10 mb, respectively) as\\nthe top five influencing features for accurate forecasting the precipitation\\nrate.\\n)': False, 'contains(non-parametric bayesian mixture of sparse regressions with application towards feature selection for statistical downscaling\\n\\nclimate projections simulated by global climate models (gcms) are often used for assessing the impacts of climate change. however, the relatively coarse resolutions of gcm outputs often preclude their application to accurately assessing the effects of climate change on finer regional-scale phenomena. downscaling of climate variables from coarser to finer regional scales using statistical methods is often performed for regional climate projections. statistical downscaling (sd) is based on the understanding that the regional climate is influenced by two factors - the large-scale climatic state and the regional or local features. a transfer function approach of sd involves learning a regression model that relates these features (predictors) to a climatic variable of interest (predictand) based on the past observations. however, often a single regression model is not sufficient to describe complex dynamic relationships between the predictors and predictand. we focus on the covariate selection part of the transfer function approach and propose a nonparametric bayesian mixture of sparse regression models based on dirichlet process (dp) for simultaneous clustering and discovery of covariates within the clusters while automatically finding the number of clusters. sparse linear models are parsimonious and hence more generalizable than non-sparse alternatives, and lend themselves to domain relevant interpretation. applications to synthetic data demonstrate the value of the new approach and preliminary results related to feature selection for statistical downscaling show that our method can lead to new insights.\\n)': False, 'contains(remote sensing image regression for heterogeneous change detection\\n  change detection in heterogeneous multitemporal satellite images is an\\nemerging topic in remote sensing. in this paper we propose a framework, based\\non image regression, to perform change detection in heterogeneous multitemporal\\nsatellite images, which has become a main topic in remote sensing. our method\\nlearns a transformation to map the first image to the domain of the other\\nimage, and vice versa. four regression methods are selected to carry out the\\ntransformation: gaussian processes, support vector machines, random forests,\\nand a recently proposed kernel regression method called homogeneous pixel\\ntransformation. to evaluate not only potentials and limitations of our\\nframework, but also the pros and cons of each regression method, we perform\\nexperiments on two data sets. the results indicates that random forests achieve\\ngood performance, are fast and robust to hyperparameters, whereas the\\nhomogeneous pixel transformation method can achieve better accuracy at the cost\\nof a higher complexity.\\n)': False, 'contains(multi-entity dependence learning with rich context via conditional\\n  variational auto-encoder\\n  multi-entity dependence learning (medl) explores conditional correlations\\namong multiple entities. the availability of rich contextual information\\nrequires a nimble learning scheme that tightly integrates with deep neural\\nnetworks and has the ability to capture correlation structures among\\nexponentially many outcomes. we propose medl_cvae, which encodes a conditional\\nmultivariate distribution as a generating process. as a result, the variational\\nlower bound of the joint likelihood can be optimized via a conditional\\nvariational auto-encoder and trained end-to-end on gpus. our medl_cvae was\\nmotivated by two real-world applications in computational sustainability: one\\nstudies the spatial correlation among multiple bird species using the ebird\\ndata and the other models multi-dimensional landscape composition and human\\nfootprint in the amazon rainforest with satellite images. we show that\\nmedl_cvae captures rich dependency structures, scales better than previous\\nmethods, and further improves on the joint likelihood taking advantage of very\\nlarge datasets that are beyond the capacity of previous methods.\\n)': False, 'contains(a deep learning approach to detecting volcano deformation from satellite\\n  imagery using synthetic datasets\\n  satellites enable widespread, regional or global surveillance of volcanoes\\nand can provide the first indication of volcanic unrest or eruption. here we\\nconsider interferometric synthetic aperture radar (insar), which can be\\nemployed to detect surface deformation with a strong statistical link to\\neruption. the ability of machine learning to automatically identify signals of\\ninterest in these large insar datasets has already been demonstrated, but\\ndata-driven techniques, such as convolutional neutral networks (cnn) require\\nbalanced training datasets of positive and negative signals to effectively\\ndifferentiate between real deformation and noise. as only a small proportion of\\nvolcanoes are deforming and atmospheric noise is ubiquitous, the use of machine\\nlearning for detecting volcanic unrest is more challenging. in this paper, we\\naddress this problem using synthetic interferograms to train the alexnet. the\\nsynthetic interferograms are composed of 3 parts: 1) deformation patterns based\\non a monte carlo selection of parameters for analytic forward models, 2)\\nstratified atmospheric effects derived from weather models and 3) turbulent\\natmospheric effects based on statistical simulations of correlated noise. the\\nalexnet architecture trained with synthetic data outperforms that trained using\\nreal interferograms alone, based on classification accuracy and positive\\npredictive value (ppv). however, the models used to generate the synthetic\\nsignals are a simplification of the natural processes, so we retrain the cnn\\nwith a combined dataset consisting of synthetic models and selected real\\nexamples, achieving a final ppv of 82%. although applying atmospheric\\ncorrections to the entire dataset is computationally expensive, it is\\nrelatively simple to apply them to the small subset of positive results. this\\nfurther improves the detection performance without a significant increase in\\ncomputational burden.\\n)': False, 'contains(deep-learning inversion: a next generation seismic velocity-model\\n  building method\\n  seismic velocity is one of the most important parameters used in seismic\\nexploration. accurate velocity models are key prerequisites for reverse-time\\nmigration and other high-resolution seismic imaging techniques. such velocity\\ninformation has traditionally been derived by tomography or full-waveform\\ninversion (fwi), which are time consuming and computationally expensive, and\\nthey rely heavily on human interaction and quality control. we investigate a\\nnovel method based on the supervised deep fully convolutional neural network\\n(fcn) for velocity-model building (vmb) directly from raw seismograms. unlike\\nthe conventional inversion method based on physical models, the supervised\\ndeep-learning methods are based on big-data training rather than\\nprior-knowledge assumptions. during the training stage, the network establishes\\na nonlinear projection from the multi-shot seismic data to the corresponding\\nvelocity models. during the prediction stage, the trained network can be used\\nto estimate the velocity models from the new input seismic data. one key\\ncharacteristic of the deep-learning method is that it can automatically extract\\nmulti-layer useful features without the need for human-curated activities and\\ninitial velocity setup. the data-driven method usually requires more time\\nduring the training stage, and actual predictions take less time, with only\\nseconds needed. therefore, the computational time of geophysical inversions,\\nincluding real-time inversions, can be dramatically reduced once a good\\ngeneralized network is built. by using numerical experiments on synthetic\\nmodels, the promising performances of our proposed method are shown in\\ncomparison with conventional fwi even when the input data are in more realistic\\nscenarios. discussions on the deep-learning methods, training dataset, lack of\\nlow frequencies, and advantages and disadvantages of the new method are also\\nprovided.\\n)': False, 'contains(short-term solar irradiance and irradiation forecasts via different time\\n  series techniques: a preliminary study\\n  this communication is devoted to solar irradiance and irradiation short-term\\nforecasts, which are useful for electricity production. several different time\\nseries approaches are employed. our results and the corresponding numerical\\nsimulations show that techniques which do not need a large amount of historical\\ndata behave better than those which need them, especially when those data are\\nquite noisy.\\n)': False, \"contains(using machine learning to discern eruption in noisy environments: a case\\n  study using co2-driven cold-water geyser in chimayo, new mexico\\n  we present an approach based on machine learning (ml) to distinguish eruption\\nand precursory signals of chimay\\\\'{o} geyser (new mexico, usa) under noisy\\nenvironments. this geyser can be considered as a natural analog of\\n$\\\\mathrm{co}_2$ intrusion into shallow water aquifers. by studying this geyser,\\nwe can understand upwelling of $\\\\mathrm{co}_2$-rich fluids from depth, which\\nhas relevance to leak monitoring in a $\\\\mathrm{co}_2$ sequestration project. ml\\nmethods such as random forests (rf) are known to be robust multi-class\\nclassifiers and perform well under unfavorable noisy conditions. however, the\\nextent of the rf method's accuracy is poorly understood for this\\n$\\\\mathrm{co}_2$-driven geysering application. the current study aims to\\nquantify the performance of rf-classifiers to discern the geyser state. towards\\nthis goal, we first present the data collected from the seismometer that is\\ninstalled near the chimay\\\\'{o} geyser. the seismic signals collected at this\\nsite contain different types of noises such as daily temperature variations,\\nseasonal trends, animal movement near the geyser, and human activity. first, we\\nfilter the signals from these noises by combining the butterworth-highpass\\nfilter and an autoregressive method in a multi-level fashion. we show that by\\ncombining these filtering techniques, in a hierarchical fashion, leads to\\nreduction in the noise in the seismic data without removing the precursors and\\neruption event signals. we then use rf on the filtered data to classify the\\nstate of geyser into three classes -- remnant noise, precursor, and eruption\\nstates. we show that the classification accuracy using rf on the filtered data\\nis greater than 90\\\\%.these aspects make the proposed ml framework attractive\\nfor event discrimination and signal enhancement under noisy conditions, with\\nstrong potential for application to monitoring leaks in $\\\\mathrm{co}_2$\\nsequestration.\\n)\": False, 'contains(recovering gaps in the gamma-ray logging method\\n  the gamma-ray logging method is one of the mandatory well logging methods for\\ngeophysical exploration of wells. however, during the conduct of such a study,\\nthe sensor, for one reason or another, may stop recording observations in the\\nwell. if a small number of values are missing, you can restore these values\\nusing standard methods to fill in gaps like in time series. if data miss a\\nlarge number of values, observations usually are made again, which leads to\\nadditional financial costs. this work proposes an alternative solution, in the\\nform of filling missed observations in data with the help of machine learning\\nmethods. the main idea of this method is to construct a simple two- layer\\nneural network that is trained on data from the well, and then synthesise the\\nmissing values based on the trained neural network. this work evaluates the\\neffectiveness of the proposed method, and gives reasons for the appropriateness\\nof using different methods of filling gaps, depending on the number of missed\\nvalues.\\n)': False, 'contains(learning the representations of moist convection with convolutional\\n  neural networks\\n  the representations of atmospheric moist convection in general circulation\\nmodels have been one of the most challenging tasks due to its complexity in\\nphysical processes, and the interaction between processes under different\\ntime/spatial scales. this study proposes a new method to predict the effects of\\nmoist convection on the environment using convolutional neural networks. with\\nthe help of considering the gradient of physical fields between adjacent grids\\nin the grey zone resolution, the effects of moist convection predicted by the\\nconvolutional neural networks are more realistic compared to the effects\\npredicted by other machine learning models. the result also suggests that the\\nmethod proposed in this study has the potential to replace the conventional\\ncumulus parameterization in the general circulation models.\\n)': False, 'contains(improving human-machine cooperative visual search with soft highlighting\\n  advances in machine learning have produced systems that attain human-level\\nperformance on certain visual tasks, e.g., object identification. nonetheless,\\nother tasks requiring visual expertise are unlikely to be entrusted to machines\\nfor some time, e.g., satellite and medical imagery analysis. we describe a\\nhuman-machine cooperative approach to visual search, the aim of which is to\\noutperform either human or machine acting alone. the traditional route to\\naugmenting human performance with automatic classifiers is to draw boxes around\\nregions of an image deemed likely to contain a target. human experts typically\\nreject this type of hard highlighting. we propose instead a soft highlighting\\ntechnique in which the saliency of regions of the visual field is modulated in\\na graded fashion based on classifier confidence level. we report on experiments\\nwith both synthetic and natural images showing that soft highlighting achieves\\na performance synergy surpassing that attained by hard highlighting.\\n)': False, 'contains(automated ground truth estimation of vulnerable road users in automotive\\n  radar data using gnss\\n  annotating automotive radar data is a difficult task. this article presents\\nan automated way of acquiring data labels which uses a highly accurate and\\nportable global navigation satellite system (gnss). the proposed system is\\ndiscussed besides a revision of other label acquisitions techniques and a\\nproblem description of manual data annotation. the article concludes with a\\nsystematic comparison of conventional hand labeling and automatic data\\nacquisition. the results show clear advantages of the proposed method without a\\nrelevant loss in labeling accuracy. minor changes can be observed in the\\nmeasured radar data, but the so introduced bias of the gnss reference is\\nclearly outweighed by the indisputable time savings. beside data annotation,\\nthe proposed system can also provide a ground truth for validating object\\ntracking or other automated driving system applications.\\n)': False, 'contains(predicting climate variability over the indian region using data mining strategies\\n\\nin this paper an approach based on expectation maximization (em) clustering to nd the climate regions and a support vector machine to build a predictive model for each of these regions is proposed. to minimize the biases in the estimations a ten cross fold validation is adopted both for obtaining clusters and building the predictive models. the em clustering could identify all the zones as per the koppen classi cation over indian region. the proposed strategy when employed for predicting temperature has resulted in an rmse of 1:19 in the montane climate region and 0:89 in the humid sub tropical region as compared to 2:9 and 0:95 respectively predicted using k-means and linear regression method.\\n)': False, 'contains(spatial patterns of wind speed distributions in switzerland\\n  this paper presents an initial exploration of high frequency records of\\nextreme wind speed in two steps. the first consists in finding the suitable\\nextreme distribution for $120$ measuring stations in switzerland, by comparing\\nthree known distributions: weibull, gamma, and generalized extreme value. this\\ncomparison serves as a basis for the second step which applies a spatial\\nmodelling by using extreme learning machine. the aim is to model distribution\\nparameters by employing a high dimensional input space of topographical\\ninformation. the knowledge of probability distribution gives a comprehensive\\ninformation and a global overview of wind phenomena. through this study, a\\nflexible and a simple modelling approach is presented, which can be generalized\\nto almost extreme environmental data for risk assessment and to model renewable\\nenergy.\\n)': False, 'contains(topic modeling of behavioral modes using sensor data\\n  the field of movement ecology, like so many other fields, is experiencing a\\nperiod of rapid growth in availability of data. as the volume rises,\\ntraditional methods are giving way to machine learning and data science, which\\nare playing an increasingly large part it turning this data into\\nscience-driving insights. one rich and interesting source is the bio-logger.\\nthese small electronic wearable devices are attached to animals free to roam in\\ntheir natural habitats, and report back readings from multiple sensors,\\nincluding gps and accelerometer bursts. a common use of accelerometer data is\\nfor supervised learning of behavioral modes. however, we need unsupervised\\nanalysis tools as well, in order to overcome the inherent difficulties of\\nobtaining a labeled dataset, which in some cases is either infeasible or does\\nnot successfully encompass the full repertoire of behavioral modes of interest.\\nhere we present a matrix factorization based topic-model method for\\naccelerometer bursts, derived using a linear mixture property of patch\\nfeatures. our method is validated via comparison to a labeled dataset, and is\\nfurther compared to standard clustering algorithms.\\n)': False, 'contains(solar flare prediction model with three machine-learning algorithms\\n  using ultraviolet brightening and vector magnetogram\\n  we developed a flare prediction model using machine learning, which is\\noptimized to predict the maximum class of flares occurring in the following 24\\nh. machine learning is used to devise algorithms that can learn from and make\\ndecisions on a huge amount of data. we used solar observation data during the\\nperiod 2010-2015, such as vector magnetogram, ultraviolet (uv) emission, and\\nsoft x-ray emission taken by the solar dynamics observatory and the\\ngeostationary operational environmental satellite. we detected active regions\\nfrom the full-disk magnetogram, from which 60 features were extracted with\\ntheir time differentials, including magnetic neutral lines, the current\\nhelicity, the uv brightening, and the flare history. after standardizing the\\nfeature database, we fully shuffled and randomly separated it into two for\\ntraining and testing. to investigate which algorithm is best for flare\\nprediction, we compared three machine learning algorithms: the support vector\\nmachine (svm), k-nearest neighbors (k-nn), and extremely randomized trees\\n(ert). the prediction score, the true skill statistic (tss), was higher than\\n0.9 with a fully shuffled dataset, which is higher than that for human\\nforecasts. it was found that k-nn has the highest performance among the three\\nalgorithms. the ranking of the feature importance showed that the previous\\nflare activity is most effective, followed by the length of magnetic neutral\\nlines, the unsigned magnetic flux, the area of uv brightening, and the time\\ndifferentials of features over 24 h, all of which are strongly correlated with\\nthe flux emergence dynamics in an active region.\\n)': False, 'contains(trusted multi-party computation and verifiable simulations: a scalable\\n  blockchain approach\\n  large-scale computational experiments, often running over weeks and over\\nlarge datasets, are used extensively in fields such as epidemiology,\\nmeteorology, computational biology, and healthcare to understand phenomena, and\\ndesign high-stakes policies affecting everyday health and economy. for\\ninstance, the openmalaria framework is a computationally-intensive simulation\\nused by various non-governmental and governmental agencies to understand\\nmalarial disease spread and effectiveness of intervention strategies, and\\nsubsequently design healthcare policies. given that such shared results form\\nthe basis of inferences drawn, technological solutions designed, and day-to-day\\npolicies drafted, it is essential that the computations are validated and\\ntrusted. in particular, in a multi-agent environment involving several\\nindependent computing agents, a notion of trust in results generated by peers\\nis critical in facilitating transparency, accountability, and collaboration.\\nusing a novel combination of distributed validation of atomic computation\\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\\nuniversal framework for distributed trust in computations. in particular we\\naddress the scalaibility problem by reducing the storage and communication\\ncosts using a lossy compression scheme. this framework guarantees not only\\nverifiability of final results, but also the validity of local computations,\\nand its cost-benefit tradeoffs are studied using a synthetic example of\\ntraining a neural network.\\n)': False, 'contains(content based image retrieval from awifs images repository of irs\\n  resourcesat-2 satellite based on water bodies and burnt areas\\n  satellite remote sensing technology is becoming a major milestone in the\\nprediction of weather anomalies, natural disasters as well as finding\\nalternative resources in proximity using multiple multi-spectral sensors\\nemitting electromagnetic waves at distinct wavelengths. hence, it is imperative\\nto extract water bodies and burnt areas from orthorectified tiles and\\ncorrespondingly rank them using similarity measures. different objects in all\\nthe spheres of the earth have the inherent capability of absorbing\\nelectromagnetic waves of distant wavelengths. this creates various unique masks\\nin terms of reflectance on the receptor. we propose dynamic semantic\\nsegmentation (dss) algorithms that utilized the mentioned capability to extract\\nand rank advanced wide field sensor (awifs) images according to various\\nfeatures. this system stores data intelligently in the form of a sparse feature\\nvector which drastically mitigates the computational and spatial costs incurred\\nfor further analysis. the compressed source image is divided into chunks and\\nstored in the database for quicker retrieval. this work is intended to utilize\\nreadily available and cost effective resources like awifs dataset instead of\\ndepending on advanced technologies like moderate resolution imaging\\nspectroradiometer (modis) for data which is scarce.\\n)': False, 'contains(semantic segmentation for urban planning maps based on u-net\\n  the automatic digitizing of paper maps is a significant and challenging task\\nfor both academia and industry. as an important procedure of map digitizing,\\nthe semantic segmentation section mainly relies on manual visual interpretation\\nwith low efficiency. in this study, we select urban planning maps as a\\nrepresentative sample and investigate the feasibility of utilizing u-shape\\nfully convolutional based architecture to perform end-to-end map semantic\\nsegmentation. the experimental results obtained from the test area in shibuya\\ndistrict, tokyo, demonstrate that our proposed method could achieve a very high\\njaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. for\\nimplementation on gpgpu and cudnn, the required processing time for the whole\\nshibuya district can be less than three minutes. the results indicate the\\nproposed method can serve as a viable tool for urban planning map semantic\\nsegmentation task with high accuracy and efficiency.\\n)': False, 'contains(disaggregation of remotely sensed soil moisture in heterogeneous\\n  landscapes using holistic structure based models\\n  in this study, a novel machine learning algorithm is presented for\\ndisaggregation of satellite soil moisture (sm) based on self-regularized\\nregressive models (srrm) using high-resolution correlated information from\\nauxiliary sources. it includes regularized clustering that assigns soft\\nmemberships to each pixel at fine-scale followed by a kernel regression that\\ncomputes the value of the desired variable at all pixels. coarse-scale remotely\\nsensed sm were disaggregated from 10km to 1km using land cover, precipitation,\\nland surface temperature, leaf area index, and in-situ observations of sm. this\\nalgorithm was evaluated using multi-scale synthetic observations in nc florida\\nfor heterogeneous agricultural land covers. it was found that the root mean\\nsquare error (rmse) for 96% of the pixels was less than 0.02 $m^3/m^3$. the\\nclusters generated represented the data well and reduced the rmse by upto 40%\\nduring periods of high heterogeneity in land-cover and meteorological\\nconditions. the kullback leibler divergence (kld) between the true sm and the\\ndisaggregated estimates is close to 0, for both vegetated and baresoil\\nlandcovers. the disaggregated estimates were compared to those generated by the\\nprinciple of relevant information (pri) method. the rmse for the pri\\ndisaggregated estimates is higher than the rmse for the srrm on each day of the\\nseason. the kld of the disaggregated estimates generated by the srrm is at\\nleast four orders of magnitude lower than those for the pri disaggregated\\nestimates, while the computational time needed was reduced by three times. the\\nresults indicate that the srrm can be used for disaggregating sm with complex\\nnon-linear correlations on a grid with high accuracy.\\n)': False, 'contains(using network theory and machine learning to predict el ni√±o\\n  the skill of current predictions of the warm phase of the el ni\\\\~no southern\\noscillation (enso) reduces significantly beyond a lag of six months. in this\\npaper, we aim to increase this prediction skill at lags up to one year. the new\\nmethod to do so combines a classical autoregressive integrated moving average\\ntechnique with a modern machine learning approach (through an artificial neural\\nnetwork). the attributes in such a neural network are derived from topological\\nproperties of climate networks and are tested on both a zebiak-cane-type model\\nand observations. for predictions up to six months ahead, the results of the\\nhybrid model give a better skill than the cfsv2 ensemble prediction by the\\nnational centers for environmental prediction (ncep). moreover, results for a\\ntwelve month lead time prediction have a similar skill as the shorter lead time\\npredictions.\\n)': False, \"contains(geochemical discrimination and characteristics of magmatic tectonic\\n  settings; a machine learning-based approach\\n  geochemically discriminating between magmatism in different tectonic settings\\nremains a fundamental part of understanding the processes of magma generation\\nwithin the earth's mantle. here, we present an approach where machine-learning\\n(ml) methods are used for quantitative tectonic discrimination and feature\\nselection using global geochemical datasets containing data for volcanic rocks\\ngenerated in eight different tectonic settings. this study uses support vector\\nmachine, random forest, and sparse multinomial regression (smr) approaches. all\\nthese ml methods with data for 20 elements and 5 isotopic ratios allowed the\\nsuccessful geochemical discrimination between igneous rocks formed in eight\\ndifferent tectonic settings with a discriminant ratio better than 83% for all\\nsettings barring oceanic plateaus and back-arc basins. smr is a particularly\\npowerful and interpretable ml method because it quantitatively identifies\\ngeochemical signatures that characterize the tectonic settings of interest and\\nthe characteristics of each sample as a probability of the membership of the\\nsample for each setting. we also present the most representative basalt\\ncomposition for each tectonic setting. the new data provide reference points\\nfor future geochemical discussions. our results indicate that at least 17\\nelements and isotopic ratios are required to characterize each tectonic\\nsetting, suggesting that geochemical tectonic discrimination cannot be achieved\\nusing only a small number of elemental compositions and/or isotopic ratios. the\\nresults show that volcanic rocks formed in different tectonic settings have\\nunique geochemical signatures, indicating that both volcanic rock geochemistry\\nand magma generation processes are closely connected to the tectonic setting.\\n)\": False, \"contains(applying machine learning to improve simulations of a chaotic dynamical system using empirical error correction\\n\\ndynamical weather and climate prediction models underpin many studies of the earth system and hold the promise of being able to make robust projections of future climate change based on physical laws. however, simulations from these models still show many di erences compared with observations. machine learning has been applied to solve certain prediction problems with great success, and recently it's been proposed that this could replace the role of physically-derived dynamical weather and climate models to give better quality simulations. here, instead, a framework using machine learning together with physically-derived models is tested, in which it is learnt how to correct the errors of the latter from timestep to timestep. this maintains the physical understanding built into the models, whilst allowing performance improvements, and also requires much simpler algorithms and less training data. this is tested in the context of simulating the chaotic lorenz '96 system, and it is shown that the approach yields models that are stable and that give both improved skill in initialised predictions and better long-term climate statistics. improvements in long-term statistics are smaller than for single time-step tendencies, however, indicating that it would be valuable to develop methods that target improvements on longer time scales. future strategies for the development of this approach and possible applications to making progress on important scienti c problems are discussed. numerical weather prediction and climate models attempt to predict and simulate components of the earth system, including the atmosphere and perhaps also the oceans, land surface and biosphere. whilst the fundamental physical equations governing the system are known, they cannot be solved accurately with available computational resources. instead, approximations are made in the models' equations, and this gives rise to errors in their output. methods to reduce these errors are highly valuable for giving better warning of major meteorological and climatic events. recently, great advances in machine learning have taken place, for example in the domains of image recognition and game-playing [e.g. he et al., 2016; silver et al., 2017]. the algorithms developed have been found to excel at certain problems that involve predicting an unknown value given values of predictor variables (for example, predicting what objects a photograph contains given its pixel values)|this is similar to the problem of predicting future behaviour of the earth system given knowledge of its past and present state, and so there has been high interest in applying\\n)\": False, 'contains(autoencoder-driven weather clustering for source estimation during nuclear eventsi\\n\\nemergency response applications for nuclear or radiological events can be significantly improved via deep feature learning due its ability to capture the inherent complexity of the data involved. in this paper we present a novel methodology for rapid source estimation during radiological releases based on deep feature extraction and weather clustering. atmospheric dispersions are then calculated based on identified predominant weather patterns and are matched against simulated incidents indicated by radiation readings on the ground. we evaluate the accuracy of our methods over multiple years of weather reanalysis data in the european region. we juxtapose these results with deep classification convolution networks and discuss advantages and disadvantages. we find that deep autoencoder configurations can lead to accurate-enough origin estimation to complement existing systems, while allowing for rapid initial response.\\n)': False, 'contains(on monitoring development indicators using high resolution satellite\\n  images\\n  we develop a machine learning based tool for accurate prediction of\\nsocio-economic indicators from daytime satellite imagery. the diverse set of\\nindicators are often not intuitively related to observable features in\\nsatellite images, and are not even always well correlated with each other. our\\npredictive tool is more accurate than using night light as a proxy, and can be\\nused to predict missing data, smooth out noise in surveys, monitor development\\nprogress of a region, and flag potential anomalies. finally, we use predicted\\nvariables to do robustness analysis of a regression study of high rate of\\nstunting in india.\\n)': False, 'contains(forecasting drought using multilayer perceptron artificial neural\\n  network model\\n  these days human beings are facing many environmental challenges due to\\nfrequently occurring drought hazards. it may have an effect on the countrys\\nenvironment, the community, and industries. several adverse impacts of drought\\nhazard are continued in pakistan, including other hazards. however, early\\nmeasurement and detection of drought can provide guidance to water resources\\nmanagement for employing drought mitigation policies. in this paper, we used a\\nmultilayer perceptron neural network (mlpnn) algorithm for drought forecasting.\\nwe applied and tested mlpnn algorithm on monthly time series data of\\nstandardized precipitation evapotranspiration index (spei) for seventeen\\nclimatological stations located in northern area and kpk (pakistan). we found\\nthat mlpnn has potential capability for spei drought forecasting based on\\nperformance measures (i.e., mean average error (mae), the coefficient of\\ncorrelation r, and root mean square error (rmse). water resources and\\nmanagement planner can take necessary action in advance (e.g., in water\\nscarcity areas) by using mlpnn model as part of their decision making.\\n)': False, 'contains(efficient bayesian experimentation using an expected information gain\\n  lower bound\\n  experimental design is crucial for inference where limitations in the data\\ncollection procedure are present due to cost or other restrictions. optimal\\nexperimental designs determine parameters that in some appropriate sense make\\nthe data the most informative possible. in a bayesian setting this is\\ntranslated to updating to the best possible posterior. information theoretic\\narguments have led to the formation of the expected information gain as a\\ndesign criterion. this can be evaluated mainly by monte carlo sampling and\\nmaximized by using stochastic approximation methods, both known for being\\ncomputationally expensive tasks. we propose a framework where a lower bound of\\nthe expected information gain is used as an alternative design criterion. in\\naddition to alleviating the computational burden, this also addresses issues\\nconcerning estimation bias. the problem of permeability inference in a large\\ncontaminated area is used to demonstrate the validity of our approach where we\\nemploy the massively parallel version of the multiphase multicomponent\\nsimulator tough2 to simulate contaminant transport and a polynomial chaos\\napproximation of the forward model that further accelerates the objective\\nfunction evaluations. the proposed methodology is demonstrated to a setting\\nwhere field measurements are available.\\n)': False, 'contains(city-scale road extraction from satellite imagery\\n  automated road network extraction from remote sensing imagery remains a\\nsignificant challenge despite its importance in a broad array of applications.\\nto this end, we leverage recent open source advances and the high quality\\nspacenet dataset to explore road network extraction at scale, and approach we\\ncall city-scale road extraction from satellite imagery (cresi). specifically,\\nwe create an algorithm to extract road networks directly from imagery over\\ncity-scale regions, which can subsequently be used for routing purposes. we\\nquantify the performance of our algorithm with the apls and topo\\ngraph-theoretic metrics over a diverse 608 square kilometer test area covering\\nfour cities. we find an aggregate score of apls = 0.73, and a topo score of\\n0.58 (a significant improvement over existing methods). inference speed is 160\\nsquare kilometers per hour on modest hardware.\\n)': False, 'contains(individual common dolphin identification via metric embedding learning\\n  photo-identification (photo-id) of dolphin individuals is a commonly used\\ntechnique in ecological sciences to monitor state and health of individuals, as\\nwell as to study the social structure and distribution of a population.\\ntraditional photo-id involves a laborious manual process of matching each\\ndolphin fin photograph captured in the field to a catalogue of known\\nindividuals.\\n  we examine this problem in the context of open-set recognition and utilise a\\ntriplet loss function to learn a compact representation of fin images in a\\neuclidean embedding, where the euclidean distance metric represents fin\\nsimilarity. we show that this compact representation can be successfully learnt\\nfrom a fairly small (in deep learning context) training set and still\\ngeneralise well to out-of-sample identities (completely new dolphin\\nindividuals), with top-1 and top-5 test set (37 individuals) accuracy of\\n$90.5\\\\pm2$ and $93.6\\\\pm1$ percent. in the presence of 1200 distractors, top-1\\naccuracy dropped by $12\\\\%$; however, top-5 accuracy saw only a $2.8\\\\%$ drop\\n)': False, 'contains(shallow water bathymetry mapping from uav imagery based on machine\\n  learning\\n  the determination of accurate bathymetric information is a key element for\\nnear offshore activities, hydrological studies such as coastal engineering\\napplications, sedimentary processes, hydrographic surveying as well as\\narchaeological mapping and biological research. uav imagery processed with\\nstructure from motion (sfm) and multi view stereo (mvs) techniques can provide\\na low-cost alternative to established shallow seabed mapping techniques\\noffering as well the important visual information. nevertheless, water\\nrefraction poses significant challenges on depth determination. till now, this\\nproblem has been addressed through customized image-based refraction correction\\nalgorithms or by modifying the collinearity equation. in this paper, in order\\nto overcome the water refraction errors, we employ machine learning tools that\\nare able to learn the systematic underestimation of the estimated depths. in\\nthe proposed approach, based on known depth observations from bathymetric lidar\\nsurveys, an svr model was developed able to estimate more accurately the real\\ndepths of point clouds derived from sfm-mvs procedures. experimental results\\nover two test sites along with the performed quantitative validation indicated\\nthe high potential of the developed approach.\\n)': False, 'contains(source localization in an ocean waveguide using supervised machine\\n  learning\\n  source localization in ocean acoustics is posed as a machine learning problem\\nin which data-driven methods learn source ranges directly from observed\\nacoustic data. the pressure received by a vertical linear array is preprocessed\\nby constructing a normalized sample covariance matrix (scm) and used as the\\ninput. three machine learning methods (feed-forward neural networks (fnn),\\nsupport vector machines (svm) and random forests (rf)) are investigated in this\\npaper, with focus on the fnn. the range estimation problem is solved both as a\\nclassification problem and as a regression problem by these three machine\\nlearning algorithms. the results of range estimation for the noise09 experiment\\nare compared for fnn, svm, rf and conventional matched-field processing and\\ndemonstrate the potential of machine learning for underwater source\\nlocalization..\\n)': False, 'contains(new approach for solar tracking systems based on computer vision, low\\n  cost hardware and deep learning\\n  in this work, a new approach for sun tracking systems is presented. due to\\nthe current system limitations regarding costs and operational problems, a new\\napproach based on low cost, computer vision open hardware and deep learning has\\nbeen developed. the preliminary tests carried out successfully in plataforma\\nsolar de almeria (psa), reveal the great potential and show the new approach as\\na good alternative to traditional systems. the proposed approach can provide\\nkey variables for the sun tracking system control like cloud movements\\nprediction, block and shadow detection, atmospheric attenuation or measures of\\nconcentrated solar radiation, which can improve the control strategies of the\\nsystem and therefore the system performance.\\n)': False, 'contains(development details and computational benchmarking of depam\\n  in the big data era of observational oceanography, passive acoustics datasets\\nare becoming too high volume to be processed on local computers due to their\\nprocessor and memory limitations. as a result there is a current need for our\\ncommunity to turn to cloud-based distributed computing. we present a scalable\\ncomputing system for fft (fast fourier transform)-based features (e.g., power\\nspectral density) based on the apache distributed frameworks hadoop and spark.\\nthese features are at the core of many different types of acoustic analysis\\nwhere the need of processing data at scale with speed is evident, e.g. serving\\nas long-term averaged learning representations of soundscapes to identify\\nperiods of acoustic interest. in addition to provide a complete description of\\nour system implementation, we also performed a computational benchmark\\ncomparing our system to three other scala-only, matlab and python based systems\\nin standalone executions, and evaluated its scalability using the speed up\\nmetric. our current results are very promising in terms of computational\\nperformance, as we show that our proposed hadoop/spark system performs\\nreasonably well on a single node setup comparatively to state-of-the-art\\nprocessing tools used by the pam community, and that it could also fully\\nleverage more intensive cluster resources with a almost-linear scalability\\nbehaviour above a certain dataset volume.\\n)': False, 'contains(deep learning based large-scale automatic satellite crosswalk\\n  classification\\n  high-resolution satellite imagery have been increasingly used on remote\\nsensing classification problems. one of the main factors is the availability of\\nthis kind of data. even though, very little effort has been placed on the zebra\\ncrossing classification problem. in this letter, crowdsourcing systems are\\nexploited in order to enable the automatic acquisition and annotation of a\\nlarge-scale satellite imagery database for crosswalks related tasks. then, this\\ndataset is used to train deep-learning-based models in order to accurately\\nclassify satellite images that contains or not zebra crossings. a novel dataset\\nwith more than 240,000 images from 3 continents, 9 countries and more than 20\\ncities was used in the experiments. experimental results showed that freely\\navailable crowdsourcing data can be used to accurately (97.11%) train robust\\nmodels to perform crosswalk classification on a global scale.\\n)': False, 'contains(breizhcrops: a satellite time series dataset for crop type\\n  identification\\n  this dataset challenges the time series community with the task of\\nsatellite-based vegetation identification on large scale real-world dataset of\\nsatellite data acquired during one entire year. it consists of time series data\\nwith associated crop types from 580k field parcels in brittany, france (breizh\\nin local language). along with this dataset, we provide results and code of a\\nlong short-term memory network and transformer network as baselines. we release\\ndataset, along with preprocessing scripts and baseline models in\\nhttps://github.com/tum-lmf/breizhcrops and encourage methodical researchers to\\nbenchmark and develop novel methods applied to satellite-based crop monitoring.\\n)': False, 'contains(harmonic exponential families on manifolds\\n  in a range of fields including the geosciences, molecular biology, robotics\\nand computer vision, one encounters problems that involve random variables on\\nmanifolds. currently, there is a lack of flexible probabilistic models on\\nmanifolds that are fast and easy to train. we define an extremely flexible\\nclass of exponential family distributions on manifolds such as the torus,\\nsphere, and rotation groups, and show that for these distributions the gradient\\nof the log-likelihood can be computed efficiently using a non-commutative\\ngeneralization of the fast fourier transform (fft). we discuss applications to\\nbayesian camera motion estimation (where harmonic exponential families serve as\\nconjugate priors), and modelling of the spatial distribution of earthquakes on\\nthe surface of the earth. our experimental results show that harmonic densities\\nyield a significantly higher likelihood than the best competing method, while\\nbeing orders of magnitude faster to train.\\n)': False, 'contains(topological characteristics of oil and gas reservoirs and their\\n  applications\\n  we demonstrate applications of topological characteristics of oil and gas\\nreservoirs considered as three-dimensional bodies to geological modeling.\\n)': False, \"contains(a tunable multiresolution smoother for scattered data with application\\n  to particle filtering\\n  a smoothing algorithm is presented that can reduce the small-scale content of\\ndata observed at scattered locations in a spatially extended domain. the\\nsmoother works by forming a gaussian interpolant of the input data, and then\\nconvolving the interpolant with a multiresolution gaussian approximation of the\\ngreen's function to a differential operator whose spectrum can be tuned for\\nproblem-specific considerations. this smoother is developed for its potential\\napplication to particle filtering, which often involves data scattered over a\\nspatial domain, since preprocessing observations with a smoother reduces the\\nensemble size required to avoid particle filter collapse. an example on\\nmeteorological data verifies that our smoother improves the balance of particle\\nfilter weights.\\n)\": False, \"contains(applying machine learning to improve simulations of a chaotic dynamical\\n  system using empirical error correction\\n  dynamical weather and climate prediction models underpin many studies of the\\nearth system and hold the promise of being able to make robust projections of\\nfuture climate change based on physical laws. however, simulations from these\\nmodels still show many differences compared with observations. machine learning\\nhas been applied to solve certain prediction problems with great success, and\\nrecently it's been proposed that this could replace the role of\\nphysically-derived dynamical weather and climate models to give better quality\\nsimulations. here, instead, a framework using machine learning together with\\nphysically-derived models is tested, in which it is learnt how to correct the\\nerrors of the latter from timestep to timestep. this maintains the physical\\nunderstanding built into the models, whilst allowing performance improvements,\\nand also requires much simpler algorithms and less training data. this is\\ntested in the context of simulating the chaotic lorenz '96 system, and it is\\nshown that the approach yields models that are stable and that give both\\nimproved skill in initialised predictions and better long-term climate\\nstatistics. improvements in long-term statistics are smaller than for single\\ntime-step tendencies, however, indicating that it would be valuable to develop\\nmethods that target improvements on longer time scales. future strategies for\\nthe development of this approach and possible applications to making progress\\non important scientific problems are discussed.\\n)\": False, 'contains(a machine learning benchmark for facies classification\\n  the recent interest in using deep learning for seismic interpretation tasks,\\nsuch as facies classification, has been facing a significant obstacle, namely\\nthe absence of large publicly available annotated datasets for training and\\ntesting models. as a result, researchers have often resorted to annotating\\ntheir own training and testing data. however, different researchers may\\nannotate different classes, or use different train and test splits. in\\naddition, it is common for papers that apply machine learning for facies\\nclassification to not contain quantitative results, and rather rely solely on\\nvisual inspection of the results. all of these practices have lead to\\nsubjective results and have greatly hindered the ability to compare different\\nmachine learning models against each other and understand the advantages and\\ndisadvantages of each approach.\\n  to address these issues, we open-source a fully-annotated 3d geological model\\nof the netherlands f3 block. this model is based on the study of the 3d seismic\\ndata in addition to 26 well logs, and is grounded on the careful study of the\\ngeology of the region. furthermore, we propose two baseline models for facies\\nclassification based on a deconvolution network architecture and make their\\ncodes publicly available. finally, we propose a scheme for evaluating different\\nmodels on this dataset, and we share the results of our baseline models. in\\naddition to making the dataset and the code publicly available, this work helps\\nadvance research in this area by creating an objective benchmark for comparing\\nthe results of different machine learning approaches for facies classification.\\n)': False, \"contains(the fog of war: a machine learning approach to forecasting weather on\\n  mars\\n  for over a decade, scientists at nasa's jet propulsion laboratory (jpl) have\\nbeen recording measurements from the martian surface as a part of the mars\\nexploration rovers mission. one quantity of interest has been the opacity of\\nmars's atmosphere for its importance in day-to-day estimations of the amount of\\npower available to the rover from its solar arrays. this paper proposes the use\\nof neural networks as a method for forecasting martian atmospheric opacity that\\nis more effective than the current empirical model. the more accurate\\nprediction provided by these networks would allow operators at jpl to make more\\naccurate predictions of the amount of energy available to the rover when they\\nplan activities for coming sols.\\n)\": False, 'contains(an ensemble of bayesian neural networks for exoplanetary atmospheric\\n  retrieval\\n  machine learning is now used in many areas of astrophysics, from detecting\\nexoplanets in kepler transit signals to removing telescope systematics. recent\\nwork demonstrated the potential of using machine learning algorithms for\\natmospheric retrieval by implementing a random forest to perform retrievals in\\nseconds that are consistent with the traditional, computationally-expensive\\nnested-sampling retrieval method. we expand upon their approach by presenting a\\nnew machine learning model, \\\\texttt{plan-net}, based on an ensemble of bayesian\\nneural networks that yields more accurate inferences than the random forest for\\nthe same data set of synthetic transmission spectra. we demonstrate that an\\nensemble provides greater accuracy and more robust uncertainties than a single\\nmodel. in addition to being the first to use bayesian neural networks for\\natmospheric retrieval, we also introduce a new loss function for bayesian\\nneural networks that learns correlations between the model outputs.\\nimportantly, we show that designing machine learning models to explicitly\\nincorporate domain-specific knowledge both improves performance and provides\\nadditional insight by inferring the covariance of the retrieved atmospheric\\nparameters. we apply \\\\texttt{plan-net} to the hubble space telescope wide field\\ncamera 3 transmission spectrum for wasp-12b and retrieve an isothermal\\ntemperature and water abundance consistent with the literature. we highlight\\nthat our method is flexible and can be expanded to higher-resolution spectra\\nand a larger number of atmospheric parameters.\\n)': False, 'contains(multivariate, multistep forecasting, reconstruction and feature\\n  selection of ocean waves via recurrent and sequence-to-sequence networks\\n  this article explores the concepts of ocean wave multivariate multistep\\nforecasting, reconstruction and feature selection. we introduce recurrent\\nneural network frameworks, integrated with bayesian hyperparameter optimization\\nand elastic net methods. we consider both short- and long-term forecasts and\\nreconstruction, for significant wave height and output power of the ocean\\nwaves. sequence-to-sequence neural networks are being developed for the first\\ntime to reconstruct the missing characteristics of ocean waves based on\\ninformation from nearby wave sensors. our results indicate that the adam and\\namsgrad optimization algorithms are the most robust ones to optimize the\\nsequence-to-sequence network. for the case of significant wave height\\nreconstruction, we compare the proposed methods with alternatives on a\\nwell-studied dataset. we show the superiority of the proposed methods\\nconsidering several error metrics. we design a new case study based on\\nmeasurement stations along the east coast of the united states and investigate\\nthe feature selection concept. comparisons substantiate the benefit of\\nutilizing elastic net. moreover, case study results indicate that when the\\nnumber of features is considerable, having deeper structures improves the\\nperformance.\\n)': False, \"contains(probabilistic data analysis with probabilistic programming\\n  probabilistic techniques are central to data analysis, but different\\napproaches can be difficult to apply, combine, and compare. this paper\\nintroduces composable generative population models (cgpms), a computational\\nabstraction that extends directed graphical models and can be used to describe\\nand compose a broad class of probabilistic data analysis techniques. examples\\ninclude hierarchical bayesian models, multivariate kernel methods,\\ndiscriminative machine learning, clustering algorithms, dimensionality\\nreduction, and arbitrary probabilistic programs. we also demonstrate the\\nintegration of cgpms into bayesdb, a probabilistic programming platform that\\ncan express data analysis tasks using a modeling language and a structured\\nquery language. the practical value is illustrated in two ways. first, cgpms\\nare used in an analysis that identifies satellite data records which probably\\nviolate kepler's third law, by composing causal probabilistic programs with\\nnon-parametric bayes in under 50 lines of probabilistic code. second, for\\nseveral representative data analysis tasks, we report on lines of code and\\naccuracy measurements of various cgpms, plus comparisons with standard baseline\\nsolutions from python and matlab libraries.\\n)\": False, \"contains(non-exchangeable random partition models for microclustering\\n  many popular random partition models, such as the chinese restaurant process\\nand its two-parameter extension, fall in the class of exchangeable random\\npartitions, and have found wide applicability in model-based clustering,\\npopulation genetics, ecology or network analysis. while the exchangeability\\nassumption is sensible in many cases, it has some strong implications. in\\nparticular, kingman's representation theorem implies that the size of the\\nclusters necessarily grows linearly with the sample size; this feature may be\\nundesirable for some applications, as recently pointed out by miller et al.\\n(2015). we present here a flexible class of non-exchangeable random partition\\nmodels which are able to generate partitions whose cluster sizes grow\\nsublinearly with the sample size, and where the growth rate is controlled by\\none parameter. along with this result, we provide the asymptotic behaviour of\\nthe number of clusters of a given size, and show that the model can exhibit a\\npower-law behavior, controlled by another parameter. the construction is based\\non completely random measures and a poisson embedding of the random partition,\\nand inference is performed using a sequential monte carlo algorithm.\\nadditionally, we show how the model can also be directly used to generate\\nsparse multigraphs with power-law degree distributions and degree sequences\\nwith sublinear growth. finally, experiments on real datasets emphasize the\\nusefulness of the approach compared to a two-parameter chinese restaurant\\nprocess.\\n)\": False, 'contains(a bayesian network approach to county-level corn yield prediction using\\n  historical data and expert knowledge\\n  crop yield forecasting is the methodology of predicting crop yields prior to\\nharvest. the availability of accurate yield prediction frameworks have enormous\\nimplications from multiple standpoints, including impact on the crop commodity\\nfutures markets, formulation of agricultural policy, as well as crop insurance\\nrating. the focus of this work is to construct a corn yield predictor at the\\ncounty scale. corn yield (forecasting) depends on a complex, interconnected set\\nof variables that include economic, agricultural, management and meteorological\\nfactors. conventional forecasting is either knowledge-based computer programs\\n(that simulate plant-weather-soil-management interactions) coupled with\\ntargeted surveys or statistical model based. the former is limited by the need\\nfor painstaking calibration, while the latter is limited to univariate analysis\\nor similar simplifying assumptions that fail to capture the complex\\ninterdependencies affecting yield. in this paper, we propose a data-driven\\napproach that is \"gray box\" i.e. that seamlessly utilizes expert knowledge in\\nconstructing a statistical network model for corn yield forecasting. our\\nmultivariate gray box model is developed on bayesian network analysis to build\\na directed acyclic graph (dag) between predictors and yield. starting from a\\ncomplete graph connecting various carefully chosen variables and yield, expert\\nknowledge is used to prune or strengthen edges connecting variables.\\nsubsequently the structure (connectivity and edge weights) of the dag that\\nmaximizes the likelihood of observing the training data is identified via\\noptimization. we curated an extensive set of historical data (1948-2012) for\\neach of the 99 counties in iowa as data to train the model.\\n)': False, 'contains(machine learning in apogee: unsupervised spectral classification with\\n  $k$-means\\n  the data volume generated by astronomical surveys is growing rapidly.\\ntraditional analysis techniques in spectroscopy either demand intensive human\\ninteraction or are computationally expensive. in this scenario, machine\\nlearning, and unsupervised clustering algorithms in particular offer\\ninteresting alternatives. the apache point observatory galactic evolution\\nexperiment (apogee) offers a vast data set of near-infrared stellar spectra\\nwhich is perfect for testing such alternatives. apply an unsupervised\\nclassification scheme based on $k$-means to the massive apogee data set.\\nexplore whether the data are amenable to classification into discrete classes.\\nwe apply the $k$-means algorithm to 153,847 high resolution spectra\\n($r\\\\approx22,500$). we discuss the main virtues and weaknesses of the\\nalgorithm, as well as our choice of parameters. we show that a classification\\nbased on normalised spectra captures the variations in stellar atmospheric\\nparameters, chemical abundances, and rotational velocity, among other factors.\\nthe algorithm is able to separate the bulge and halo populations, and\\ndistinguish dwarfs, sub-giants, rc and rgb stars. however, a discrete\\nclassification in flux space does not result in a neat organisation in the\\nparameters space. furthermore, the lack of obvious groups in flux space causes\\nthe results to be fairly sensitive to the initialisation, and disrupts the\\nefficiency of commonly-used methods to select the optimal number of clusters.\\nour classification is publicly available, including extensive online material\\nassociated with the apogee data release 12 (dr12). our description of the\\napogee database can enormously help with the identification of specific types\\nof targets for various applications. we find a lack of obvious groups in flux\\nspace, and identify limitations of the $k$-means algorithm in dealing with this\\nkind of data.\\n)': False, 'contains(a comparison of super-resolution and nearest neighbors interpolation\\n  applied to object detection on satellite data\\n  as super-resolution (sr) has matured as a research topic, it has been applied\\nto additional topics beyond image reconstruction. in particular, combining\\nclassification or object detection tasks with a super-resolution preprocessing\\nstage has yielded improvements in accuracy especially with objects that are\\nsmall relative to the scene. while sr has shown promise, a study comparing sr\\nand naive upscaling methods such as nearest neighbors (nn) interpolation when\\napplied as a preprocessing step for object detection has not been performed. we\\napply the topic to satellite data and compare the multi-scale deep\\nsuper-resolution (mdsr) system to nn on the xview challenge dataset. to do so,\\nwe propose a pipeline for processing satellite data that combines multi-stage\\nimage tiling and upscaling, the yolov2 object detection architecture, and label\\nstitching. we compare the effects of training models using an upscaling factor\\nof 4, upscaling images from 30cm ground sample distance (gsd) to an effective\\ngsd of 7.5cm. upscaling by this factor significantly improves detection\\nresults, increasing average precision (ap) of a generalized vehicle class by 23\\npercent. we demonstrate that while sr produces upscaled images that are more\\nvisually pleasing than their nn counterparts, object detection networks see\\nlittle difference in accuracy with images upsampled using nn obtaining nearly\\nidentical results to the mdsrx4 enhanced images with a difference of 0.0002 ap\\nbetween the two methods.\\n)': False, \"contains(hierarchical implicit models and likelihood-free variational inference\\n  implicit probabilistic models are a flexible class of models defined by a\\nsimulation process for data. they form the basis for theories which encompass\\nour understanding of the physical world. despite this fundamental nature, the\\nuse of implicit models remains limited due to challenges in specifying complex\\nlatent structure in them, and in performing inferences in such models with\\nlarge data sets. in this paper, we first introduce hierarchical implicit models\\n(hims). hims combine the idea of implicit densities with hierarchical bayesian\\nmodeling, thereby defining models via simulators of data with rich hidden\\nstructure. next, we develop likelihood-free variational inference (lfvi), a\\nscalable variational inference algorithm for hims. key to lfvi is specifying a\\nvariational family that is also implicit. this matches the model's flexibility\\nand allows for accurate approximation of the posterior. we demonstrate diverse\\napplications: a large-scale physical simulator for predator-prey populations in\\necology; a bayesian generative adversarial network for discrete data; and a\\ndeep implicit model for text generation.\\n)\": False, 'contains(metrics for learning in topological persistence\\n  persistent homology analysis provides means to capture the connectivity\\nstructure of data sets in various dimensions. on the mathematical level, by\\ndefining a metric between the objects that persistence attaches to data sets,\\nwe can stabilize invariants characterizing these objects. we outline how so\\ncalled contour functions induce relevant metrics for stabilizing the rank\\ninvariant. on the practical level, the stable ranks are used as fingerprints\\nfor data. different choices of contour lead to different stable ranks and the\\ntopological learning is then the question of finding the optimal contour. we\\noutline our analysis pipeline and show how it can enhance classification of\\nphysical activities data. as our main application we study how stable ranks and\\ncontours provide robust descriptors of spatial patterns of atmospheric cloud\\nfields.\\n)': False, 'contains(a data-driven approach to precipitation parameterizations using\\n  convolutional encoder-decoder neural networks\\n  numerical weather prediction (nwp) models represent sub-grid processes using\\nparameterizations, which are often complex and a major source of uncertainty in\\nweather forecasting. in this work, we devise a simple machine learning (ml)\\nmethodology to learn parameterizations from basic nwp fields. specifically, we\\ndemonstrate how encoder-decoder convolutional neural networks (cnn) can be used\\nto derive total precipitation using geopotential height as the only input.\\nseveral popular neural network architectures, from the field of image\\nprocessing, are considered and a comparison with baseline ml methodologies is\\nprovided. we use nwp reanalysis data to train different ml models showing how\\nencoder-decoder cnns are able to interpret the spatial information contained in\\nthe geopotential field to infer total precipitation with a high degree of\\naccuracy. we also provide a method to identify the levels of the geopotential\\nheight that have a higher influence on precipitation through a variable\\nselection process. as far as we know, this paper covers the first attempt to\\nmodel nwp parameterizations using cnn methodologies.\\n)': False, 'contains(towards a robust parameterization for conditioning facies models using\\n  deep variational autoencoders and ensemble smoother\\n  the literature about history matching is vast and despite the impressive\\nnumber of methods proposed and the significant progresses reported in the last\\ndecade, conditioning reservoir models to dynamic data is still a challenging\\ntask. ensemble-based methods are among the most successful and efficient\\ntechniques currently available for history matching. these methods are usually\\nable to achieve reasonable data matches, especially if an iterative formulation\\nis employed. however, they sometimes fail to preserve the geological realism of\\nthe model, which is particularly evident in reservoir with complex facies\\ndistributions. this occurs mainly because of the gaussian assumptions inherent\\nin these methods. this fact has encouraged an intense research activity to\\ndevelop parameterizations for facies history matching. despite the large number\\nof publications, the development of robust parameterizations for facies remains\\nan open problem.\\n  deep learning techniques have been delivering impressive results in a number\\nof different areas and the first applications in data assimilation in\\ngeoscience have started to appear in literature. the present paper reports the\\ncurrent results of our investigations on the use of deep neural networks\\ntowards the construction of a continuous parameterization of facies which can\\nbe used for data assimilation with ensemble methods. specifically, we use a\\nconvolutional variational autoencoder and the ensemble smoother with multiple\\ndata assimilation. we tested the parameterization in three synthetic\\nhistory-matching problems with channelized facies. we focus on this type of\\nfacies because they are among the most challenging to preserve after the\\nassimilation of data. the parameterization showed promising results\\noutperforming previous methods and generating well-defined channelized facies.\\n)': False, 'contains(a deterministic self-organizing map approach and its application on\\n  satellite data based cloud type classification\\n  a self-organizing map (som) is a type of competitive artificial neural\\nnetwork, which projects the high-dimensional input space of the training\\nsamples into a low-dimensional space with the topology relations preserved.\\nthis makes soms supportive of organizing and visualizing complex data sets and\\nhave been pervasively used among numerous disciplines with different\\napplications. notwithstanding its wide applications, the self-organizing map is\\nperplexed by its inherent randomness, which produces dissimilar som patterns\\neven when being trained on identical training samples with the same parameters\\nevery time, and thus causes usability concerns for other domain practitioners\\nand precludes more potential users from exploring som based applications in a\\nbroader spectrum. motivated by this practical concern, we propose a\\ndeterministic approach as a supplement to the standard self-organizing map. in\\naccordance with the theoretical design, the experimental results with satellite\\ncloud data demonstrate the effective and efficient organization as well as\\nsimplification capabilities of the proposed approach.\\n)': False, 'contains(learning disentangled representations of satellite image time series\\n  in this paper, we investigate how to learn a suitable representation of\\nsatellite image time series in an unsupervised manner by leveraging large\\namounts of unlabeled data. additionally , we aim to disentangle the\\nrepresentation of time series into two representations: a shared representation\\nthat captures the common information between the images of a time series and an\\nexclusive representation that contains the specific information of each image\\nof the time series. to address these issues, we propose a model that combines a\\nnovel component called cross-domain autoencoders with the variational\\nautoencoder (vae) and generative ad-versarial network (gan) methods. in order\\nto learn disentangled representations of time series, our model learns the\\nmultimodal image-to-image translation task. we train our model using satellite\\nimage time series from the sentinel-2 mission. several experiments are carried\\nout to evaluate the obtained representations. we show that these disentangled\\nrepresentations can be very useful to perform multiple tasks such as image\\nclassification, image retrieval, image segmentation and change detection.\\n)': False, 'contains(deepfreak: learning crystallography diffraction patterns with automated\\n  machine learning\\n  serial crystallography is the field of science that studies the structure and\\nproperties of crystals via diffraction patterns. in this paper, we introduce a\\nnew serial crystallography dataset comprised of real and synthetic images; the\\nsynthetic images are generated through the use of a simulator that is both\\nscalable and accurate. the resulting dataset is called diffranet, and it is\\ncomposed of 25,457 512x512 grayscale labeled images. we explore several\\ncomputer vision approaches for classification on diffranet such as standard\\nfeature extraction algorithms associated with random forests and support vector\\nmachines but also an end-to-end cnn topology dubbed deepfreak tailored to work\\non this new dataset. all implementations are publicly available and have been\\nfine-tuned using off-the-shelf automl optimization tools for a fair comparison.\\nour best model achieves 98.5% accuracy on synthetic images and 94.51% accuracy\\non real images. we believe that the diffranet dataset and its classification\\nmethods will have in the long term a positive impact in accelerating\\ndiscoveries in many disciplines, including chemistry, geology, biology,\\nmaterials science, metallurgy, and physics.\\n)': False, 'contains(a machine learning approach for underwater gas leakage detection\\n\\nfrom the past recent years, co2 capture and storage underwater gas reservoirs are used in many situa- (ccs) technology has been considered to be a gametions. in particular, carbon capture and storage changing technology to avoid human-induced global (ccs) facilities that are currently being developed warming and the resulting climatic change[6]. intend to store greenhouse gases inside geological for- there are many challenges that must be met in mations in the deep sea. in these formations, how- order to guarantee the safety of the geologic reservoirs ever, the gas might percolate, leaking back to the used to store greenhouse gases. one of them is to water and eventually to the atmosphere. the early avoid and monitor leakages from the reservoir. detection of such leaks is therefore tantamount to any international literature describes plenty of moniunderwater ccs project. in this work, we propose to toring tools that have been tested and have been used use passive acoustic monitoring (pam) and a ma- in the last years for marine co2 storage monitoring chine learning approach to design e cient detectors programs [12, 7]. some of them are used for rapid that can signal the presence of a leakage. we use and focused spatial monitoring; others are intended data obtained from simulation experiments o the for long time and large area coverage. brazilian shore, and show that the detection based on regarding passive acoustic monitoring, when these classi cation algorithms achieve good performance. leakages arise in the form of bubbles a characwe also propose a smoothing strategy based on hid- teristic acoustic signal is produced, as shown by den markov models in order to incorporate previous [15, 18, 1, 14]. this signal can be used for detectknowledge about the probabilities of leakage occur- ing and locating gaseous leaks. rences. this work proposes the development of a passive\\n)': False, 'contains(non-convex regularization in remote sensing\\n  in this paper, we study the effect of different regularizers and their\\nimplications in high dimensional image classification and sparse linear\\nunmixing. although kernelization or sparse methods are globally accepted\\nsolutions for processing data in high dimensions, we present here a study on\\nthe impact of the form of regularization used and its parametrization. we\\nconsider regularization via traditional squared (2) and sparsity-promoting (1)\\nnorms, as well as more unconventional nonconvex regularizers (p and log sum\\npenalty). we compare their properties and advantages on several classification\\nand linear unmixing tasks and provide advices on the choice of the best\\nregularizer for the problem at hand. finally, we also provide a fully\\nfunctional toolbox for the community.\\n)': False, 'contains(estimating regional ground-level pm2.5 directly from satellite\\n  top-of-atmosphere reflectance using deep learning\\n  almost all remote sensing atmospheric pm2.5 estimation methods need satellite\\naerosol optical depth (aod) products, which are often retrieved from\\ntop-of-atmosphere (toa) reflectance via an atmospheric radiative transfer\\nmodel. then, is it possible to estimate ground-level pm2.5 directly from\\nsatellite toa reflectance without a physical model? in this study, this\\nchallenging work are achieved based on a machine learning model. specifically,\\nwe establish the relationship between pm2.5, satellite toa reflectance,\\nobservation angles, and meteorological factors in a deep learning architecture\\n(denoted as ref-pm modeling). taking the wuhan urban agglomeration (wua) as a\\ncase study, the results demonstrate that compared with the aod-pm modeling, the\\nref-pm modeling obtains a competitive performance, with out-of-sample\\ncross-validated r2 and rmse values of 0.87 and 9.89 ug/m3 respectively. also,\\nthe toa-reflectance-derived pm2.5 have a finer resolution and larger spatial\\ncoverage than the aod-derived pm2.5. this work updates the traditional\\ncognition of remote sensing pm2.5 estimation and has the potential to promote\\nthe application in atmospheric environmental monitoring.\\n)': False, 'contains(learning to regularize with a variational autoencoder for hydrologic\\n  inverse analysis\\n  inverse problems often involve matching observational data using a physical\\nmodel that takes a large number of parameters as input. these problems tend to\\nbe under-constrained and require regularization to impose additional structure\\non the solution in parameter space. a central difficulty in regularization is\\nturning a complex conceptual model of this additional structure into a\\nfunctional mathematical form to be used in the inverse analysis. in this work\\nwe propose a method of regularization involving a machine learning technique\\nknown as a variational autoencoder (vae). the vae is trained to map a\\nlow-dimensional set of latent variables with a simple structure to the\\nhigh-dimensional parameter space that has a complex structure. we train a vae\\non unconditioned realizations of the parameters for a hydrological inverse\\nproblem. these unconditioned realizations neither rely on the observational\\ndata used to perform the inverse analysis nor require any forward runs of the\\nphysical model, thus making the computational cost of generating the training\\ndata minimal. the central benefit of this approach is that regularization is\\nthen performed on the latent variables from the vae, which can be regularized\\nsimply. a second benefit of this approach is that the vae reduces the number of\\nvariables in the optimization problem, thus making gradient-based optimization\\nmore computationally efficient when adjoint methods are unavailable. after\\nperforming regularization and optimization on the latent variables, the vae\\nthen decodes the problem back to the original parameter space. our approach\\nconstitutes a novel framework for regularization and optimization, readily\\napplicable to a wide range of inverse problems. we call the approach regae.\\n)': False, \"contains(decentralized flood forecasting using deep neural networks\\n\\nthis article is a pre-print submitted to arxiv. 9 1 0 abstract-predicting flood for any location at times of extreme 2 storms is a longstanding problem that has utmost importance n in emergency management. conventional methods that aim to u predict water levels in streams use advanced hydrological models j still lack of giving accurate forecasts everywhere. this study aims to explore artificial deep neural networks' performance 1 on flood prediction. while providing models that can be used 2 in forecasting stream stage, this paper presents a dataset that ] focuses on the connectivity of data points on river networks. it also shows that neural networks can be very helpful in time-series gforecasting as in flood events, and support improving existing .slmoidnedlesxthtreorumgsh-dnaetuaraals,sinmetilwaotirokns., flood, forecasting c [\\n)\": False, 'contains(locdyn: robust distributed localization for mobile underwater networks\\n  how to self-localize large teams of underwater nodes using only noisy range\\nmeasurements? how to do it in a distributed way, and incorporating dynamics\\ninto the problem? how to reject outliers and produce trustworthy position\\nestimates? the stringent acoustic communication channel and the accuracy needs\\nof our geophysical survey application demand faster and more accurate\\nlocalization methods. we approach dynamic localization as a map estimation\\nproblem where the prior encodes dynamics, and we devise a convex relaxation\\nmethod that takes advantage of previous estimates at each measurement\\nacquisition step; the algorithm converges at an optimal rate for first order\\nmethods. locdyn is distributed: there is no fusion center responsible for\\nprocessing acquired data and the same simple computations are performed for\\neach node. locdyn is accurate: experiments attest to a smaller positioning\\nerror than a comparable kalman filter. locdyn is robust: it rejects outlier\\nnoise, while the comparing methods succumb in terms of positioning error.\\n)': False, 'contains(an introduction to (smoothing spline) anova models in rkhs with examples\\n  in geographical data, medicine, atmospheric science and machine learning\\n  smoothing spline anova (ss-anova) models in reproducing kernel hilbert spaces\\n(rkhs) provide a very general framework for data analysis, modeling and\\nlearning in a variety of fields. discrete, noisy scattered, direct and indirect\\nobservations can be accommodated with multiple inputs and multiple possibly\\ncorrelated outputs and a variety of meaningful structures. the purpose of this\\npaper is to give a brief overview of the approach and describe and contrast a\\nseries of applications, while noting some recent results.\\n)': False, 'contains(exploiting capacity of sewer system using unsupervised learning\\n  algorithms combined with dimensionality reduction\\n  exploiting capacity of sewer system using decentralized control is a cost\\neffective mean of minimizing the overflow. given the size of the real sewer\\nsystem, exploiting all the installed control structures in the sewer pipes can\\nbe challenging. this paper presents a divide and conquer solution to implement\\ndecentralized control measures based on unsupervised learning algorithms. a\\nsewer system is first divided into a number of subcatchments. a series of\\nnatural and built factors that have the impact on sewer system performance is\\nthen collected. clustering algorithms are then applied to grouping\\nsubcatchments with similar hydraulic hydrologic characteristics. following\\nwhich, principal component analysis is performed to interpret the main features\\nof sub-catchment groups and identify priority control locations. overflows\\nunder different control scenarios are compared based on the hydraulic model.\\nsimulation results indicate that priority control applied to the most suitable\\ncluster could bring the most profitable result.\\n)': False, 'contains(a new bayesian ensemble of trees classifier for identifying multi-class\\n  labels in satellite images\\n  classification of satellite images is a key component of many remote sensing\\napplications. one of the most important products of a raw satellite image is\\nthe classified map which labels the image pixels into meaningful classes.\\nthough several parametric and non-parametric classifiers have been developed\\nthus far, accurate labeling of the pixels still remains a challenge. in this\\npaper, we propose a new reliable multiclass-classifier for identifying class\\nlabels of a satellite image in remote sensing applications. the proposed\\nmulticlass-classifier is a generalization of a binary classifier based on the\\nflexible ensemble of regression trees model called bayesian additive regression\\ntrees (bart). we used three small areas from the landsat 5 tm image, acquired\\non august 15, 2009 (path/row: 08/29, l1t product, utm map projection) over\\nkings county, nova scotia, canada to classify the land-use. several prediction\\naccuracy and uncertainty measures have been used to compare the reliability of\\nthe proposed classifier with the state-of-the-art classifiers in remote\\nsensing.\\n)': False, 'contains(combined machine learning and calphad approach for discovering\\n  processing-structure relationships in soft magnetic alloys\\n  we aim to investigate relationships between select processing parameters or\\ninputs (composition, temperature, annealing time) and two structural\\nparameters, specifically, the mean radius and volume fraction of the fe$_3$si\\nnanocrystals. to this end, we have deviced a combined calphad and machine\\nlearning approach that led to well-calibrated metamodels able to predict\\nstructural parameters quickly and accurately for any desired inputs. in order\\nto generate data for the mean radius and volume fraction of fe$_3$si\\nnanocrystals, we have used a precipitation model based in the software\\nthermocalc to perform annealing simulations at a set of temperatures\\n(490-550~\\\\degree c) and for varying fe and si concentrations (fe$_{72.89\\n+x}$si$_{16.21-x}$b$_{6.90}$nb$_{3}$cu$_{1}$, $-3\\\\leq x \\\\leq 3$ atomic \\\\%).\\nthereafter, we used the data to develop metamodels for the mean radius and\\nvolume fraction via the \\\\emph{k}-nearest neighbour algorithm. the metamodels\\nare shown to reproduce closely the trends obtained from the precipitation model\\nover the entire annealing timescale. our further analysis via parallel\\ncoordinate charts shows the effect of composition, temperature, and annealing\\ntime, and helps identify combinations thereof that lead to the desired mean\\nradius and volume fraction for the nanocrystalline phase. this approach\\nutilizes experimental (thermodynamic and kinetic) databases from the calphad\\napproach so as to capture the physics of nucleation and growth, while the\\nmachine learning algorithm provides the robustness needed to analyze the\\neffects of processing parameters for this complex precipitation problem. this\\nwork contributes to understanding the linkages between processing parameters\\nand desired microstructural characteristics (crystal size and volume fraction)\\nresponsible for achieving targeted properties, and illustrates ways to reduce\\nthe time from alloy discovery to deployment.\\n)': False, 'contains(approximate variational inference based on a finite sample of gaussian\\n  latent variables\\n  variational methods are employed in situations where exact bayesian inference\\nbecomes intractable due to the difficulty in performing certain integrals.\\ntypically, variational methods postulate a tractable posterior and formulate a\\nlower bound on the desired integral to be approximated, e.g. marginal\\nlikelihood. the lower bound is then optimised with respect to its free\\nparameters, the so called variational parameters. however, this is not always\\npossible as for certain integrals it is very challenging (or tedious) to come\\nup with a suitable lower bound. here we propose a simple scheme that overcomes\\nsome of the awkward cases where the usual variational treatment becomes\\ndifficult. the scheme relies on a rewriting of the lower bound on the model\\nlog-likelihood. we demonstrate the proposed scheme on a number of synthetic and\\nreal examples, as well as on a real geophysical model for which the standard\\nvariational approaches are inapplicable.\\n)': False, 'contains(machine learning for the geosciences: challenges and oppor tunities\\n\\n-geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. as geosciences enters the era of big data, machine learning (ml)- that has been widely successful in commercial domains-offers immense potential to contribute to problems in geosciences. however, problems in geosciences have several unique challenges that are seldom found in traditional applications, requiring novel problem formulations and methodologies in machine learning. this article introduces researchers in the machine learning (ml) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. we first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques. we then describe some of the common categories of geoscience problems where machine learning can play a role, and discuss some of the existing efforts and promising directions for methodological development in machine learning. we conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.\\n)': False, 'contains(an enhanced neural network based approach towards object extraction\\n  the improvements in spectral and spatial resolution of the satellite images\\nhave facilitated the automatic extraction and identification of the features\\nfrom satellite images and aerial photographs. an automatic object extraction\\nmethod is presented for extracting and identifying the various objects from\\nsatellite images and the accuracy of the system is verified with regard to irs\\nsatellite images. the system is based on neural network and simulates the\\nprocess of visual interpretation from remote sensing images and hence increases\\nthe efficiency of image analysis. this approach obtains the basic\\ncharacteristics of the various features and the performance is enhanced by the\\nautomatic learning approach, intelligent interpretation, and intelligent\\ninterpolation. the major advantage of the method is its simplicity and that the\\nsystem identifies the features not only based on pixel value but also based on\\nthe shape, haralick features etc of the objects. further the system allows\\nflexibility for identifying the features within the same category based on size\\nand shape. the successful application of the system verified its effectiveness\\nand the accuracy of the system were assessed by ground truth verification.\\n)': False, 'contains(spatially constrained spectral clustering algorithms for region\\n  delineation\\n  regionalization is the task of dividing up a landscape into homogeneous\\npatches with similar properties. although this task has a wide range of\\napplications, it has two notable challenges. first, it is assumed that the\\nresulting regions are both homogeneous and spatially contiguous. second, it is\\nwell-recognized that landscapes are hierarchical such that fine-scale regions\\nare nested wholly within broader-scale regions. to address these two\\nchallenges, first, we develop a spatially constrained spectral clustering\\nframework for region delineation that incorporates the tradeoff between region\\nhomogeneity and spatial contiguity. the framework uses a flexible, truncated\\nexponential kernel to represent the spatial contiguity constraints, which is\\nintegrated with the landscape feature similarity matrix for region delineation.\\nto address the second challenge, we extend the framework to create fine-scale\\nregions that are nested within broader-scaled regions using a greedy, recursive\\nbisection approach. we present a case study of a terrestrial ecology data set\\nin the united states that compares the proposed framework with several baseline\\nmethods for regionalization. experimental results suggest that the proposed\\nframework for regionalization outperforms the baseline methods, especially in\\nterms of balancing region contiguity and homogeneity, as well as creating\\nregions of more similar size, which is often a desired trait of regions.\\n)': False, 'contains(a machine learning approach to adaptive covariance localization\\n  data assimilation plays a key role in large-scale atmospheric weather\\nforecasting, where the state of the physical system is estimated from model\\noutputs and observations, and is then used as initial condition to produce\\naccurate future forecasts. the ensemble kalman filter (enkf) provides a\\npractical implementation of the statistical solution of the data assimilation\\nproblem and has gained wide popularity as. this success can be attributed to\\nits simple formulation and ease of implementation. enkf is a monte-carlo\\nalgorithm that solves the data assimilation problem by sampling the probability\\ndistributions involved in bayes theorem. because of this, all flavors of enkf\\nare fundamentally prone to sampling errors when the ensemble size is small. in\\ntypical weather forecasting applications, the model state space has dimension\\n$10^{9}-10^{12}$, while the ensemble size typically ranges between $30-100$\\nmembers. sampling errors manifest themselves as long-range spurious\\ncorrelations and have been shown to cause filter divergence. to alleviate this\\neffect covariance localization dampens spurious correlations between state\\nvariables located at a large distance in the physical space, via an empirical\\ndistance-dependent function. the quality of the resulting analysis and forecast\\nis greatly influenced by the choice of the localization function parameters,\\ne.g., the radius of influence. the localization radius is generally tuned\\nempirically to yield desirable results.this work, proposes two adaptive\\nalgorithms for covariance localization in the enkf framework, both based on a\\nmachine learning approach. the first algorithm adapts the localization radius\\nin time, while the second algorithm tunes the localization radius in both time\\nand space. numerical experiments carried out with the lorenz-96 model, and a\\nquasi-geostrophic model, reveal the potential of the proposed machine learning\\napproaches.\\n)': False, 'contains(solving petrological problems through machine learning: the study case\\n  of tectonic discrimination using geochemical and isotopic data\\n  machine learning methods are evaluated to study the intriguing and debated\\ntopic of discrimination among different tectonic environments using geochemical\\nand isotopic data. volcanic rocks characterized by a whole geochemical\\nsignature of major elements (sio2, tio2, al2o3, fe2o3t, cao, mgo, na2o, k2o),\\nselected trace elements (sr, ba, rb, zr, nb, la, ce, nd, hf, sm, gd, y, yb, lu,\\nta, th) and isotopes (206pb/204pb, 207pb/204pb, 208pb/204pb, 87sr/86sr and\\n143nd/144nd) have been extracted from open-access and comprehensive\\npetrological databases (i.e. petdb and georoc). the obtained dataset has been\\nanalyzed using support vector machines, a set of supervised machine learning\\nmethods, which are considered particularly powerful in classification problems.\\nresults from the application of the machine learning methods show that the\\ncombined use of major, trace elements and isotopes allow associating the\\ngeochemical composition of rocks to the relative tectonic setting with high\\nclassification scores (93%, on average). the lowest scores are recorded from\\nvolcanic rocks deriving from back-arc basins (65%). all the other tectonic\\nsettings display higher classification scores, with oceanic islands reaching\\nvalues up to 99%. results of this study could have a significant impact in\\nother petrological studies potentially opening new perspectives for\\npetrologists and geochemists. other examples of applications include the\\ndevelopment of more robust geo-thermometers and geo-barometers and the\\nrecognition of volcanic sources for tephra layers in tephro-chronological\\nstudies.\\n)': False, 'contains(parametric modelling of multivariate count data using probabilistic\\n  graphical models\\n  multivariate count data are defined as the number of items of different\\ncategories issued from sampling within a population, which individuals are\\ngrouped into categories. the analysis of multivariate count data is a recurrent\\nand crucial issue in numerous modelling problems, particularly in the fields of\\nbiology and ecology (where the data can represent, for example, children counts\\nassociated with multitype branching processes), sociology and econometrics. we\\nfocus on i) identifying categories that appear simultaneously, or on the\\ncontrary that are mutually exclusive. this is achieved by identifying\\nconditional independence relationships between the variables; ii)building\\nparsimonious parametric models consistent with these relationships; iii)\\ncharacterising and testing the effects of covariates on the joint distribution\\nof the counts. to achieve these goals, we propose an approach based on\\ngraphical probabilistic models, and more specifically partially directed\\nacyclic graphs.\\n)': False, 'contains(a mixed model approach to drought prediction using artificial neural\\n  networks: case of an operational drought monitoring environment\\n  droughts, with their increasing frequency of occurrence, continue to\\nnegatively affect livelihoods and elements at risk. for example, the 2011 in\\ndrought in east africa has caused massive losses document to have cost the\\nkenyan economy over $12bn. with the foregoing, the demand for ex-ante drought\\nmonitoring systems is ever-increasing. the study uses 10 precipitation and\\nvegetation variables that are lagged over 1, 2 and 3-month time-steps to\\npredict drought situations. in the model space search for the most predictive\\nartificial neural network (ann) model, as opposed to the traditional greedy\\nsearch for the most predictive variables, we use the general additive model\\n(gam) approach. together with a set of assumptions, we thereby reduce the\\ncardinality of the space of models. even though we build a total of 102 gam\\nmodels, only 21 have r2 greater than 0.7 and are thus subjected to the ann\\nprocess. the ann process itself uses the brute-force approach that\\nautomatically partitions the training data into 10 sub-samples, builds the ann\\nmodels in these samples and evaluates their performance using multiple metrics.\\nthe results show the superiority of 1-month lag of the variables as compared to\\nlonger time lags of 2 and 3 months. the champion ann model recorded an r2 of\\n0.78 in model testing using the out-of-sample data. this illustrates its\\nability to be a good predictor of drought situations 1-month ahead.\\ninvestigated as a classifier, the champion has a modest accuracy of 66% and a\\nmulti-class area under the roc curve (auroc) of 89.99%\\n)': False, 'contains(a learning based approach for uncertainty analysis in numerical weather\\n  prediction models\\n  complex numerical weather prediction models incorporate a variety of physical\\nprocesses, each described by multiple alternative physical schemes with\\nspecific parameters. the selection of the physical schemes and the choice of\\nthe corresponding physical parameters during model configuration can\\nsignificantly impact the accuracy of model forecasts. there is no combination\\nof physical schemes that works best for all times, at all locations, and under\\nall conditions. it is therefore of considerable interest to understand the\\ninterplay between the choice of physics and the accuracy of the resulting\\nforecasts under different conditions. this paper demonstrates the use of\\nmachine learning techniques to study the uncertainty in numerical weather\\nprediction models due to the interaction of multiple physical processes. the\\nfirst problem addressed herein is the estimation of systematic model errors in\\noutput quantities of interest at future times, and the use of this information\\nto improve the model forecasts. the second problem considered is the\\nidentification of those specific physical processes that contribute most to the\\nforecast uncertainty in the quantity of interest under specified meteorological\\nconditions.\\n  the discrepancies between model results and observations at past times are\\nused to learn the relationships between the choice of physical processes and\\nthe resulting forecast errors. numerical experiments are carried out with the\\nweather research and forecasting (wrf) model. the output quantity of interest\\nis the model precipitation, a variable that is both extremely important and\\nvery challenging to forecast. the physical processes under consideration\\ninclude various micro-physics schemes, cumulus parameterizations, short wave,\\nand long wave radiation schemes. the experiments demonstrate the strong\\npotential of machine learning approaches to aid the study of model errors.\\n)': False, 'contains(topological descriptors of spatial coherence in a convective boundary\\n  layer\\n  the interaction between a turbulent convective boundary layer (cbl) and the\\nunderlying land surface is an important research problem in the geosciences. in\\norder to model this interaction adequately, it is necessary to develop tools\\nwhich can describe it quantitatively. commonly employed methods, such as bulk\\nflow statistics, are known to be insufficient for this task, especially when\\nland surfaces with equal aggregate statistics but different spatial patterns\\nare involved. while geometrical properties of the surface forcing have a strong\\ninfluence on flow structure, it is precisely those properties that get\\nneglected when computing bulk statistics. here, we present a set of descriptors\\nbased on low-level topological information (i.\\\\,e. connectivity), and show how\\nthese can be used both in the structural analysis of the cbl and in modeling\\nits response to differences in surface forcing. the topological property of\\nconnectivity is not only easier to compute than its higher-dimensional\\nhomological counterparts, but also has a natural relation to the physical\\nconcept of a coherent structure.\\n)': False, 'contains(enabling large-scale viscoelastic calculations via neural network\\n  acceleration\\n  one of the most significant challenges involved in efforts to understand the\\neffects of repeated earthquake cycle activity are the computational costs of\\nlarge-scale viscoelastic earthquake cycle models. computationally intensive\\nviscoelastic codes must be evaluated thousands of times and locations, and as a\\nresult, studies tend to adopt a few fixed rheological structures and model\\ngeometries, and examine the predicted time-dependent deformation over short\\n(<10 yr) time periods at a given depth after a large earthquake. training a\\ndeep neural network to learn a computationally efficient representation of\\nviscoelastic solutions, at any time, location, and for a large range of\\nrheological structures, allows these calculations to be done quickly and\\nreliably, with high spatial and temporal resolution. we demonstrate that this\\nmachine learning approach accelerates viscoelastic calculations by more than\\n50,000%. this magnitude of acceleration will enable the modeling of\\ngeometrically complex faults over thousands of earthquake cycles across wider\\nranges of model parameters and at larger spatial and temporal scales than have\\nbeen previously possible.\\n)': False, 'contains(a step towards procedural terrain generation with gans\\n  procedural terrain generation for video games has been traditionally been\\ndone with smartly designed but handcrafted algorithms that generate heightmaps.\\nwe propose a first step toward the learning and synthesis of these using recent\\nadvances in deep generative modelling with openly available satellite imagery\\nfrom nasa.\\n)': False, \"contains(the fluxcom ensemble of global land-atmosphere energy fluxes\\n  although a key driver of earth's climate system, global land-atmosphere\\nenergy fluxes are poorly constrained. here we use machine learning to merge\\nenergy flux measurements from fluxnet eddy covariance towers with remote\\nsensing and meteorological data to estimate net radiation, latent and sensible\\nheat and their uncertainties. the resulting fluxcom database comprises 147\\nglobal gridded products in two setups: (1) 0.0833${\\\\deg}$ resolution using\\nmodis remote sensing data (rs) and (2) 0.5${\\\\deg}$ resolution using remote\\nsensing and meteorological data (rs+meteo). within each setup we use a full\\nfactorial design across machine learning methods, forcing datasets and energy\\nbalance closure corrections. for rs and rs+meteo setups respectively, we\\nestimate 2001-2013 global (${\\\\pm}$ 1 standard deviation) net radiation as\\n75.8${\\\\pm}$1.4 ${w\\\\ m^{-2}}$ and 77.6${\\\\pm}$2 ${w\\\\ m^{-2}}$, sensible heat as\\n33${\\\\pm}$4 ${w\\\\ m^{-2}}$ and 36${\\\\pm}$5 ${w\\\\ m^{-2}}$, and evapotranspiration\\nas 75.6${\\\\pm}$10 ${\\\\times}$ 10$^3$ ${km^3\\\\ yr^{-1}}$ and 76${\\\\pm}$6 ${\\\\times}$\\n10$^3$ ${km^3\\\\ yr^{-1}}$. fluxcom products are suitable to quantify global\\nland-atmosphere interactions and benchmark land surface model simulations.\\n)\": False, 'contains(remote sensor design for visual recognition with convolutional neural\\n  networks\\n  while deep learning technologies for computer vision have developed rapidly\\nsince 2012, modeling of remote sensing systems has remained focused around\\nhuman vision. in particular, remote sensing systems are usually constructed to\\noptimize sensing cost-quality trade-offs with respect to human image\\ninterpretability. while some recent studies have explored remote sensing system\\ndesign as a function of simple computer vision algorithm performance, there has\\nbeen little work relating this design to the state-of-the-art in computer\\nvision: deep learning with convolutional neural networks. we develop\\nexperimental systems to conduct this analysis, showing results with modern deep\\nlearning algorithms and recent overhead image data. our results are compared to\\nstandard image quality measurements based on human visual perception, and we\\nconclude not only that machine and human interpretability differ significantly,\\nbut that computer vision performance is largely self-consistent across a range\\nof disparate conditions. this research is presented as a cornerstone for a new\\ngeneration of sensor design systems which focus on computer algorithm\\nperformance instead of human visual perception.\\n)': False, 'contains(prospects for the use of photosensor timing information with machine\\n  learning techniques in background rejection\\n  recent developments in machine learning (ml) techniques present a promising\\nnew analysis method for high-speed imaging in astroparticle physics\\nexperiments, for example with imaging atmospheric cherenkov telescopes (iacts).\\nin particular, the use of timing information with new machine learning\\ntechniques provides a novel method for event classification. previous work in\\nthis field has utilised images of the integrated charge from iact camera\\nphotomultipliers, but the majority of current and upcoming iact cameras have\\nthe capacity to read out the entire photosensor waveform following a trigger.\\nas the arrival times of cherenkov photons from extensive air showers (eas) at\\nthe camera plane are dependent upon the altitude of their emission, these\\nwaveforms contain information useful for iact event classification. in this\\nwork, we investigate the potential for using these waveforms with ml\\ntechniques, and find that a highly effective means of utilising their\\ninformation is to create a set of seven additional two dimensional histograms\\nof waveform parameters to be fed into the machine learning algorithm along with\\nthe integrated charge image. this appears to be superior to using only these\\nnew ml techniques with the waveform integrated charge alone. we also examine\\nthese timing-based ml techniques in the context of other experiments.\\n)': False, 'contains(data-driven air quality characterisation for urban environments: a case\\n  study\\n  the economic and social impact of poor air quality in towns and cities is\\nincreasingly being recognised, together with the need for effective ways of\\ncreating awareness of real-time air quality levels and their impact on human\\nhealth. with local authority maintained monitoring stations being\\ngeographically sparse and the resultant datasets also featuring missing labels,\\ncomputational data-driven mechanisms are needed to address the data sparsity\\nchallenge. in this paper, we propose a machine learning-based method to\\naccurately predict the air quality index (aqi), using environmental monitoring\\ndata together with meteorological measurements. to do so, we develop an air\\nquality estimation framework that implements a neural network that is enhanced\\nwith a novel non-linear autoregressive neural network with exogenous input\\n(narx), especially designed for time series prediction. the framework is\\napplied to a case study featuring different monitoring sites in london, with\\ncomparisons against other standard machine-learning based predictive algorithms\\nshowing the feasibility and robust performance of the proposed method for\\ndifferent kinds of areas within an urban region.\\n)': False, 'contains(machine learning reveals the state of intermittent frictional dynamics\\n  in a sheared granular fault\\n  seismogenic plate boundaries are presumed to behave in a similar manner to a\\ndensely packed granular medium, where fault and blocks systems rapidly\\nrearrange the distribution of forces within themselves, as particles do in\\nslowly sheared granular systems. we use machine learning and show that\\nstatistical features of velocity signals from individual particles in a\\nsimulated sheared granular fault contain information regarding the\\ninstantaneous global state of intermittent frictional stick-slip dynamics. we\\ndemonstrate that combining features built from the signals of more particles\\ncan improve the accuracy of the global model, and discuss the physical basis\\nbehind decrease in error. we show that the statistical features such as median\\nand higher moments of the signals that represent the particle displacement in\\nthe direction of shearing are among the best predictive features. our work\\nprovides novel insights into the applications of machine learning in studying\\nfrictional processes that take place in geophysical systems.\\n)': False, \"contains(a trans-disciplinary review of deep learning research for water\\n  resources scientists\\n  deep learning (dl), a new-generation of artificial neural network research,\\nhas transformed industries, daily lives and various scientific disciplines in\\nrecent years. dl represents significant progress in the ability of neural\\nnetworks to automatically engineer problem-relevant features and capture highly\\ncomplex data distributions. i argue that dl can help address several major new\\nand old challenges facing research in water sciences such as\\ninter-disciplinarity, data discoverability, hydrologic scaling, equifinality,\\nand needs for parameter regionalization. this review paper is intended to\\nprovide water resources scientists and hydrologists in particular with a simple\\ntechnical overview, trans-disciplinary progress update, and a source of\\ninspiration about the relevance of dl to water. the review reveals that various\\nphysical and geoscientific disciplines have utilized dl to address data\\nchallenges, improve efficiency, and gain scientific insights. dl is especially\\nsuited for information extraction from image-like data and sequential data.\\ntechniques and experiences presented in other disciplines are of high relevance\\nto water research. meanwhile, less noticed is that dl may also serve as a\\nscientific exploratory tool. a new area termed 'ai neuroscience,' where\\nscientists interpret the decision process of deep networks and derive insights,\\nhas been born. this budding sub-discipline has demonstrated methods including\\ncorrelation-based analysis, inversion of network-extracted features,\\nreduced-order approximations by interpretable models, and attribution of\\nnetwork decisions to inputs. moreover, dl can also use data to condition\\nneurons that mimic problem-specific fundamental organizing units, thus\\nrevealing emergent behaviors of these units. vast opportunities exist for dl to\\npropel advances in water sciences.\\n)\": False, 'contains(shape recognition of volcanic ash by simple convolutional neural network\\n  shape analyses of tephra grains result in understanding eruption mechanism of\\nvolcanoes. however, we have to define and select parameter set such as\\nconvexity for the precise discrimination of tephra grains. selection of the\\nbest parameter set for the recognition of tephra shapes is complicated.\\nactually, many shape parameters have been suggested. recently, neural network\\nhas made a great success in the field of machine learning. convolutional neural\\nnetwork can recognize the shape of images without human bias and shape\\nparameters. we applied the simple convolutional neural network developed for\\nthe handwritten digits to the recognition of tephra shapes. the network was\\ntrained by morphologi tephra images, and it can recognize the tephra shapes\\nwith approximately 90% of accuracy.\\n)': False, \"contains(characterizing the structural diversity of complex networks across\\n  domains\\n  the structure of complex networks has been of interest in many scientific and\\nengineering disciplines over the decades. a number of studies in the field have\\nbeen focused on finding the common properties among different kinds of networks\\nsuch as heavy-tail degree distribution, small-worldness and modular structure\\nand they have tried to establish a theory of structural universality in complex\\nnetworks. however, there is no comprehensive study of network structure across\\na diverse set of domains in order to explain the structural diversity we\\nobserve in the real-world networks. in this paper, we study 986 real-world\\nnetworks of diverse domains ranging from ecological food webs to online social\\nnetworks along with 575 networks generated from four popular network models.\\nour study utilizes a number of machine learning techniques such as random\\nforest and confusion matrix in order to show the relationships among network\\ndomains in terms of network structure. our results indicate that there are some\\npartitions of network categories in which networks are hard to distinguish\\nbased purely on network structure. we have found that these partitions of\\nnetwork categories tend to have similar underlying functions, constraints\\nand/or generative mechanisms of networks even though networks in the same\\npartition have different origins, e.g., biological processes, results of\\nengineering by human being, etc. this suggests that the origin of a network,\\nwhether it's biological, technological or social, may not necessarily be a\\ndecisive factor of the formation of similar network structure. our findings\\nshed light on the possible direction along which we could uncover the hidden\\nprinciples for the structural diversity of complex networks.\\n)\": False, 'contains(nonlinear unmixing of hyperspectral images: models and algorithms\\n  when considering the problem of unmixing hyperspectral images, most of the\\nliterature in the geoscience and image processing areas relies on the widely\\nused linear mixing model (lmm). however, the lmm may be not valid and other\\nnonlinear models need to be considered, for instance, when there are\\nmulti-scattering effects or intimate interactions. consequently, over the last\\nfew years, several significant contributions have been proposed to overcome the\\nlimitations inherent in the lmm. in this paper, we present an overview of\\nrecent advances in nonlinear unmixing modeling.\\n)': False, 'contains(combining machine learning techniques, microanalyses and large\\n  geochemical datasets for tephrochronological studies in complex volcanic\\n  areas: new age constraints for the pleistocene magmatism of central italy\\n  characterization, correlation and provenance determination of tephra samples\\nin sedimentary sections (tephrochronological studies) are powerful tools for\\nestablishing ages of depositional events, volcanic eruptions, and tephra\\ndispersion. despite the large literature and the advancements in this research\\nfield, the univocal attribution of tephra deposits to specific volcanic sources\\nremains too often elusive. in this contribution, we test the application of a\\nmachine learning technique named support vector machine to attempt shedding new\\nlight upon tephra deposits related to one of the most complex and debated\\nvolcanic regions on earth: the pliocene-pleistocene magmatism in italy. the\\nmachine learning algorithm was trained using one of the most comprehensive\\nglobal petrological databases (georoc); 17 chemical elements including major\\nand selected trace elements were chosen as input parameters. we first show the\\nability of support vector machines in discriminating among different\\npliocene-pleistocene volcanic provinces in italy and then apply the same\\nmethodology to determine the volcanic source of tephra samples occurring in the\\ncaio outcrop, an early pleistocene sedimentary section located in central\\nitaly.\\n)': False, 'contains(semantic referee: a neural-symbolic framework for enhancing geospatial\\n  semantic segmentation\\n  understanding why machine learning algorithms may fail is usually the task of\\nthe human expert that uses domain knowledge and contextual information to\\ndiscover systematic shortcomings in either the data or the algorithm. in this\\npaper, we propose a semantic referee, which is able to extract qualitative\\nfeatures of the errors emerging from deep machine learning frameworks and\\nsuggest corrections. the semantic referee relies on ontological reasoning about\\nspatial knowledge in order to characterize errors in terms of their spatial\\nrelations with the environment. using semantics, the reasoner interacts with\\nthe learning algorithm as a supervisor. in this paper, the proposed method of\\nthe interaction between a neural network classifier and a semantic referee\\nshows how to improve the performance of semantic segmentation for satellite\\nimagery data.\\n)': False, 'contains(deep autoassociative neural networks for noise reduction in seismic data\\n  machine learning is currently a trending topic in various science and\\nengineering disciplines, and the field of geophysics is no exception. with the\\nadvent of powerful computers, it is now possible to train the machine to learn\\ncomplex patterns in the data, which may not be easily realized using the\\ntraditional methods. among the various machine learning methods, the artificial\\nneural networks (anns) have received enormous attention. a variant of anns,\\nautoassociative neural network (autonn) tries to learn the reconstruction of\\ninput itself using backpropagation. in an autonn, the input and output are the\\nsame, and an approximation to the identity mapping is obtained in a nonlinear\\nsetting. autonns have primarily been used to extract sparse internal\\nrepresentations of any input and reduce its dimensionality. in this paper, we\\nexplore the potential of autonns in reducing random noise in geophysical data.\\nin this paper, the first results of this study are presented. the synthetic\\nmathematical example demonstrates the concept of autonn. for the test seismic\\ndata, it is observed that autonn can significantly remove the vertical time-\\nand frequency-local noise, however, the resolution of the output signal is\\ncompromised to a certain extent. future work includes testing larger examples\\nwith several different types of noise, and using deep-stacked-autonns to\\nfurther reduce the noise, ensuring minimal compromise with the resolution of\\nthe signal.\\n)': False, 'contains(deep learning for experimental hybrid terrestrial and satellite\\n  interference management\\n  interference management is a vast topic present in many disciplines. the\\nmajority of wireless standards suffer the drawback of interference intrusion\\nand the network efficiency drop due to that. traditionally, interference\\nmanagement has been addressed by proposing signal processing techniques that\\nminimize their effects locally. however, the fast evolution of future\\ncommunications makes difficult to adapt to new era. in this paper we propose\\nthe use of deep learning techniques to present a compact system for\\ninterference management. in particular, we describe two subsystems capable to\\ndetect the presence of interference, even in high signal to interference ratio\\n(sir), and interference classification in several radio standards. finally, we\\npresent results based on real signals captured from terrestrial and satellite\\nnetworks and the conclusions unveil the courageous future of ai and wireless\\ncommunications.\\n)': False, \"contains(deep learning electromagnetic inversion with convolutional neural\\n  networks\\n  geophysical inversion attempts to estimate the distribution of physical\\nproperties in the earth's interior from observations collected at or above the\\nsurface. inverse problems are commonly posed as least-squares optimization\\nproblems in high-dimensional parameter spaces. existing approaches are largely\\nbased on deterministic gradient-based methods, which are limited by\\nnonlinearity and nonuniqueness of the inverse problem. probabilistic inversion\\nmethods, despite their great potential in uncertainty quantification, still\\nremain a formidable computational task. in this paper, i explore the potential\\nof deep learning methods for electromagnetic inversion. this approach does not\\nrequire calculation of the gradient and provides results instantaneously. deep\\nneural networks based on fully convolutional architecture are trained on large\\nsynthetic datasets obtained by full 3-d simulations. the performance of the\\nmethod is demonstrated on models of strong practical relevance representing an\\nonshore controlled source electromagnetic co2 monitoring scenario. the\\npre-trained networks can reliably estimate the position and lateral dimensions\\nof the anomalies, as well as their resistivity properties. several fully\\nconvolutional network architectures are compared in terms of their accuracy,\\ngeneralization, and cost of training. examples with different survey geometry\\nand noise levels confirm the feasibility of the deep learning inversion,\\nopening the possibility to estimate the subsurface resistivity distribution in\\nreal time.\\n)\": False, \"contains(predicting the plant root-associated ecological niche of 21 pseudomonas\\n  species using machine learning and metabolic modeling\\n  plants rarely occur in isolated systems. bacteria can inhabit either the\\nendosphere, the region inside the plant root, or the rhizosphere, the soil\\nregion just outside the plant root. our goal is to understand if using genomic\\ndata and media dependent metabolic model information is better for training\\nmachine learning of predicting bacterial ecological niche than media\\nindependent models or pure genome based species trees. we considered three\\nmachine learning techniques: support vector machine, non-negative matrix\\nfactorization, and artificial neural networks. in all three machine-learning\\napproaches, the media-based metabolic models and flux balance analyses were\\nmore effective at predicting bacterial niche than the genome or prmt models.\\nsupport vector machine trained on a minimal media base with mannose, proline\\nand valine was most predictive of all models and media types with an f-score of\\n0.8 for rhizosphere and 0.97 for endosphere. thus we can conclude that\\nmedia-based metabolic modeling provides a holistic view of the metabolome,\\nallowing machine learning algorithms to highlight the differences between and\\ncategorize endosphere and rhizosphere bacteria. there was no single media type\\nthat best highlighted differences between endosphere and rhizosphere bacteria\\nmetabolism and therefore no single enzyme, reaction, or compound that defined\\nwhether a bacteria's origin was of the endosphere or rhizosphere.\\n)\": False, 'contains(hypothesis testing in speckled data with stochastic distances\\n  images obtained with coherent illumination, as is the case of sonar,\\nultrasound-b, laser and synthetic aperture radar -- sar, are affected by\\nspeckle noise which reduces the ability to extract information from the data.\\nspecialized techniques are required to deal with such imagery, which has been\\nmodeled by the g0 distribution and under which regions with different degrees\\nof roughness and mean brightness can be characterized by two parameters; a\\nthird parameter, the number of looks, is related to the overall signal-to-noise\\nratio. assessing distances between samples is an important step in image\\nanalysis; they provide grounds of the separability and, therefore, of the\\nperformance of classification procedures. this work derives and compares eight\\nstochastic distances and assesses the performance of hypothesis tests that\\nemploy them and maximum likelihood estimation. we conclude that tests based on\\nthe triangular distance have the closest empirical size to the theoretical one,\\nwhile those based on the arithmetic-geometric distances have the best power.\\nsince the power of tests based on the triangular distance is close to optimum,\\nwe conclude that the safest choice is using this distance for hypothesis\\ntesting, even when compared with classical distances as kullback-leibler and\\nbhattacharyya.\\n)': False, 'contains(large-scale sentiment analysis for news and blogs\\n\\nnewspapers and blogs express opinion of news entities (people, places, things) while reporting on recent events. we present a system that assigns scores indicating positive or negative opinion to each distinct entity in the text corpus. our system consists of a sentiment identi cation phase, which associates expressed opinions with each relevant entity, and a sentiment aggregation and scoring phase, which scores each entity relative to others in the same class. finally, we evaluate the signi cance of our scoring techniques over large corpus of news and blogs.\\n)': False, 'contains(somospie: a modular soil moisture spatial inference engine based on data\\n  driven decisions\\n  the current availability of soil moisture data over large areas comes from\\nsatellite remote sensing technologies (i.e., radar-based systems), but these\\ndata have coarse resolution and often exhibit large spatial information gaps.\\nwhere data are too coarse or sparse for a given need (e.g., precision\\nagriculture), one can leverage machine-learning techniques coupled with other\\nsources of environmental information (e.g., topography) to generate gap-free\\ninformation and at a finer spatial resolution (i.e., increased granularity). to\\nthis end, we develop a spatial inference engine consisting of modular stages\\nfor processing spatial environmental data, generating predictions with\\nmachine-learning techniques, and analyzing these predictions. we demonstrate\\nthe functionality of this approach and the effects of data processing choices\\nvia multiple prediction maps over a united states ecological region with a\\nhighly diverse soil moisture profile (i.e., the middle atlantic coastal\\nplains). the relevance of our work derives from a pressing need to improve the\\nspatial representation of soil moisture for applications in environmental\\nsciences (e.g., ecological niche modeling, carbon monitoring systems, and other\\nearth system models) and precision agriculture (e.g., optimizing irrigation\\npractices and other land management decisions).\\n)': False, 'contains(predictslums: a new model for identifying and predicting informal\\n  settlements and slums in cities from street intersections using machine\\n  learning\\n  identifying current and future informal regions within cities remains a\\ncrucial issue for policymakers and governments in developing countries. the\\ndelineation process of identifying such regions in cities requires a lot of\\nresources. while there are various studies that identify informal settlements\\nbased on satellite image classification, relying on both supervised or\\nunsupervised machine learning approaches, these models either require multiple\\ninput data to function or need further development with regards to precision.\\nin this paper, we introduce a novel method for identifying and predicting\\ninformal settlements using only street intersections data, regardless of the\\nvariation of urban form, number of floors, materials used for construction or\\nstreet width. with such minimal input data, we attempt to provide planners and\\npolicy-makers with a pragmatic tool that can aid in identifying informal zones\\nin cities. the algorithm of the model is based on spatial statistics and a\\nmachine learning approach, using multinomial logistic regression (mnl) and\\nartificial neural networks (ann). the proposed model relies on defining\\ninformal settlements based on two ubiquitous characteristics that these regions\\ntend to be filled in with smaller subdivided lots of housing relative to the\\nformal areas within the local context, and the paucity of services and\\ninfrastructure within the boundary of these settlements that require relatively\\nbigger lots. we applied the model in five major cities in egypt and india that\\nhave spatial structures in which informality is present. these cities are\\ngreater cairo, alexandria, hurghada and minya in egypt, and mumbai in india.\\nthe predictslums model shows high validity and accuracy for identifying and\\npredicting informality within the same city the model was trained on or in\\ndifferent ones of a similar context.\\n)': False, 'contains(deep learning to represent sub-grid processes in climate models\\n  the representation of nonlinear sub-grid processes, especially clouds, has\\nbeen a major source of uncertainty in climate models for decades.\\ncloud-resolving models better represent many of these processes and can now be\\nrun globally but only for short-term simulations of at most a few years because\\nof computational limitations. here we demonstrate that deep learning can be\\nused to capture many advantages of cloud-resolving modeling at a fraction of\\nthe computational cost. we train a deep neural network to represent all\\natmospheric sub-grid processes in a climate model by learning from a\\nmulti-scale model in which convection is treated explicitly. the trained neural\\nnetwork then replaces the traditional sub-grid parameterizations in a global\\ngeneral circulation model in which it freely interacts with the resolved\\ndynamics and the surface-flux scheme. the prognostic multi-year simulations are\\nstable and closely reproduce not only the mean climate of the cloud-resolving\\nsimulation but also key aspects of variability, including precipitation\\nextremes and the equatorial wave spectrum. furthermore, the neural network\\napproximately conserves energy despite not being explicitly instructed to.\\nfinally, we show that the neural network parameterization generalizes to new\\nsurface forcing patterns but struggles to cope with temperatures far outside\\nits training manifold. our results show the feasibility of using deep learning\\nfor climate model parameterization. in a broader context, we anticipate that\\ndata-driven earth system model development could play a key role in reducing\\nclimate prediction uncertainty in the coming decade.\\n)': False, 'contains(disaggregation of smap l3 brightness temperatures to 9km using kernel\\n  machines\\n  in this study, a machine learning algorithm is used for disaggregation of\\nsmap brightness temperatures (t$_{\\\\textrm{b}}$) from 36km to 9km. it uses image\\nsegmentation to cluster the study region based on meteorological and land cover\\nsimilarity, followed by a support vector machine based regression that computes\\nthe value of the disaggregated t$_{\\\\textrm{b}}$ at all pixels. high resolution\\nremote sensing products such as land surface temperature, normalized difference\\nvegetation index, enhanced vegetation index, precipitation, soil texture, and\\nland-cover were used for disaggregation. the algorithm was implemented in iowa,\\nunited states, from april to july 2015, and compared with the smap l3_sm_ap\\nt$_{\\\\textrm{b}}$ product at 9km. it was found that the disaggregated\\nt$_{\\\\textrm{b}}$ were very similar to the smap-t$_{\\\\textrm{b}}$ product, even\\nfor vegetated areas with a mean difference $\\\\leq$ 5k. however, the standard\\ndeviation of the disaggregation was lower by 7k than that of the ap product.\\nthe probability density functions of the disaggregated t$_{\\\\textrm{b}}$ were\\nsimilar to the smap-t$_{\\\\textrm{b}}$. the results indicate that this algorithm\\nmay be used for disaggregating t$_{\\\\textrm{b}}$ using complex non-linear\\ncorrelations on a grid.\\n)': False, 'contains(first-person perceptual guidance behavior decomposition using active\\n  constraint classification\\n  humans exhibit a wide range of adaptive and robust dynamic motion behavior\\nthat is yet unmatched by autonomous control systems. these capabilities are\\nessential for real-time behavior generation in cluttered environments. recent\\nwork suggests that human capabilities rely on task structure learning and\\nembedded or ecological cognition in the form of perceptual guidance. this paper\\ndescribes the experimental investigation of the functional elements of human\\nmotion guidance, focusing on the control and perceptual mechanisms. the motion,\\ncontrol, and perceptual data from first-person guidance experiments is\\ndecomposed into elemental segments based on invariants. these elements are then\\nanalyzed to determine their functional characteristics. the resulting model\\nexplains the structure of the agent-environment interaction and provides lawful\\ndescriptions of specific perceptual guidance and control mechanisms.\\n)': False, \"contains(learning dynamical systems from partial observations\\n  we consider the problem of forecasting complex, nonlinear space-time\\nprocesses when observations provide only partial information of on the system's\\nstate. we propose a natural data-driven framework, where the system's dynamics\\nare modelled by an unknown time-varying differential equation, and the\\nevolution term is estimated from the data, using a neural network. any future\\nstate can then be computed by placing the associated differential equation in\\nan ode solver. we first evaluate our approach on shallow water and euler\\nsimulations. we find that our method not only demonstrates high quality\\nlong-term forecasts, but also learns to produce hidden states closely\\nresembling the true states of the system, without direct supervision on the\\nlatter. additional experiments conducted on challenging, state of the art ocean\\nsimulations further validate our findings, while exhibiting notable\\nimprovements over classical baselines.\\n)\": False, 'contains(earthquake detection in 1-d time series data with feature selection and\\n  dictionary learning\\n  earthquakes can be detected by matching spatial patterns or phase properties\\nfrom 1-d seismic waves. current earthquake detection methods, such as waveform\\ncorrelation and template matching, have difficulty detecting anomalous\\nearthquakes that are not similar to other earthquakes. in recent years,\\nmachine-learning techniques for earthquake detection have been emerging as a\\nnew active research direction. in this paper, we develop a novel earthquake\\ndetection method based on dictionary learning. our detection method first\\ngenerates rich features via signal processing and statistical methods and\\nfurther employs feature selection techniques to choose features that carry the\\nmost significant information. based on these selected features, we build a\\ndictionary for classifying earthquake events from non-earthquake events. to\\nevaluate the performance of our dictionary-based detection methods, we test our\\nmethod on a labquake dataset from penn state university, which contains\\n3,357,566 time series data points with a 400 mhz sampling rate. 1,000\\nearthquake events are manually labeled in total, and the length of these\\nearthquake events varies from 74 to 7151 data points. through comparison to\\nother detection methods, we show that our feature selection and dictionary\\nlearning incorporated earthquake detection method achieves an 80.1% prediction\\naccuracy and outperforms the baseline methods in earthquake detection,\\nincluding template matching (tm) and support vector machine (svm).\\n)': False, \"contains(breaking cascadia's silence: machine learning reveals the constant\\n  chatter of the megathrust\\n  tectonic faults slip in various manners, ranging from ordinary earthquakes to\\nslow slip events to aseismic fault creep. the frequent occurrence of slow\\nearthquakes and their sensitivity to stress make them a promising probe of the\\nneighboring locked zone where megaquakes take place. this relationship,\\nhowever, remains poorly understood. we show that the cascadia megathrust is\\ncontinuously broadcasting a tremor-like signal that precisely informs of fault\\ndisplacement rate throughout the slow slip cycle. we posit that this signal\\nprovides indirect, real-time access to physical properties of the megathrust\\nand may ultimately reveal a connection between slow slip and megaquakes.\\n)\": False, 'contains(directivity modes of earthquake populations with unsupervised learning\\n  we present a novel approach for resolving modes of rupture directivity in\\nlarge populations of earthquakes. a seismic spectral decomposition technique is\\nused to first produce relative measurements of radiated energy for earthquakes\\nin a spatially-compact cluster. the azimuthal distribution of energy for each\\nearthquake is then assumed to result from one of several distinct modes of\\nrupture propagation. rather than fitting a kinematic rupture model to determine\\nthe most likely mode of rupture propagation, we instead treat the modes as\\nlatent variables and learn them with a gaussian mixture model. the mixture\\nmodel simultaneously determines the number of events that best identify with\\neach mode. the technique is demonstrated on four datasets in california with\\nseveral thousand earthquakes. we show that the datasets naturally decompose\\ninto distinct rupture propagation modes that correspond to different rupture\\ndirections, and the fault plane is unambiguously identified for all cases. we\\nfind that these small earthquakes exhibit unilateral ruptures 53-74% of the\\ntime on average. the results provide important observational constraints on the\\nphysics of earthquakes and faults.\\n)': False, 'contains(high-dimensional dependency structure learning for physical processes\\n  in this paper, we consider the use of structure learning methods for\\nprobabilistic graphical models to identify statistical dependencies in\\nhigh-dimensional physical processes. such processes are often synthetically\\ncharacterized using pdes (partial differential equations) and are observed in a\\nvariety of natural phenomena, including geoscience data capturing atmospheric\\nand hydrological phenomena. classical structure learning approaches such as the\\npc algorithm and variants are challenging to apply due to their high\\ncomputational and sample requirements. modern approaches, often based on sparse\\nregression and variants, do come with finite sample guarantees, but are usually\\nhighly sensitive to the choice of hyper-parameters, e.g., parameter $\\\\lambda$\\nfor sparsity inducing constraint or regularization. in this paper, we present\\naclime-admm, an efficient two-step algorithm for adaptive structure learning,\\nwhich estimates an edge specific parameter $\\\\lambda_{ij}$ in the first step,\\nand uses these parameters to learn the structure in the second step. both steps\\nof our algorithm use (inexact) admm to solve suitable linear programs, and all\\niterations can be done in closed form in an efficient block parallel manner. we\\ncompare aclime-admm with baselines on both synthetic data simulated by partial\\ndifferential equations (pdes) that model advection-diffusion processes, and\\nreal data (50 years) of daily global geopotential heights to study information\\nflow in the atmosphere. aclime-admm is shown to be efficient, stable, and\\ncompetitive, usually better than the baselines especially on difficult\\nproblems. on real data, aclime-admm recovers the underlying structure of global\\natmospheric circulation, including switches in wind directions at the equator\\nand tropics entirely from the data.\\n)': False, 'contains(evaluating aleatoric and epistemic uncertainties of time series deep learning models for soil moisture predictions\\n\\nsoil moisture is an important variable that determines floods, vegetation health, agriculture productivity, and land surface feedbacks to the atmosphere, etc. accurately modeling soil moisture has important implications in both weather and climate models. the recently available satellitebased observations give us a unique opportunity to build data-driven models to predict soil moisture instead of using land surface models, but previously there was no uncertainty estimate. we tested monte carlo dropout (mcd) with an aleatoric term for our long short-term memory models for this problem, and asked if the uncertainty terms behave as they were argued to. we show that the method successfully captures the predictive error after tuning a hyperparameter on a representative training dataset. we show the mcd uncertainty estimate, as previously argued, does detect dissimilarity.\\n)': False, 'contains(satellite imagery analysis for operational damage assessment in\\n  emergency situations\\n  when major disaster occurs the questions are raised how to estimate the\\ndamage in time to support the decision making process and relief efforts by\\nlocal authorities or humanitarian teams. in this paper we consider the use of\\nmachine learning and computer vision on remote sensing imagery to improve time\\nefficiency of assessment of damaged buildings in disaster affected area. we\\npropose a general workflow that can be useful in various disaster management\\napplications, and demonstrate the use of the proposed workflow for the\\nassessment of the damage caused by the wildfires in california in 2017.\\n)': False, \"contains(a new algorithm of speckle filtering using stochastic distances\\n  this paper presents a new approach for filter design based on stochastic\\ndistances and tests between distributions. a window is defined around each\\npixel, overlapping samples are compared and only those which pass a\\ngoodness-of-fit test are used to compute the filtered value. the technique is\\napplied to intensity sar data with homogeneous regions using the gamma model.\\nthe proposal is compared with the lee's filter using a protocol based on monte\\ncarlo. among the criteria used to quantify the quality of filters, we employ\\nthe equivalent number of looks, line and edge preservation. moreover, we also\\nassessed the filters by the universal image quality index and the pearson's\\ncorrelation on edges regions.\\n)\": False, 'contains(cascaded region-based densely connected network for event detection: a\\n  seismic application\\n  automatic event detection from time series signals has wide applications,\\nsuch as abnormal event detection in video surveillance and event detection in\\ngeophysical data. traditional detection methods detect events primarily by the\\nuse of similarity and correlation in data. those methods can be inefficient and\\nyield low accuracy. in recent years, because of the significantly increased\\ncomputational power, machine learning techniques have revolutionized many\\nscience and engineering domains. in this study, we apply a deep-learning-based\\nmethod to the detection of events from time series seismic signals. however, a\\ndirect adaptation of the similar ideas from 2d object detection to our problem\\nfaces two challenges. the first challenge is that the duration of earthquake\\nevent varies significantly; the other is that the proposals generated are\\ntemporally correlated. to address these challenges, we propose a novel cascaded\\nregion-based convolutional neural network to capture earthquake events in\\ndifferent sizes, while incorporating contextual information to enrich features\\nfor each individual proposal. to achieve a better generalization performance,\\nwe use densely connected blocks as the backbone of our network. because of the\\nfact that some positive events are not correctly annotated, we further\\nformulate the detection problem as a learning-from-noise problem. to verify the\\nperformance of our detection methods, we employ our methods to seismic data\\ngenerated from a bi-axial \"earthquake machine\" located at rock mechanics\\nlaboratory, and we acquire labels with the help of experts. through our\\nnumerical tests, we show that our novel detection techniques yield high\\naccuracy. therefore, our novel deep-learning-based detection methods can\\npotentially be powerful tools for locating events from time series data in\\nvarious applications.\\n)': False, 'contains(a machine learning approach to forecasting remotely sensed vegetation\\n  health\\n  drought threatens food and water security around the world, and this threat\\nis likely to become more severe under climate change. high resolution\\npredictive information can help farmers, water managers, and others to manage\\nthe effects of drought. we have created an open source tool to produce\\nshort-term forecasts of vegetation health at high spatial resolution, using\\ndata that are global in coverage. the tool automates downloading and processing\\nmoderate resolution imaging spectroradiometer (modis) datasets, and training\\ngradient-boosted machine models on hundreds of millions of observations to\\npredict future values of the enhanced vegetation index. we compared the\\npredictive power of different sets of variables (raw spectral modis data and\\nlevel-3 modis products) in two regions with distinct agro-ecological systems,\\nclimates, and cloud coverage: sri lanka and california. our tool provides\\nconsiderably greater predictive power on held-out datasets than simpler\\nbaseline models.\\n)': False, 'contains(learning representations for counterfactual inference\\n  observational studies are rising in importance due to the widespread\\naccumulation of data in fields such as healthcare, education, employment and\\necology. we consider the task of answering counterfactual questions such as,\\n\"would this patient have lower blood sugar had she received a different\\nmedication?\". we propose a new algorithmic framework for counterfactual\\ninference which brings together ideas from domain adaptation and representation\\nlearning. in addition to a theoretical justification, we perform an empirical\\ncomparison with previous approaches to causal inference from observational\\ndata. our deep learning algorithm significantly outperforms the previous\\nstate-of-the-art.\\n)': False, 'contains(learning to label seismic structures with deconvolution networks and\\n  weak labels\\n  recently, there has been increasing interest in using deep learning\\ntechniques for various seismic interpretation tasks. however, unlike shallow\\nmachine learning models, deep learning models are often far more complex and\\ncan have hundreds of millions of free parameters. this not only means that\\nlarge amounts of computational resources are needed to train these models, but\\nmore critically, they require vast amounts of labeled training data as well. in\\nthis work, we show how automatically-generated weak labels can be effectively\\nused to overcome this problem and train powerful deep learning models for\\nlabeling seismic structures in large seismic volumes. to achieve this, we\\nautomatically generate thousands of weak labels and use them to train a\\ndeconvolutional network for labeling fault, salt dome, and chaotic regions\\nwithin the netherlands f3 block. furthermore, we show how modifying the loss\\nfunction to take into account the weak training labels helps reduce false\\npositives in the labeling results. the benefit of this work is that it enables\\nthe effective training and deployment of deep learning models to various\\nseismic interpretation tasks without requiring any manual labeling effort. we\\nshow excellent results on the netherlands f3 block, and show how our model\\noutperforms other baseline models.\\n)': False, \"contains(monitoring spatial sustainable development: semi-automated analysis of\\n  satellite and aerial images for energy transition and sustainability\\n  indicators\\n  solar panels are installed by a large and growing number of households due to\\nthe convenience of having cheap and renewable energy to power house appliances.\\nin contrast to other energy sources solar installations are distributed very\\ndecentralized and spread over hundred-thousands of locations. on a global level\\nmore than 25% of solar photovoltaic (pv) installations were decentralized. the\\neffect of the quick energy transition from a carbon based economy to a green\\neconomy is though still very difficult to quantify. as a matter of fact the\\nquick adoption of solar panels by households is difficult to track, with local\\nregistries that miss a large number of the newly built solar panels. this makes\\nthe task of assessing the impact of renewable energies an impossible task.\\nalthough models of the output of a region exist, they are often black box\\nestimations. this project's aim is twofold: first automate the process to\\nextract the location of solar panels from aerial or satellite images and\\nsecond, produce a map of solar panels along with statistics on the number of\\nsolar panels. further, this project takes place in a wider framework which\\ninvestigates how official statistics can benefit from new digital data sources.\\nat project completion, a method for detecting solar panels from aerial images\\nvia machine learning will be developed and the methodology initially developed\\nfor be, de and nl will be standardized for application to other eu countries.\\nin practice, machine learning techniques are used to identify solar panels in\\nsatellite and aerial images for the province of limburg (nl), flanders (be) and\\nnorth rhine-westphalia (de).\\n)\": False, 'contains(signal-based bayesian seismic monitoring\\n  detecting weak seismic events from noisy sensors is a difficult perceptual\\ntask. we formulate this task as bayesian inference and propose a generative\\nmodel of seismic events and signals across a network of spatially distributed\\nstations. our system, sigvisa, is the first to directly model seismic\\nwaveforms, allowing it to incorporate a rich representation of the physics\\nunderlying the signal generation process. we use gaussian processes over\\nwavelet parameters to predict detailed waveform fluctuations based on\\nhistorical events, while degrading smoothly to simple parametric envelopes in\\nregions with no historical seismicity. evaluating on data from the western us,\\nwe recover three times as many events as previous work, and reduce mean\\nlocation errors by a factor of four while greatly increasing sensitivity to\\nlow-magnitude events.\\n)': False, 'contains(space navigator: a tool for the optimization of collision avoidance\\n  maneuvers\\n  the number of space objects will grow several times in a few years due to the\\nplanned launches of constellations of thousands microsatellites. it leads to a\\nsignificant increase in the threat of satellite collisions. spacecraft must\\nundertake collision avoidance maneuvers to mitigate the risk. according to\\npublicly available information, conjunction events are now manually handled by\\noperators on the earth. the manual maneuver planning requires qualified\\npersonnel and will be impractical for constellations of thousands satellites.\\nin this paper we propose a new modular autonomous collision avoidance system\\ncalled \"space navigator\". it is based on a novel maneuver optimization approach\\nthat combines domain knowledge with reinforcement learning methods.\\n)': False, 'contains(information-theoretic methods for identifying relationships among\\n  climate variables\\n  information-theoretic quantities, such as entropy, are used to quantify the\\namount of information a given variable provides. entropies can be used together\\nto compute the mutual information, which quantifies the amount of information\\ntwo variables share. however, accurately estimating these quantities from data\\nis extremely challenging. we have developed a set of computational techniques\\nthat allow one to accurately compute marginal and joint entropies. these\\nalgorithms are probabilistic in nature and thus provide information on the\\nuncertainty in our estimates, which enable us to establish statistical\\nsignificance of our findings. we demonstrate these methods by identifying\\nrelations between cloud data from the international satellite cloud climatology\\nproject (isccp) and data from other sources, such as equatorial pacific sea\\nsurface temperatures (sst).\\n)': False, \"contains(forest-based methods and ensemble model output statistics for rainfall\\n  ensemble forecasting\\n  rainfall ensemble forecasts have to be skillful for both low precipitation\\nand extreme events. we present statistical post-processing methods based on\\nquantile regression forests (qrf) and gradient forests (gf) with a parametric\\nextension for heavy-tailed distributions. our goal is to improve ensemble\\nquality for all types of precipitation events, heavy-tailed included, subject\\nto a good overall performance. our hybrid proposed methods are applied to daily\\n51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over france\\nusing the m{\\\\'e}t{\\\\'e}o-france ensemble prediction system called pearp. they\\nprovide calibrated pre-dictive distributions and compete favourably with\\nstate-of-the-art methods like analogs method or ensemble model output\\nstatistics. in particular, hybrid forest-based procedures appear to bring an\\nadded value to the forecast of heavy rainfall.\\n)\": False, 'contains(fusion of heterogeneous earth observation data for the classification of\\n  local climate zones\\n  this paper proposes a novel framework for fusing multi-temporal,\\nmultispectral satellite images and openstreetmap (osm) data for the\\nclassification of local climate zones (lczs). feature stacking is the most\\ncommonly-used method of data fusion but does not consider the heterogeneity of\\nmultimodal optical images and osm data, which becomes its main drawback. the\\nproposed framework processes two data sources separately and then combines them\\nat the model level through two fusion models (the landuse fusion model and\\nbuilding fusion model), which aim to fuse optical images with landuse and\\nbuildings layers of osm data, respectively. in addition, a new approach to\\ndetecting building incompleteness of osm data is proposed. the proposed\\nframework was trained and tested using data from the 2017 ieee grss data fusion\\ncontest, and further validated on one additional test set containing test\\nsamples which are manually labeled in munich and new york. experimental results\\nhave indicated that compared to the feature stacking-based baseline framework\\nthe proposed framework is effective in fusing optical images with osm data for\\nthe classification of lczs with high generalization capability on a large\\nscale. the classification accuracy of the proposed framework outperforms the\\nbaseline framework by more than 6% and 2%, while testing on the test set of\\n2017 ieee grss data fusion contest and the additional test set, respectively.\\nin addition, the proposed framework is less sensitive to spectral diversities\\nof optical satellite images and thus achieves more stable classification\\nperformance than state-of-the art frameworks.\\n)': False, 'contains(airpred: a flexible r package implementing methods for predicting air\\n  pollution\\n  fine particulate matter (pm$_{2.5}$) is one of the criteria air pollutants\\nregulated by the environmental protection agency in the united states. there is\\nstrong evidence that ambient exposure to (pm$_{2.5}$) increases risk of\\nmortality and hospitalization. large scale epidemiological studies on the\\nhealth effects of pm$_{2.5}$ provide the necessary evidence base for lowering\\nthe safety standards and inform regulatory policy. however, ambient monitors of\\npm$_{2.5}$ (as well as monitors for other pollutants) are sparsely located\\nacross the u.s., and therefore studies based only on the levels of pm$_{2.5}$\\nmeasured from the monitors would inevitably exclude large amounts of the\\npopulation. one approach to resolving this issue has been developing models to\\npredict local pm$_{2.5}$, no$_2$, and ozone based on satellite, meteorological,\\nand land use data. this process typically relies developing a prediction model\\nthat relies on large amounts of input data and is highly computationally\\nintensive to predict levels of air pollution in unmonitored areas. we have\\ndeveloped a flexible r package that allows for environmental health researchers\\nto design and train spatio-temporal models capable of predicting multiple\\npollutants, including pm$_{2.5}$. we utilize h2o, an open source big data\\nplatform, to achieve both performance and scalability when used in conjunction\\nwith cloud or cluster computing systems.\\n)': False, \"contains(bayesian deep learning for exoplanet atmospheric retrieval\\n  over the past decade, the study of extrasolar planets has evolved rapidly\\nfrom plain detection and identification to comprehensive categorization and\\ncharacterization of exoplanet systems and their atmospheres. atmospheric\\nretrieval, the inverse modeling technique used to determine an exoplanetary\\natmosphere's temperature structure and composition from an observed spectrum,\\nis both time-consuming and compute-intensive, requiring complex algorithms that\\ncompare thousands to millions of atmospheric models to the observational data\\nto find the most probable values and associated uncertainties for each model\\nparameter. for rocky, terrestrial planets, the retrieved atmospheric\\ncomposition can give insight into the surface fluxes of gaseous species\\nnecessary to maintain the stability of that atmosphere, which may in turn\\nprovide insight into the geological and/or biological processes active on the\\nplanet. these atmospheres contain many molecules, some of them biosignatures,\\nspectral fingerprints indicative of biological activity, which will become\\nobservable with the next generation of telescopes. runtimes of traditional\\nretrieval models scale with the number of model parameters, so as more\\nmolecular species are considered, runtimes can become prohibitively long.\\nrecent advances in machine learning (ml) and computer vision offer new ways to\\nreduce the time to perform a retrieval by orders of magnitude, given a\\nsufficient data set to train with. here we present an ml-based retrieval\\nframework called intelligent exoplanet atmospheric retrieval (inara) that\\nconsists of a bayesian deep learning model for retrieval and a data set of\\n3,000,000 synthetic rocky exoplanetary spectra generated using the nasa\\nplanetary spectrum generator. our work represents the first ml retrieval model\\nfor rocky, terrestrial exoplanets and the first synthetic data set of\\nterrestrial spectra generated at this scale.\\n)\": False, 'contains(a deep neural network to identify foreshocks in real time\\n  foreshock events provide valuable insight to predict imminent major\\nearthquakes. however, it is difficult to identify them in real time. in this\\npaper, i propose an algorithm based on deep learning to instantaneously\\nclassify a seismic waveform as a foreshock, mainshock or an aftershock event\\nachieving a high accuracy of 99% in classification. as a result, this is by far\\nthe most reliable method to predict major earthquakes that are preceded by\\nforeshocks. in addition, i discuss methods to create an earthquake dataset that\\nis compatible with deep networks.\\n)': False, \"contains(mapping informal settlements in developing countries with\\n  multi-resolution, multi-spectral data\\n  detecting and mapping informal settlements encompasses several of the united\\nnations sustainable development goals. this is because informal settlements are\\nhome to the most socially and economically vulnerable people on the planet.\\nthus, understanding where these settlements are is of paramount importance to\\nboth government and non-government organizations (ngos), such as the united\\nnations children's fund (unicef), who can use this information to deliver\\neffective social and economic aid. we propose two effective methods for\\ndetecting and mapping the locations of informal settlements. one uses only\\nlow-resolution (lr), freely available, sentinel-2 multispectral satellite\\nimagery with noisy annotations, whilst the other is a deep learning approach\\nthat uses only costly very-high-resolution (vhr) satellite imagery. to our\\nknowledge, we are the first to map informal settlements successfully with\\nlow-resolution satellite imagery. we extensively evaluate and compare the\\nproposed methods. please find additional material at\\nhttps://frontierdevelopmentlab.github.io/informal-settlements/.\\n)\": False, 'contains(recurrent neural networks for multivariate time series with missing\\n  values\\n  multivariate time series data in practical applications, such as health care,\\ngeoscience, and biology, are characterized by a variety of missing values. in\\ntime series prediction and other related tasks, it has been noted that missing\\nvalues and their missing patterns are often correlated with the target labels,\\na.k.a., informative missingness. there is very limited work on exploiting the\\nmissing patterns for effective imputation and improving prediction performance.\\nin this paper, we develop novel deep learning models, namely gru-d, as one of\\nthe early attempts. gru-d is based on gated recurrent unit (gru), a\\nstate-of-the-art recurrent neural network. it takes two representations of\\nmissing patterns, i.e., masking and time interval, and effectively incorporates\\nthem into a deep model architecture so that it not only captures the long-term\\ntemporal dependencies in time series, but also utilizes the missing patterns to\\nachieve better prediction results. experiments of time series classification\\ntasks on real-world clinical datasets (mimic-iii, physionet) and synthetic\\ndatasets demonstrate that our models achieve state-of-the-art performance and\\nprovides useful insights for better understanding and utilization of missing\\nvalues in time series analysis.\\n)': False, 'contains(theory-guided data science: a new paradigm for scientific discovery from\\n  data\\n  data science models, although successful in a number of commercial domains,\\nhave had limited applicability in scientific problems involving complex\\nphysical phenomena. theory-guided data science (tgds) is an emerging paradigm\\nthat aims to leverage the wealth of scientific knowledge for improving the\\neffectiveness of data science models in enabling scientific discovery. the\\noverarching vision of tgds is to introduce scientific consistency as an\\nessential component for learning generalizable models. further, by producing\\nscientifically interpretable models, tgds aims to advance our scientific\\nunderstanding by discovering novel domain insights. indeed, the paradigm of\\ntgds has started to gain prominence in a number of scientific disciplines such\\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\\nscience, bio-marker discovery, climate science, and hydrology. in this paper,\\nwe formally conceptualize the paradigm of tgds and present a taxonomy of\\nresearch themes in tgds. we describe several approaches for integrating domain\\nknowledge in different research themes using illustrative examples from\\ndifferent disciplines. we also highlight some of the promising avenues of novel\\nresearch for realizing the full potential of theory-guided data science.\\n)': False, 'contains(gaussian process regression for forest attribute estimation from\\n  airborne laser scanning data\\n  while the analysis of airborne laser scanning (als) data often provides\\nreliable estimates for certain forest stand attributes -- such as total volume\\nor basal area -- there is still room for improvement, especially in estimating\\nspecies-specific attributes. moreover, while information on the estimate\\nuncertainty would be useful in various economic and environmental analyses on\\nforests, a computationally feasible framework for uncertainty quantifying in\\nals is still missing. in this article, the species-specific stand attribute\\nestimation and uncertainty quantification (uq) is approached using gaussian\\nprocess regression (gpr), which is a nonlinear and nonparametric machine\\nlearning method. multiple species-specific stand attributes are estimated\\nsimultaneously: tree height, stem diameter, stem number, basal area, and stem\\nvolume. the cross-validation results show that gpr yields on average an\\nimprovement of 4.6\\\\% in estimate rmse over a state-of-the-art k-nearest\\nneighbors (knn) implementation, negligible bias and well performing uq\\n(credible intervals), while being computationally fast. the performance\\nadvantage over knn and the feasibility of credible intervals persists even when\\nsmaller training sets are used.\\n)': False, 'contains(the data big bang and the expanding digital universe: high-dimensional,\\n  complex and massive data sets in an inflationary epoch\\n  recent and forthcoming advances in instrumentation, and giant new surveys,\\nare creating astronomical data sets that are not amenable to the methods of\\nanalysis familiar to astronomers. traditional methods are often inadequate not\\nmerely because of the size in bytes of the data sets, but also because of the\\ncomplexity of modern data sets. mathematical limitations of familiar algorithms\\nand techniques in dealing with such data sets create a critical need for new\\nparadigms for the representation, analysis and scientific visualization (as\\nopposed to illustrative visualization) of heterogeneous, multiresolution data\\nacross application domains. some of the problems presented by the new data sets\\nhave been addressed by other disciplines such as applied mathematics,\\nstatistics and machine learning and have been utilized by other sciences such\\nas space-based geosciences. unfortunately, valuable results pertaining to these\\nproblems are mostly to be found only in publications outside of astronomy. here\\nwe offer brief overviews of a number of concepts, techniques and developments,\\nsome \"old\" and some new. these are generally unknown to most of the\\nastronomical community, but are vital to the analysis and visualization of\\ncomplex datasets and images. in order for astronomers to take advantage of the\\nrichness and complexity of the new era of data, and to be able to identify,\\nadopt, and apply new solutions, the astronomical community needs a certain\\ndegree of awareness and understanding of the new concepts. one of the goals of\\nthis paper is to help bridge the gap between applied mathematics, artificial\\nintelligence and computer science on the one side and astronomy on the other.\\n)': False, 'contains(contextualized spatial-temporal network for taxi origin-destination\\n  demand prediction\\n  taxi demand prediction has recently attracted increasing research interest\\ndue to its huge potential application in large-scale intelligent transportation\\nsystems. however, most of the previous methods only considered the taxi demand\\nprediction in origin regions, but neglected the modeling of the specific\\nsituation of the destination passengers. we believe it is suboptimal to\\npreallocate the taxi into each region based solely on the taxi origin demand.\\nin this paper, we present a challenging and worth-exploring task, called taxi\\norigin-destination demand prediction, which aims at predicting the taxi demand\\nbetween all region pairs in a future time interval. its main challenges come\\nfrom how to effectively capture the diverse contextual information to learn the\\ndemand patterns. we address this problem with a novel contextualized\\nspatial-temporal network (cstn), which consists of three components for the\\nmodeling of local spatial context (lsc), temporal evolution context (tec) and\\nglobal correlation context (gcc) respectively. firstly, an lsc module utilizes\\ntwo convolution neural networks to learn the local spatial dependencies of taxi\\ndemand respectively from the origin view and the destination view. secondly, a\\ntec module incorporates both the local spatial features of taxi demand and the\\nmeteorological information to a convolutional long short-term memory network\\n(convlstm) for the analysis of taxi demand evolution. finally, a gcc module is\\napplied to model the correlation between all regions by computing a global\\ncorrelation feature as a weighted sum of all regional features, with the\\nweights being calculated as the similarity between the corresponding region\\npairs. extensive experiments and evaluations on a large-scale dataset well\\ndemonstrate the superiority of our cstn over other compared methods for taxi\\norigin-destination demand prediction.\\n)': False, 'contains(aid: a benchmark dataset for performance evaluation of aerial scene\\n  classification\\n  aerial scene classification, which aims to automatically label an aerial\\nimage with a specific semantic category, is a fundamental problem for\\nunderstanding high-resolution remote sensing imagery. in recent years, it has\\nbecome an active task in remote sensing area and numerous algorithms have been\\nproposed for this task, including many machine learning and data-driven\\napproaches. however, the existing datasets for aerial scene classification like\\nuc-merced dataset and whu-rs19 are with relatively small sizes, and the results\\non them are already saturated. this largely limits the development of scene\\nclassification algorithms. this paper describes the aerial image dataset (aid):\\na large-scale dataset for aerial scene classification. the goal of aid is to\\nadvance the state-of-the-arts in scene classification of remote sensing images.\\nfor creating aid, we collect and annotate more than ten thousands aerial scene\\nimages. in addition, a comprehensive review of the existing aerial scene\\nclassification techniques as well as recent widely-used deep learning methods\\nis given. finally, we provide a performance analysis of typical aerial scene\\nclassification and deep learning approaches on aid, which can be served as the\\nbaseline results on this benchmark.\\n)': False, 'contains(learning argo profiles by using the signature method\\n  a profile from the argo ocean observing array is a sequence of\\nthree-dimensional vectors composed of pressure, salinity, and temperature,\\nappearing as a continuous curve in three-dimensional space.the shape of such\\ncurve is faithfully represented by a path signature, a collection of all the\\niterated integrals. moreover, the product of two terms of the signature of a\\npath can be expressed as a sum of higher order terms. thanks to this algebraic\\nproperty,a nonlinear function of profile shape can always be represented by a\\nweighted linear combination of the iterated integrals,which makes easy to\\nperform machine learning of a complicated function of the profile shape. in\\nthis study, we perform a supervised learning of existing argo data with quality\\ncontrol flags using the signature method, and demonstrate the prediction skill\\nby cross-validation. this technique should be a key to realizing an automatic\\nquality control of the argo profile data.\\n)': False, 'contains(neural networks for post-processing ensemble weather forecasts\\n  ensemble weather predictions require statistical post-processing of\\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\\ntraditionally, this is accomplished with distributional regression models in\\nwhich the parameters of a predictive distribution are estimated from a training\\nperiod. we propose a flexible alternative based on neural networks that can\\nincorporate nonlinear relationships between arbitrary predictor variables and\\nforecast distribution parameters that are automatically learned in a\\ndata-driven way rather than requiring pre-specified link functions. in a case\\nstudy of 2-meter temperature forecasts at surface stations in germany, the\\nneural network approach significantly outperforms benchmark post-processing\\nmethods while being computationally more affordable. key components to this\\nimprovement are the use of auxiliary predictor variables and station-specific\\ninformation with the help of embeddings. furthermore, the trained neural\\nnetwork can be used to gain insight into the importance of meteorological\\nvariables thereby challenging the notion of neural networks as uninterpretable\\nblack boxes. our approach can easily be extended to other statistical\\npost-processing and forecasting problems. we anticipate that recent advances in\\ndeep learning combined with the ever-increasing amounts of model and\\nobservation data will transform the post-processing of numerical weather\\nforecasts in the coming decade.\\n)': False, 'contains(identification of synoptic weather types over taiwan area with multiple\\n  classifiers\\n  in this study, a novel machine learning approach was used to classify three\\ntypes of synoptic weather events in taiwan area from 2001 to 2010. we used\\nreanalysis data with three machine learning algorithms to recognize weather\\nsystems and evaluated their performance. overall, the classifiers successfully\\nidentified 52-83% of weather events (hit rate), which is higher than the\\nperformance of traditional objective methods. the results showed that the\\nmachine learning approach gave low false alarm rate in general, while the\\nsupport vector machine (svm) with more principal components of reanalysis data\\nhad higher hit rate on all tested weather events. the sensitivity tests of grid\\ndata resolution indicated that the differences between the high- and\\nlow-resolution datasets are limited, which implied that the proposed method can\\nachieve reasonable performance in weather forecasting with minimal resources.\\nby identifying daily weather systems in historical reanalysis data, this method\\ncan be used to study long-term weather changes, to monitor climatological-scale\\nvariations, and to provide a better estimate of climate projections.\\nfurthermore, this method can also serve as an alternative to model output\\nstatistics and potentially be used for synoptic weather forecasting.\\n)': False, 'contains(machine learning materials physics: surrogate optimization and\\n  multi-fidelity algorithms predict precipitate morphology in an alternative to\\n  phase field dynamics\\n  machine learning has been effective at detecting patterns and predicting the\\nresponse of systems that behave free of natural laws. examples include learning\\ncrowd dynamics, recommender systems and autonomous mobility. there also have\\nbeen applications to the search for new materials that bear relations to big\\ndata classification problems. however, when it comes to physical systems\\ngoverned by conservation laws, the role of machine learning has been more\\nlimited. here, we present our recent work in exploring the role of machine\\nlearning methods in discovering, or aiding, the search for physics.\\nspecifically, we focus on using machine learning algorithms to represent\\nhigh-dimensional free energy surfaces with the goal of identifying precipitate\\nmorphologies in alloy systems. traditionally, this problem has been approached\\nby combining phase field models, which impose first-order dynamics, with\\nelasticity, to traverse a free energy landscape in search of minima.\\nequilibrium precipitate morphologies occur at these minima. here, we exploit\\nthe machine learning methods to represent high-dimensional data, combined with\\nsurrogate optimization, sensitivity analysis and multifidelity modelling as an\\nalternate framework to explore phenomena controlled by energy extremization.\\nthis combination of data-driven methods offers an alternative to the imposition\\nof first-order dynamics via phase field methods, and represents one approach to\\nmachine learning materials physics.\\n)': False, \"contains(presence-absence estimation in audio recordings of tropical frog\\n  communities\\n  one non-invasive way to study frog communities is by analyzing long-term\\nsamples of acoustic material containing calls. this immense task has been\\noptimized by the development of machine learning tools to extract ecological\\ninformation. we explored a likelihood-ratio audio detector based on gaussian\\nmixture model classification of 10 frog species, and applied it to estimate\\npresence-absence in audio recordings from an actual amphibian monitoring\\nperformed at yasun\\\\'i national park in the ecuadorian amazonia. a modified\\nfilter-bank was used to extract 20 cepstral features that model the spectral\\ncontent of frog calls. experiments were carried out to investigate the\\nhyperparameters and the minimum frog-call time needed to train an accurate gmm\\nclassifier. with 64 gaussians and 12 seconds of training time, the classifier\\nachieved an average weighted error rate of 0.9% on the 10-fold cross-validation\\nfor nine species classification, as compared to 3% with mfcc and 1.8% with plp\\nfeatures. for testing, 10 gmms were trained using all the available\\ntraining-validation dataset to study 23.5 hours in 141, 10-minute long samples\\nof unidentified real-world audio recorded at two frog communities in 2001 with\\nanalog equipment. to evaluate automatic presence-absence estimation, we\\ncharacterized the audio samples with 10 binary variables each corresponding to\\na frog species, and manually labeled a sub-set of 18 samples using headphones.\\na recall of 87.5% and precision of 100% with average accuracy of 96.66%\\nsuggests good generalization ability of the algorithm, and provides evidence of\\nthe validity of this approach to study real-world audio recorded in a tropical\\nacoustic environment. finally, we applied the algorithm to the available\\ncorpus, and show its potentiality to gain insights into the temporal\\nreproductive behavior of frogs.\\n)\": False, 'contains(improving subseasonal forecasting in the western u.s. with machine\\n  learning\\n  water managers in the western united states (u.s.) rely on longterm forecasts\\nof temperature and precipitation to prepare for droughts and other wet weather\\nextremes. to improve the accuracy of these longterm forecasts, the u.s. bureau\\nof reclamation and the national oceanic and atmospheric administration (noaa)\\nlaunched the subseasonal climate forecast rodeo, a year-long real-time\\nforecasting challenge in which participants aimed to skillfully predict\\ntemperature and precipitation in the western u.s. two to four weeks and four to\\nsix weeks in advance. here we present and evaluate our machine learning\\napproach to the rodeo and release our subseasonalrodeo dataset, collected to\\ntrain and evaluate our forecasting system.\\n  our system is an ensemble of two regression models. the first integrates the\\ndiverse collection of meteorological measurements and dynamic model forecasts\\nin the subseasonalrodeo dataset and prunes irrelevant predictors using a\\ncustomized multitask model selection procedure. the second uses only historical\\nmeasurements of the target variable (temperature or precipitation) and\\nintroduces multitask nearest neighbor features into a weighted local linear\\nregression. each model alone is significantly more accurate than the debiased\\noperational u.s. climate forecasting system (cfsv2), and our ensemble skill\\nexceeds that of the top rodeo competitor for each target variable and forecast\\nhorizon. moreover, over 2011-2018, an ensemble of our regression models and\\ndebiased cfsv2 improves debiased cfsv2 skill by 40-50% for temperature and\\n129-169% for precipitation. we hope that both our dataset and our methods will\\nhelp to advance the state of the art in subseasonal forecasting.\\n)': False, 'contains(day-ahead hail prediction integrating machine learning with storm-scale numerical weather models\\n\\nhail causes billions of dollars in losses by damaging buildings, vehicles, and crops. improving the spatial and temporal accuracy of hail forecasts would allow people to mitigate hail damage. we have developed an approach to forecasting hail that identifies potential hail storms in storm-scale numerical weather prediction models and matches them with observed hailstorms. machine learning models, including random forests, gradient boosting trees, and linear regression, are used to predict the expected hail size from each forecast storm. the individual hail size forecasts are merged with a spatial neighborhood ensemble probability technique to produce a consensus probability of hail at least 25.4 mm in diameter. the system was evaluated during the 2014 national oceanic and atmospheric administration hazardous weather testbed experimental forecast program and compared with a physics-based hail size model. the machine-learning-based technique shows advantages in producing smaller size errors and more reliable probability forecasts. the machine learning approaches correctly predicted the location and extent of a significant hail event in eastern nebraska and a marginal severe hail event in colorado.\\n)': False, \"contains(empirical evaluation of a q-learning algorithm for model-free autonomous\\n  soaring\\n  autonomous unpowered flight is a challenge for control and guidance systems:\\nall the energy the aircraft might use during flight has to be harvested\\ndirectly from the atmosphere. we investigate the design of an algorithm that\\noptimizes the closed-loop control of a glider's bank and sideslip angles, while\\nflying in the lower convective layer of the atmosphere in order to increase its\\nmission endurance. using a reinforcement learning approach, we demonstrate the\\npossibility for real-time adaptation of the glider's behaviour to the\\ntime-varying and noisy conditions associated with thermal soaring flight. our\\napproach is online, data-based and model-free, hence avoids the pitfalls of\\naerological and aircraft modelling and allow us to deal with uncertainties and\\nnon-stationarity. additionally, we put a particular emphasis on keeping low\\ncomputational requirements in order to make on-board execution feasible. this\\narticle presents the stochastic, time-dependent aerological model used for\\nsimulation, together with a standard aircraft model. then we introduce an\\nadaptation of a q-learning algorithm and demonstrate its ability to control the\\naircraft and improve its endurance by exploiting updrafts in non-stationary\\nscenarios.\\n)\": False, 'contains(domain adaptive generation of aircraft on satellite imagery via\\n  simulated and unsupervised learning\\n  object detection and classification for aircraft are the most important tasks\\nin the satellite image analysis. the success of modern detection and\\nclassification methods has been based on machine learning and deep learning.\\none of the key requirements for those learning processes is huge data to train.\\nhowever, there is an insufficient portion of aircraft since the targets are on\\nmilitary action and oper- ation. considering the characteristics of satellite\\nimagery, this paper attempts to provide a framework of the simulated and\\nunsupervised methodology without any additional su- pervision or physical\\nassumptions. finally, the qualitative and quantitative analysis revealed a\\npotential to replenish insufficient data for machine learning platform for\\nsatellite image analysis.\\n)': False, 'contains(structured priors for sparse-representation-based hyperspectral image\\n  classification\\n  pixel-wise classification, where each pixel is assigned to a predefined\\nclass, is one of the most important procedures in hyperspectral image (hsi)\\nanalysis. by representing a test pixel as a linear combination of a small\\nsubset of labeled pixels, a sparse representation classifier (src) gives rather\\nplausible results compared with that of traditional classifiers such as the\\nsupport vector machine (svm). recently, by incorporating additional structured\\nsparsity priors, the second generation srcs have appeared in the literature and\\nare reported to further improve the performance of hsi. these priors are based\\non exploiting the spatial dependencies between the neighboring pixels, the\\ninherent structure of the dictionary, or both. in this paper, we review and\\ncompare several structured priors for sparse-representation-based hsi\\nclassification. we also propose a new structured prior called the low rank\\ngroup prior, which can be considered as a modification of the low rank prior.\\nfurthermore, we will investigate how different structured priors improve the\\nresult for the hsi classification.\\n)': False, 'contains(development of a machine learning based analysis chain for the\\n  measurement of atmospheric muon spectra with icecube\\n  high-energy muons from air shower events detected in icecube are selected\\nusing state of the art machine learning algorithms. attributes to distinguish a\\nhe-muon event from the background of low-energy muon bundles are selected using\\nthe mrmr algorithm and the events are classified by a random forest model. in a\\nsubsequent analysis step the obtained sample is used to reconstruct the\\natmospheric muon energy spectrum, using the unfolding software truee. the\\nreconstructed spectrum covers an energy range from $10^4\\\\,$gev to $10^6\\\\,$gev.\\nthe general analysis scheme is presented, including results using the first\\nyear of data taken with icecube in its complete configuration with $86$\\ninstrumented strings.\\n)': False, \"contains(enforcing statistical constraints in generative adversarial networks for\\n  modeling chaotic dynamical systems\\n  simulating complex physical systems often involves solving partial\\ndifferential equations (pdes) with some closures due to the presence of\\nmulti-scale physics that cannot be fully resolved. therefore, reliable and\\naccurate closure models for unresolved physics remains an important requirement\\nfor many computational physics problems, e.g., turbulence simulation. recently,\\nseveral researchers have adopted generative adversarial networks (gans), a\\nnovel paradigm of training machine learning models, to generate solutions of\\npdes-governed complex systems without having to numerically solve these pdes.\\nhowever, gans are known to be difficult in training and likely to converge to\\nlocal minima, where the generated samples do not capture the true statistics of\\nthe training data. in this work, we present a statistical constrained\\ngenerative adversarial network by enforcing constraints of covariance from the\\ntraining data, which results in an improved machine-learning-based emulator to\\ncapture the statistics of the training data generated by solving fully resolved\\npdes. we show that such a statistical regularization leads to better\\nperformance compared to standard gans, measured by (1) the constrained model's\\nability to more faithfully emulate certain physical properties of the system\\nand (2) the significantly reduced (by up to 80%) training time to reach the\\nsolution. we exemplify this approach on the rayleigh-benard convection, a\\nturbulent flow system that is an idealized model of the earth's atmosphere.\\nwith the growth of high-fidelity simulation databases of physical systems, this\\nwork suggests great potential for being an alternative to the explicit modeling\\nof closures or parameterizations for unresolved physics, which are known to be\\na major source of uncertainty in simulating multi-scale physical systems, e.g.,\\nturbulence or earth's climate.\\n)\": False, 'contains(new hybrid neuro-evolutionary algorithms for renewable energy and\\n  facilities management problems\\n  this ph.d. thesis deals with the optimization of several renewable energy\\nresources development as well as the improvement of facilities management in\\noceanic engineering and airports, using computational hybrid methods belonging\\nto ai to this end. energy is essential to our society in order to ensure a good\\nquality of life. this means that predictions over the characteristics on which\\nrenewable energies depend are necessary, in order to know the amount of energy\\nthat will be obtained at any time. the second topic tackled in this thesis is\\nrelated to the basic parameters that influence in different marine activities\\nand airports, whose knowledge is necessary to develop a proper facilities\\nmanagement in these environments. within this work, a study of the\\nstate-of-the-art machine learning have been performed to solve the problems\\nassociated with the topics above-mentioned, and several contributions have been\\nproposed: one of the pillars of this work is focused on the estimation of the\\nmost important parameters in the exploitation of renewable resources. the\\nsecond contribution of this thesis is related to feature selection problems.\\nthe proposed methodologies are applied to multiple problems: the prediction of\\n$h_s$, relevant for marine energy applications and marine activities, the\\nestimation of wpres, undesirable variations in the electric power produced by a\\nwind farm, the prediction of global solar radiation in areas from spain and\\naustralia, really important in terms of solar energy, and the prediction of\\nlow-visibility events at airports. all of these practical issues are developed\\nwith the consequent previous data analysis, normally, in terms of\\nmeteorological variables.\\n)': False, 'contains(deep reinforcement learning architecture for continuous power allocation\\n  in high throughput satellites\\n  in the coming years, the satellite broadband market will experience\\nsignificant increases in the service demand, especially for the mobility\\nsector, where demand is burstier. many of the next generation of satellites\\nwill be equipped with numerous degrees of freedom in power and bandwidth\\nallocation capabilities, making manual resource allocation impractical and\\ninefficient. therefore, it is desirable to automate the operation of these\\nhighly flexible satellites. this paper presents a novel power allocation\\napproach based on deep reinforcement learning (drl) that represents the problem\\nas continuous state and action spaces. we make use of the proximal policy\\noptimization (ppo) algorithm to optimize the allocation policy for minimum\\nunmet system demand (usd) and power consumption. the performance of the\\nalgorithm is analyzed through simulations of a multibeam satellite system,\\nwhich show promising results for drl to be used as a dynamic resource\\nallocation algorithm.\\n)': False, 'contains(matrix product state based quantum classifier\\n  in recent years, interest in expressing the success of neural networks to the\\nquantum computing has increased significantly. tensor network theory has become\\nincreasingly popular and widely used to simulate strongly entangled correlated\\nsystems. matrix product state (mps) is the well-designed class of tensor\\nnetwork states, which plays an important role in processing of quantum\\ninformation. in this paper, we have shown that matrix product state as\\none-dimensional array of tensors can be used to classify classical and quantum\\ndata. we have performed binary classification of classical machine learning\\ndataset iris encoded in a quantum state. further, we have investigated the\\nperformance by considering different parameters on the ibmqx4 quantum computer\\nand proved that mps circuits can be used to attain better accuracy. further,\\nthe learning ability of mps quantum classifier is tested to classify\\nevapotranspiration ($et_{o}$) for patiala meteorological station located in\\nnorthern punjab (india), using three years of historical dataset (agri).\\nfurthermore, we have used different performance metrics of classification to\\nmeasure its capability. finally, the results are plotted and degree of\\ncorrespondence among values of each sample is shown.\\n)': False, 'contains(crime topic modeling\\n  the classification of crime into discrete categories entails a massive loss\\nof information. crimes emerge out of a complex mix of behaviors and situations,\\nyet most of these details cannot be captured by singular crime type labels.\\nthis information loss impacts our ability to not only understand the causes of\\ncrime, but also how to develop optimal crime prevention strategies. we apply\\nmachine learning methods to short narrative text descriptions accompanying\\ncrime records with the goal of discovering ecologically more meaningful latent\\ncrime classes. we term these latent classes \"crime topics\" in reference to\\ntext-based topic modeling methods that produce them. we use topic distributions\\nto measure clustering among formally recognized crime types. crime topics\\nreplicate broad distinctions between violent and property crime, but also\\nreveal nuances linked to target characteristics, situational conditions and the\\ntools and methods of attack. formal crime types are not discrete in topic\\nspace. rather, crime types are distributed across a range of crime topics.\\nsimilarly, individual crime topics are distributed across a range of formal\\ncrime types. key ecological groups include identity theft, shoplifting,\\nburglary and theft, car crimes and vandalism, criminal threats and confidence\\ncrimes, and violent crimes. though not a replacement for formal legal crime\\nclassifications, crime topics provide a unique window into the heterogeneous\\ncausal processes underlying crime.\\n)': False, 'contains(multi$^{\\\\mathbf{3}}$net: segmenting flooded buildings via fusion of\\n  multiresolution, multisensor, and multitemporal satellite imagery\\n  we propose a novel approach for rapid segmentation of flooded buildings by\\nfusing multiresolution, multisensor, and multitemporal satellite imagery in a\\nconvolutional neural network. our model significantly expedites the generation\\nof satellite imagery-based flood maps, crucial for first responders and local\\nauthorities in the early stages of flood events. by incorporating multitemporal\\nsatellite imagery, our model allows for rapid and accurate post-disaster damage\\nassessment and can be used by governments to better coordinate medium- and\\nlong-term financial assistance programs for affected areas. the network\\nconsists of multiple streams of encoder-decoder architectures that extract\\nspatiotemporal information from medium-resolution images and spatial\\ninformation from high-resolution images before fusing the resulting\\nrepresentations into a single medium-resolution segmentation map of flooded\\nbuildings. we compare our model to state-of-the-art methods for building\\nfootprint segmentation as well as to alternative fusion approaches for the\\nsegmentation of flooded buildings and find that our model performs best on both\\ntasks. we also demonstrate that our model produces highly accurate segmentation\\nmaps of flooded buildings using only publicly available medium-resolution data\\ninstead of significantly more detailed but sparsely available very\\nhigh-resolution data. we release the first open-source dataset of fully\\npreprocessed and labeled multiresolution, multispectral, and multitemporal\\nsatellite images of disaster sites along with our source code.\\n)': False, 'contains(object detection in satellite imagery using 2-step convolutional neural\\n  networks\\n  this paper presents an efficient object detection method from satellite\\nimagery. among a number of machine learning algorithms, we proposed a\\ncombination of two convolutional neural networks (cnn) aimed at high precision\\nand high recall, respectively. we validated our models using golf courses as\\ntarget objects. the proposed deep learning method demonstrated higher accuracy\\nthan previous object identification methods.\\n)': False, \"contains(consistency of random forests\\n  random forests are a learning algorithm proposed by breiman [mach. learn. 45\\n(2001) 5--32] that combines several randomized decision trees and aggregates\\ntheir predictions by averaging. despite its wide usage and outstanding\\npractical performance, little is known about the mathematical properties of the\\nprocedure. this disparity between theory and practice originates in the\\ndifficulty to simultaneously analyze both the randomization process and the\\nhighly data-dependent tree structure. in the present paper, we take a step\\nforward in forest exploration by proving a consistency result for breiman's\\n[mach. learn. 45 (2001) 5--32] original algorithm in the context of additive\\nregression models. our analysis also sheds an interesting light on how random\\nforests can nicely adapt to sparsity. 1. introduction. random forests are an\\nensemble learning method for classification and regression that constructs a\\nnumber of randomized decision trees during the training phase and predicts by\\naveraging the results. since its publication in the seminal paper of breiman\\n(2001), the procedure has become a major data analysis tool, that performs well\\nin practice in comparison with many standard methods. what has greatly\\ncontributed to the popularity of forests is the fact that they can be applied\\nto a wide range of prediction problems and have few parameters to tune. aside\\nfrom being simple to use, the method is generally recognized for its accuracy\\nand its ability to deal with small sample sizes, high-dimensional feature\\nspaces and complex data structures. the random forest methodology has been\\nsuccessfully involved in many practical problems, including air quality\\nprediction (winning code of the emc data science global hackathon in 2012, see\\nhttp://www.kaggle.com/c/dsg-hackathon), chemoinformatics [svetnik et al.\\n(2003)], ecology [prasad, iverson and liaw (2006), cutler et al. (2007)], 3d\\n)\": False, 'contains(seismic bayesian evidential learning: estimation and uncertainty\\n  quantification of sub-resolution reservoir properties\\n  we present a framework that enables estimation of low-dimensional\\nsub-resolution reservoir properties directly from seismic data, without\\nrequiring the solution of a high dimensional seismic inverse problem. our\\nworkflow is based on the bayesian evidential learning approach and exploits\\nlearning the direct relation between seismic data and reservoir properties to\\nefficiently estimate reservoir properties. the theoretical framework we develop\\nallows incorporation of non-linear statistical models for seismic estimation\\nproblems. uncertainty quantification is performed with approximate bayesian\\ncomputation. with the help of a synthetic example of estimation of reservoir\\nnet-to-gross and average fluid saturations in sub-resolution thin-sand\\nreservoir, several nuances are foregrounded regarding the applicability of\\nunsupervised and supervised learning methods for seismic estimation problems.\\nfinally, we demonstrate the efficacy of our approach by estimating posterior\\nuncertainty of reservoir net-to-gross in sub-resolution thin-sand reservoir\\nfrom an offshore delta dataset using 3d pre-stack seismic data.\\n)': False, 'contains(using poisson binomial glms to reveal voter preferences\\n  we present a new modeling technique for solving the problem of ecological\\ninference, in which individual-level associations are inferred from labeled\\ndata available only at the aggregate level. we model aggregate count data as\\narising from the poisson binomial, the distribution of the sum of independent\\nbut not identically distributed bernoulli random variables. we relate\\nindividual-level probabilities to individual covariates using both a logistic\\nregression and a neural network. a normal approximation is derived via the\\nlyapunov central limit theorem, allowing us to efficiently fit these models on\\nlarge datasets. we apply this technique to the problem of revealing voter\\npreferences in the 2016 presidential election, fitting a model to a sample of\\nover four million voters from the highly contested swing state of pennsylvania.\\nwe validate the model at the precinct level via a holdout set, and at the\\nindividual level using weak labels, finding that the model is predictive and it\\nlearns intuitively reasonable associations.\\n)': False, 'contains(geogan: a conditional gan with reconstruction and style loss to generate\\n  standard layer of maps from satellite images\\n  automatically generating maps from satellite images is an important task.\\nthere is a body of literature which tries to address this challenge. we created\\na more expansive survey of the task by experimenting with different models and\\nadding new loss functions to improve results. we created a database of pairs of\\nsatellite images and the corresponding map of the area. our model translates\\nthe satellite image to the corresponding standard layer map image using three\\nmain model architectures: (i) a conditional generative adversarial network\\n(gan) which compresses the images down to a learned embedding, (ii) a generator\\nwhich is trained as a normalizing flow (realnvp) model, and (iii) a conditional\\ngan where the generator translates via a series of convolutions to the standard\\nlayer of a map and the discriminator input is the concatenation of the\\nreal/generated map and the satellite image. model (iii) was by far the most\\npromising of three models. to improve the results we also added a\\nreconstruction loss and style transfer loss in addition to the gan losses. the\\nthird model architecture produced the best quality of sampled images. in\\ncontrast to the other generative model where evaluation of the model is a\\nchallenging problem. since we have access to the real map for a given satellite\\nimage, we are able to assign a quantitative metric to the quality of the\\ngenerated images in addition to inspecting them visually. while we are\\ncontinuing to work on increasing the accuracy of the model, one challenge has\\nbeen the coarse resolution of the data which upper-bounds the quality of the\\nresults of our model. nevertheless, as will be seen in the results, the\\ngenerated map is more accurate in the features it produces since the generator\\narchitecture demands a pixel-wise image translation/pixel-wise coloring. a\\nvideo presentation summarizing this paper is available at:\\nhttps://youtu.be/ur0flox-ji0\\n)': False, 'contains(predicting climate variability over the indian region using data mining\\n  strategies\\n  in this paper an approach based on expectation maximization (em) clustering\\nto find the climate regions and a support vector machine to build a predictive\\nmodel for each of these regions is proposed. to minimize the biases in the\\nestimations a ten cross fold validation is adopted both for obtaining clusters\\nand building the predictive models. the em clustering could identify all the\\nzones as per the koppen classification over indian region. the proposed\\nstrategy when employed for predicting temperature has resulted in an rmse of\\n$1.19$ in the montane climate region and $0.89$ in the humid sub tropical\\nregion as compared to $2.9$ and $0.95$ respectively predicted using k-means and\\nlinear regression method.\\n)': False, \"contains(a hybrid precipitation prediction method based on multicellular gene\\n  expression programming\\n  prompt and accurate precipitation forecast is very important for development\\nmanagement of regional water resource, flood disaster prevention and people's\\ndaily activity and production plan; however, non-linear and nonstationary\\ncharacteristics of precipitation data and noise seriously affect forecast\\naccuracy. this paper combines multicellular gene expression programming with\\nmore powerful function mining ability and wavelet analysis with more powerful\\ndenoising and extracting data fine feature capability for precipitation\\nforecast modeling, proposing to estimate meteorological precipitation with\\nwtgeprp algorithm. comparative result for simulation experiment with actual\\nprecipitation data in zhengzhou, nanning and melbourne in australia indicated\\nthat: fitting and forecasting performance of wtgeprp algorithm is better than\\nthe algorithm multicellular gene expression programming-based hybrid model for\\nprecipitation prediction coupled with emd, supporting vector regression, bp\\nneural network, multicellular gene expression programming and gene expression\\nprogramming, and has good application prospect.\\n)\": False, 'contains(accurate reconstruction of ebsd datasets by a multimodal data approach\\n  using an evolutionary algorithm\\n  a new method has been developed for the correction of the distortions and/or\\nenhanced phase differentiation in electron backscatter diffraction (ebsd) data.\\nusing a multi-modal data approach, the method uses segmented images of the\\nphase of interest (laths, precipitates, voids, inclusions) on images gathered\\nby backscattered or secondary electrons of the same area as the ebsd map. the\\nproposed approach then search for the best transformation to correct their\\nrelative distortions and recombines the data in a new ebsd file. speckles of\\nthe features of interest are first segmented in both the ebsd and image data\\nmodes. the speckle extracted from the ebsd data is then meshed, and the\\ncovariance matrix adaptation evolution strategy (cma-es) is implemented to\\ndistort the mesh until the speckles superimpose. the quality of the matching is\\nquantified via a score that is linked to the number of overlapping pixels in\\nthe speckles. the locations of the points of the distorted mesh are compared to\\nthose of the initial positions to create pairs of matching points that are used\\nto calculate the polynomial function that describes the distortion the best.\\nthis function is then applied to un-distort the ebsd data, and the phase\\ninformation is inferred using the data of the segmented speckle. fast and\\nversatile, this method does not require any human annotation and can be applied\\nto large datasets and wide areas. besides, this method requires very few\\nassumptions concerning the shape of the distortion function. it can be used for\\nthe single compensation of the distortions or combined with the phase\\ndifferentiation. the accuracy of this method is of the order of the pixel size.\\nsome application examples in multiphase materials with feature sizes down to 1\\n$\\\\mu$m are presented, including ti-6al-4v titanium alloy, rene 65 and additive\\nmanufactured inconel 718 nickel-base superalloys.\\n)': False, 'contains(the nonparametric metadata dependent relational model\\n  we introduce the nonparametric metadata dependent relational (nmdr) model, a\\nbayesian nonparametric stochastic block model for network data. the nmdr allows\\nthe entities associated with each node to have mixed membership in an unbounded\\ncollection of latent communities. learned regression models allow these\\nmemberships to depend on, and be predicted from, arbitrary node metadata. we\\ndevelop efficient mcmc algorithms for learning nmdr models from partially\\nobserved node relationships. retrospective mcmc methods allow our sampler to\\nwork directly with the infinite stick-breaking representation of the nmdr,\\navoiding the need for finite truncations. our results demonstrate recovery of\\nuseful latent communities from real-world social and ecological networks, and\\nthe usefulness of metadata in link prediction tasks.\\n)': False, 'contains(renewable energy prediction using weather forecasts for optimal\\n  scheduling in hpc systems\\n  the objective of the greenpad project is to use green energy (wind, solar and\\nbiomass) for powering data-centers that are used to run hpc jobs. as a part of\\nthis it is important to predict the renewable (wind) energy for efficient\\nscheduling (executing jobs that require higher energy when there is more green\\nenergy available and vice-versa). for predicting the wind energy we first\\nanalyze the historical data to find a statistical model that gives relation\\nbetween wind energy and weather attributes. then we use this model based on the\\nweather forecast data to predict the green energy availability in the future.\\nusing the green energy prediction obtained from the statistical model we are\\nable to precompute job schedules for maximizing the green energy utilization in\\nthe future. we propose a model which uses live weather data in addition to\\nmachine learning techniques (which can predict future deviations in weather\\nconditions based on current deviations from the forecast) to make on-the-fly\\nchanges to the precomputed schedule (based on green energy prediction).\\n  for this we first analyze the data using histograms and simple statistical\\ntools such as correlation. in addition we build (correlation) regression model\\nfor finding the relation between wind energy availability and weather\\nattributes (temperature, cloud cover, air pressure, wind speed / direction,\\nprecipitation and sunshine). we also analyze different algorithms and machine\\nlearning techniques for optimizing the job schedules for maximizing the green\\nenergy utilization.\\n)': False, 'contains(does weather matter? causal analysis of tv logs\\n  weather affects our mood and behaviors, and many aspects of our life. when it\\nis sunny, most people become happier; but when it rains, some people get\\ndepressed. despite this evidence and the abundance of data, weather has mostly\\nbeen overlooked in the machine learning and data science research. this work\\npresents a causal analysis of how weather affects tv watching patterns. we show\\nthat some weather attributes, such as pressure and precipitation, cause major\\nchanges in tv watching patterns. to the best of our knowledge, this is the\\nfirst large-scale causal study of the impact of weather on tv watching\\npatterns.\\n)': False, 'contains(an efficient stochastic newton algorithm for parameter estimation in\\n  logistic regressions\\n  logistic regression is a well-known statistical model which is commonly used\\nin the situation where the output is a binary random variable. it has a wide\\nrange of applications including machine learning, public health, social\\nsciences, ecology and econometry. in order to estimate the unknown parameters\\nof logistic regression with data streams arriving sequentially and at high\\nspeed, we focus our attention on a recursive stochastic algorithm. more\\nprecisely, we investigate the asymptotic behavior of a new stochastic newton\\nalgorithm. it enables to easily update the estimates when the data arrive\\nsequentially and to have research steps in all directions. we establish the\\nalmost sure convergence of our stochastic newton algorithm as well as its\\nasymptotic normality. all our theoretical results are illustrated by numerical\\nexperiments.\\n)': False, \"contains(machine learning phases of matter\\n\\n-\\ncondensed-matter physics is the study of the collective\\nbehaviour of infinitely complex assemblies of electrons, nuclei,\\nmagnetic moments, atoms or qubits1. this complexity is\\nreflected in the size of the state space, which grows\\nexponentially with the number of particles, reminiscent of the\\n‚Äòcurse of dimensionality‚Äô commonly encountered in machine\\nlearning2. despite this curse, the machine learning community\\nhas developed techniques with remarkable abilities to\\nrecognize, classify, and characterize complex sets of data. here, we\\nshow that modern machine learning architectures, such as fully\\nconnected and convolutional neural networks3, can identify\\nphases and phase transitions in a variety of condensed-matter\\nhamiltonians. readily programmable through modern\\nsoftware libraries4,5, neural networks can be trained to detect\\nmultiple types of order parameter, as well as highly non-trivial states\\nwith no conventional order, directly from raw state\\nconfigurations sampled with monte carlo6,7.\\nconventionally, the study of phases in condensed-matter systems\\nis performed with the help of tools that have been carefully designed\\nto elucidate the underlying physical structures of various states.\\namong the most powerful are monte carlo simulations, which\\nconsist of two steps: a stochastic importance sampling over state\\nspace, and the evaluation of estimators for physical quantities\\ncalculated from these samples7. these estimators are constructed\\non the basis of a variety of physical motivations; for example, the\\navailability of an experimental measure such as a specific heat; or,\\nthe encoding of a theoretical device such as an order parameter1.\\nhowever, some technologically important states of matter, such\\nas topologically ordered states1,8, can not be straightforwardly\\nidentified with standard estimators9,10.\\nmachine learning, already explored as a tool in\\ncondensedmatter research11 16, provides a complementary paradigm to the\\nabove approach. the ability of modern machine learning techniques\\nto classify, identify, or interpret massive data sets such as images\\nforeshadows their suitability to provide physicists with similar\\nanalyses on the exponentially large data sets embodied in the state\\nspace of condensed-matter systems. we first demonstrate this on\\nthe prototypical example of the square-lattice ferromagnetic ising\\nmizoddel, 1hsod thajtpfohriji niz lajzt.ticweesisteets, jthde1staanted stphaeceisiisngofvsairzieab2lens.\\nstandard monte carlo techniques provide samples of configurations\\nfor any temperature t , weighted by the boltzmann distribution.\\nthe existence of a well-understood phase transition at temperature\\ntc (ref. 17), between a high-temperature paramagnetic phase and\\na low-temperature ferromagnetic phase, allows us the opportunity\\nto attempt to classify the two di erent types of configurations\\nwithout the use of monte carlo estimators. instead, we construct\\na fully connected feed-forward neural network, implemented\\nwith tensorflow4, to perform supervised learning directly on the\\nthermalized and uncorrelated raw configurations sampled by a\\nmonte carlo simulation. as illustrated in fig. 1a, the neural network\\nis composed of an input layer with values determined by the spin\\nconfigurations, a 100-unit hidden layer of sigmoid neurons, and\\nan analogous output layer. when trained on a broad range of data\\nat temperatures above and below tc, the neural network is able\\nto correctly classify data in a test set. finite-size scaling is capable\\nof systematically narrowing in on the thermodynamic value of tc\\nin a way analogous to measurements of the magnetization: a data\\ncollapse of the output layer (fig. 1b) leads to an estimate of the\\ncritical exponent ' 1.0 0.2, while a size scaling of the crossing\\ntemperature t =j estimates tc=j ' 2.266 0.002 (fig. 1c). one\\ncan understand the training of the network through a simple toy\\nmodel involving a hidden layer of only three analytically `trained'\\nperceptrons, representing the possible combinations of high- and\\nlow-temperature magnetic states exclusively on the basis of their\\nmagnetization. similarly, our 100-unit neural network relies on the\\nmagnetization of the configurations in the classification task. details\\nabout the toy model, the 100-unit neural network, as well as a\\nlowdimensional visualization of the training data, which may be used\\nas a preprocessing step to generate the labels if they are not available\\na priori, are discussed in the supplementary figs 1, 2, and 4. we\\nnote that in a recent development, a closely related\\nneural-networkbased approach allows for the determination of critical points using\\na confusion scheme18, which works even in the absence of labels.\\nfinally, we mention that similar success rates occur if the model\\nis modified to have antiferromagnetic couplings, h d j phiji iz jz ,\\nillustrating that the neural network is not only useful in identifying a\\nglobal spin polarization, but an order parameter with other ordering\\nwavevectors (here q d . , /).\\nthe power of neural networks lies in their ability to generalize\\nto tasks beyond their original design. for example, what if one\\nwas presented with a data set of configurations from an ising\\nhamiltonian where the lattice structure (and therefore its tc)\\nis not known? we illustrate this scenario by taking our above\\nfeed-forward neural network, already trained on configurations\\nfor the square-lattice ferromagnetic ising model, and provide it\\na test set produced by monte carlo simulations of the triangular\\nlattice ferromagnetic ising hamiltonian. in fig. 1d f we present the\\naveraged output layer versus t , the corresponding data collapse,\\nand a size scaling of t =j , allowing us to successfully estimate the\\ncritical parameters tc=j d 3.65 0.01 and ' 1.0 0.3 consistent\\nwith the exact values19.\\nwe now turn to the application of such techniques to problems\\nof greater interest in modern condensed matter, such as disordered\\nor topological phases, where no conventional order parameter\\nexists. coulomb phases, for example, are states of frustrated\\nlattice models where local energetic constraints imposed by the\\nhamiltonian lead to extensively degenerate classical ground states.\\nwe consider a two-dimensional square-ice hamiltonian given by\\nh d j pv qv2, where the charge qv d pi2v iz is the sum over the\\nising variables located in the lattice bonds incident on vertex v,\\nas shown in fig. 2. in a conventional approach, the ground states\\na\\nd\\n1.0\\n0.8\\nand the high-temperature states are distinguished by their spin spin\\ncorrelation functions: power-law decay at t d 0, and exponential\\ndecay at t d 1. instead we feed raw monte carlo configurations\\nto train a neural network (fig. 1a) to distinguish ground states\\nfrom high-temperature states (fig. 2a,b). for a square-ice system\\nwith n d 2 16 16 spins, we find that a neural network with\\n100 hidden units successfully distinguishes the states with a 99%\\naccuracy. the network does so solely based on spin configurations,\\nwith no information about the underlying lattice a feat di cult for\\nthe human eye, even if supplemented with a layout of the underlying\\nhamiltonian locality.\\nwe now examine an ising lattice gauge theory, the prototypical\\nexample of a topological phase of matter, without an order\\nparameter at t d 0 (refs 8,20). the hamiltonian is h d j pp qi2p iz ,\\nwhere the ising spins live on the bonds of a two-dimensional square\\nlattice with plaquettes p (see fig. 2c). the ground state is again a\\ndegenerate manifold8,21 with exponentially decaying spin spin\\ncorrelations. as in the square-ice model, we attempt to use the neural\\nnetwork in fig. 1a to classify the high- and low-temperature states,\\nbut find that the training fails to classify the test sets to an accuracy\\nof over 50% equivalent to simply guessing. instead, we employ a\\nconvolutional neural network (cnn)3,22 which readily takes\\nadvantage of the two-dimensional structure as well as the translational\\ninvariance of the model. we optimize the cnn in fig. 2d using\\nmonte carlo configurations from the ising gauge theory at t d 0\\nand t d 1. the cnn discriminates high-temperature from ground\\nstates with an accuracy of 100% in spite of the lack of an order\\nparameter or qualitative di erences in the spin spin correlations.\\n432\\nwe find that the discriminative power of the cnn relies on the\\ndetection of satisfied local energetic constraints of the theory, namely\\nwhether qi2p iz is either c1 (satisfied) or 1 (unsatisfied) on\\neach plaquette of the system (see the supplementary fig. 5). we\\nconstruct an analytical model to explicitly exploit the presence of\\nlocal constraints in the classification task, which discriminates our\\ntest sets with an accuracy of 100% (see supplementary fig. 6).\\nnotice that, because there is no finite-temperature phase\\ntransition in the ising gauge theory, we have restricted our analysis\\nto temperatures t d 0 and t d 1, only. however, in finite systems,\\nviolations of the local constraints are strongly suppressed, and the\\nsystem is expected to slowly cross over to the high-temperature\\nphase. the crossover temperature t happens as the number of\\nitmheprlmyianlglytex=cjited1=dlenfepctns (renf. 2ex3p)..as2tjhe/pirsesoefntcheeoofrlodcearl odfefoenctes,\\nis the mechanism through which the cnn decides whether a\\nsystem is in its ground state or not, we expect that it will be\\nable to detect the crossover temperature in a test set at small but\\nfinite temperatures. in fig. 3 we present the results of the output\\nneurons of our analytical model for di erent system sizes averaged\\nover test sets at di erent temperatures. we estimate the inverse\\ncrossover temperature j based on the crossing point of the\\nlowand high-temperature output neurons. as expected theoretically,\\nthis depends on the system size, and as shown in the inset in fig. 3,\\na clear logarithmic crossover is apparent. this result showcases the\\nability of the cnn to detect not only phase transitions, but also\\nnon-trivial crossovers between topological phases and their\\nhightemperature counterparts.\\n)\": False, 'contains(rebec: robust evolutionary-based calibration approach for the numerical\\n  wind wave model\\n  the adaptation of numerical wind wave models to the local time-spatial\\nconditions is a problem that can be solved by using various calibration\\ntechniques. however, the obtained sets of physical parameters become over-tuned\\nto specific events if there is a lack of observations. in this paper, we\\npropose a robust evolutionary calibration approach that allows to build the\\nstochastic ensemble of perturbed models and use it to achieve the trade-off\\nbetween quality and robustness of the target model. the implemented robust\\nensemble-based evolutionary calibration (rebec) approach was compared to the\\nbaseline spea2 algorithm in a set of experiments with the swan wind wave model\\nconfiguration for the kara sea domain. provided metrics for the set of\\nscenarios confirm the effectiveness of the rebec approach for the majority of\\ncalibration scenarios.\\n)': False, 'contains(machine learning based hyperspectral image analysis: a survey\\n  hyperspectral sensors enable the study of the chemical properties of scene\\nmaterials remotely for the purpose of identification, detection, and chemical\\ncomposition analysis of objects in the environment. hence, hyperspectral images\\ncaptured from earth observing satellites and aircraft have been increasingly\\nimportant in agriculture, environmental monitoring, urban planning, mining, and\\ndefense. machine learning algorithms due to their outstanding predictive power\\nhave become a key tool for modern hyperspectral image analysis. therefore, a\\nsolid understanding of machine learning techniques have become essential for\\nremote sensing researchers and practitioners. this paper reviews and compares\\nrecent machine learning-based hyperspectral image analysis methods published in\\nliterature. we organize the methods by the image analysis task and by the type\\nof machine learning algorithm, and present a two-way mapping between the image\\nanalysis tasks and the types of machine learning algorithms that can be applied\\nto them. the paper is comprehensive in coverage of both hyperspectral image\\nanalysis tasks and machine learning algorithms. the image analysis tasks\\nconsidered are land cover classification, target detection, unmixing, and\\nphysical parameter estimation. the machine learning algorithms covered are\\ngaussian models, linear regression, logistic regression, support vector\\nmachines, gaussian mixture model, latent linear models, sparse linear models,\\ngaussian mixture models, ensemble learning, directed graphical models,\\nundirected graphical models, clustering, gaussian processes, dirichlet\\nprocesses, and deep learning. we also discuss the open challenges in the field\\nof hyperspectral image analysis and explore possible future directions.\\n)': False, 'contains(learning to play with intrinsically-motivated self-aware agents\\n  infants are experts at playing, with an amazing ability to generate novel\\nstructured behaviors in unstructured environments that lack clear extrinsic\\nreward signals. we seek to mathematically formalize these abilities using a\\nneural network that implements curiosity-driven intrinsic motivation. using a\\nsimple but ecologically naturalistic simulated environment in which an agent\\ncan move and interact with objects it sees, we propose a \"world-model\" network\\nthat learns to predict the dynamic consequences of the agent\\'s actions.\\nsimultaneously, we train a separate explicit \"self-model\" that allows the agent\\nto track the error map of its own world-model, and then uses the self-model to\\nadversarially challenge the developing world-model. we demonstrate that this\\npolicy causes the agent to explore novel and informative interactions with its\\nenvironment, leading to the generation of a spectrum of complex behaviors,\\nincluding ego-motion prediction, object attention, and object gathering.\\nmoreover, the world-model that the agent learns supports improved performance\\non object dynamics prediction, detection, localization and recognition tasks.\\ntaken together, our results are initial steps toward creating flexible\\nautonomous agents that self-supervise in complex novel physical environments.\\n)': False, 'contains(fully scalable forward model grid of exoplanet transmission spectra\\n  simulated exoplanet transmission spectra are critical for planning and\\ninterpretation of observations and to explore the sensitivity of spectral\\nfeatures to atmospheric thermochemical processes. we present a publicly\\navailable generic model grid of planetary transmission spectra, scalable to a\\nwide range of h$_2$/he dominated atmospheres. the grid is computed using the\\n1d/2d atmosphere model atmo for two different chemical scenarios, first\\nconsidering local condensation only, secondly considering global condensation\\nand removal of species from the atmospheric column (rainout). the entire grid\\nconsists of 56,320 model simulations across 22 equilibrium temperatures (400 -\\n2600 k), four planetary gravities (5 - 50 ms$^{-2}$), five atmospheric\\nmetallicities (1x - 200x), four c/o ratios (0.35 - 1.0), four scattering haze\\nparameters, four uniform cloud parameters, and two chemical scenarios. we\\nderive scaling equations which can be used with this grid, for a wide range of\\nplanet-star combinations. we validate this grid by comparing it with other\\nmodel transmission spectra available in the literature. we highlight some of\\nthe important findings, such as the rise of so$_2$ features at 100x solar\\nmetallicity, differences in spectral features at high c/o ratios between two\\ncondensation approaches, the importance of vo features without tio to constrain\\nthe limb temperature and features of tio/vo both, to constrain the condensation\\nprocesses. finally, this generic grid can be used to plan future observations\\nusing the hst, vlt, jwst and various other telescopes. the fine variation of\\nparameters in the grid also allows it to be incorporated in a retrieval\\nframework, with various machine learning techniques.\\n)': False, 'contains(biogeography-based informative gene selection and cancer classification\\n  using svm and random forests\\n  microarray cancer gene expression data comprise of very high dimensions.\\nreducing the dimensions helps in improving the overall analysis and\\nclassification performance. we propose two hybrid techniques, biogeography -\\nbased optimization - random forests (bbo - rf) and bbo - svm (support vector\\nmachines) with gene ranking as a heuristic, for microarray gene expression\\nanalysis. this heuristic is obtained from information gain filter ranking\\nprocedure. the bbo algorithm generates a population of candidate subset of\\ngenes, as part of an ecosystem of habitats, and employs the migration and\\nmutation processes across multiple generations of the population to improve the\\nclassification accuracy. the fitness of each gene subset is assessed by the\\nclassifiers - svm and random forests. the performances of these hybrid\\ntechniques are evaluated on three cancer gene expression datasets retrieved\\nfrom the kent ridge biomedical datasets collection and the libsvm data\\nrepository. our results demonstrate that genes selected by the proposed\\ntechniques yield classification accuracies comparable to previously reported\\nalgorithms.\\n)': False, \"contains(generating material maps to map informal settlements\\n  detecting and mapping informal settlements encompasses several of the united\\nnations sustainable development goals. this is because informal settlements are\\nhome to the most socially and economically vulnerable people on the planet.\\nthus, understanding where these settlements are is of paramount importance to\\nboth government and non-government organizations (ngos), such as the united\\nnations children's fund (unicef), who can use this information to deliver\\neffective social and economic aid. we propose a method that detects and maps\\nthe locations of informal settlements using only freely available, sentinel-2\\nlow-resolution satellite spectral data and socio-economic data. this is in\\ncontrast to previous studies that only use costly very-high resolution (vhr)\\nsatellite and aerial imagery. we show how we can detect informal settlements by\\ncombining both domain knowledge and machine learning techniques, to build a\\nclassifier that looks for known roofing materials used in informal settlements.\\nplease find additional material at\\nhttps://frontierdevelopmentlab.github.io/informal-settlements/.\\n)\": False, \"contains(deeprain: convlstm network for precipitation prediction using\\n  multichannel radar data\\n  accurate rainfall forecasting is critical because it has a great impact on\\npeople's social and economic activities. recent trends on various literatures\\nshow that deep learning (neural network) is a promising methodology to tackle\\nmany challenging tasks. in this study, we introduce a brand-new data-driven\\nprecipitation prediction model called deeprain. this model predicts the amount\\nof rainfall from weather radar data, which is three-dimensional and\\nfour-channel data, using convolutional lstm (convlstm). convlstm is a variant\\nof lstm (long short-term memory) containing a convolution operation inside the\\nlstm cell. for the experiment, we used radar reflectivity data for a two-year\\nperiod whose input is in a time series format in units of 6 min divided into 15\\nrecords. the output is the predicted rainfall information for the input data.\\nexperimental results show that two-stacked convlstm reduced rmse by 23.0%\\ncompared to linear regression.\\n)\": False, 'contains(correcting biased observation model error in data assimilation\\n  while the formulation of most data assimilation schemes assumes an unbiased\\nobservation model error, in real applications, model error with nontrivial\\nbiases is unavoidable. a practical example is the error in the radiative\\ntransfer model (which is used to assimilate satellite measurements) in the\\npresence of clouds. as a consequence, many (in fact 99\\\\%) of the cloudy\\nobserved measurements are not being used although they may contain useful\\ninformation. this paper presents a novel nonparametric bayesian scheme which is\\nable to learn the observation model error distribution and correct the bias in\\nincoming observations. this scheme can be used in tandem with any data\\nassimilation forecasting system. the proposed model error estimator uses\\nnonparametric likelihood functions constructed with data-driven basis functions\\nbased on the theory of kernel embeddings of conditional distributions developed\\nin the machine learning community. numerically, we show positive results with\\ntwo examples. the first example is designed to produce a bimodality in the\\nobservation model error (typical of \"cloudy\" observations) by introducing\\nobstructions to the observations which occur randomly in space and time. the\\nsecond example, which is physically more realistic, is to assimilate cloudy\\nsatellite brightness temperature-like quantities, generated from a stochastic\\ncloud model for tropical convection and a simple radiative transfer model.\\n)': False, 'contains(autoencoder-driven weather clustering for source estimation during\\n  nuclear events\\n  emergency response applications for nuclear or radiological events can be\\nsignificantly improved via deep feature learning due to the hidden complexity\\nof the data and models involved. in this paper we present a novel methodology\\nfor rapid source estimation during radiological releases based on deep feature\\nextraction and weather clustering. atmospheric dispersions are then calculated\\nbased on identified predominant weather patterns and are matched against\\nsimulated incidents indicated by radiation readings on the ground. we evaluate\\nthe accuracy of our methods over multiple years of weather reanalysis data in\\nthe european region. we juxtapose these results with deep classification\\nconvolution networks and discuss advantages and disadvantages.\\n)': False, 'contains(application of deep convolutional neural networks for detecting extreme weather in climate datasets\\n\\n-\\ndetecting extreme events in large datasets is a major\\nchallenge in climate science research. current algorithms for\\nextreme event detection are build upon human expertise in\\n1 de ning events based on subjective thresholds of relevant\\nv physical variables. often, multiple competing methods\\npro6 duce vastly di erent results on the same dataset. accurate\\n5 characterization of extreme events in climate simulations\\n11 and observational data archives is critical for\\nunderstand0 ing the trends and potential impacts of such events in a\\n. climate change content. this study presents the rst\\nappli5 cation of deep learning techniques as alternative\\nmethod0 ology for climate extreme events detection. deep neural\\n6 networks are able to learn high-level representations of a\\n:1 broad class of patterns from labeled data. in this work, we\\nv developed deep convolutional neural network (cnn)\\nclasi si cation system and demonstrated the usefulness of deep\\nx learning technique for tackling climate pattern detection\\nr problems. coupled with bayesian based hyper-parameter\\na optimization scheme, our deep cnn system achieves\\n89%99% of accuracy in detecting extreme events (tropical\\ncyclones, atmospheric rivers and weather fronts).\\npermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. copyrights for components of this work owned by others than the\\nauthor(s) must be honored. abstracting with credit is permitted. to copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. request permissions from permissions@acm.org.\\nkdd 2016 august 13-17, san francisco, ca, usa\\nc 2016 copyright held by the owner/author(s). publication rights licensed to acm.\\nisbn 978-1-4503-2138-9. . . $15.00\\ndoi: 10.475/123 4\\n)': False, 'contains(plug-in martingales for testing exchangeability on-line\\n  a standard assumption in machine learning is the exchangeability of data,\\nwhich is equivalent to assuming that the examples are generated from the same\\nprobability distribution independently. this paper is devoted to testing the\\nassumption of exchangeability on-line: the examples arrive one by one, and\\nafter receiving each example we would like to have a valid measure of the\\ndegree to which the assumption of exchangeability has been falsified. such\\nmeasures are provided by exchangeability martingales. we extend known\\ntechniques for constructing exchangeability martingales and show that our new\\nmethod is competitive with the martingales introduced before. finally we\\ninvestigate the performance of our testing method on two benchmark datasets,\\nusps and statlog satellite data; for the former, the known techniques give\\nsatisfactory results, but for the latter our new more flexible method becomes\\nnecessary.\\n)': False, \"contains(machine learning techniques and applications for ground-based image\\n  analysis\\n  ground-based whole sky cameras have opened up new opportunities for\\nmonitoring the earth's atmosphere. these cameras are an important complement to\\nsatellite images by providing geoscientists with cheaper, faster, and more\\nlocalized data. the images captured by whole sky imagers can have high spatial\\nand temporal resolution, which is an important pre-requisite for applications\\nsuch as solar energy modeling, cloud attenuation analysis, local weather\\nprediction, etc.\\n  extracting valuable information from the huge amount of image data by\\ndetecting and analyzing the various entities in these images is challenging.\\nhowever, powerful machine learning techniques have become available to aid with\\nthe image analysis. this article provides a detailed walk-through of recent\\ndevelopments in these techniques and their applications in ground-based\\nimaging. we aim to bridge the gap between computer vision and remote sensing\\nwith the help of illustrative examples. we demonstrate the advantages of using\\nmachine learning techniques in ground-based image analysis via three primary\\napplications -- segmentation, classification, and denoising.\\n)\": False, 'contains(learning to discretize: solving 1d scalar conservation laws via deep\\n  reinforcement learning\\n  conservation laws are considered to be fundamental laws of nature. it has\\nbroad application in many fields including physics, chemistry, biology,\\ngeology, and engineering. solving the differential equations associated with\\nconservation laws is a major branch in computational mathematics. recent\\nsuccess of machine learning, especially deep learning, in areas such as\\ncomputer vision and natural language processing, has attracted a lot of\\nattention from the community of computational mathematics and inspired many\\nintriguing works in combining machine learning with traditional methods. in\\nthis paper, we are the first to explore the possibility and benefit of solving\\nnonlinear conservation laws using deep reinforcement learning. as a proof of\\nconcept, we focus on 1-dimensional scalar conservation laws. we deploy the\\nmachinery of deep reinforcement learning to train a policy network that can\\ndecide on how the numerical solutions should be approximated in a sequential\\nand spatial-temporal adaptive manner. we will show that the problem of solving\\nconservation laws can be naturally viewed as a sequential decision making\\nprocess and the numerical schemes learned in such a way can easily enforce\\nlong-term accuracy. furthermore, the learned policy network can determine a\\ngood local discrete approximation based on the current state of the solution,\\nwhich essentially makes the proposed method a meta-learning approach. in other\\nwords, the proposed method is capable of learning how to discretize for a given\\nsituation mimicking human experts. finally, we will provide details on how the\\npolicy network is trained, how well it performs compared with some\\nstate-of-the-art numerical solvers such as weno schemes, and how well it\\ngeneralizes.\\n)': False, 'contains(sparse-promoting full waveform inversion based on online orthonormal\\n  dictionary learning\\n  full waveform inversion (fwi) delivers high-resolution images of the\\nsubsurface by minimizing iteratively the misfit between the recorded and\\ncalculated seismic data. it has been attacked successfully with the\\ngauss-newton method and sparsity promoting regularization based on fixed\\nmultiscale transforms that permit significant subsampling of the seismic data\\nwhen the model perturbation at each fwi data-fitting iteration can be\\nrepresented with sparse coefficients. rather than using analytical transforms\\nwith predefined dictionaries to achieve sparse representation, we introduce an\\nadaptive transform called the sparse orthonormal transform (sot) whose\\ndictionary is learned from many small training patches taken from the model\\nperturbations in previous iterations. the patch-based dictionary is constrained\\nto be orthonormal and trained with an online approach to provide the best\\nsparse representation of the complex features and variations of the entire\\nmodel perturbation. the complexity of the training method is proportional to\\nthe cube of the number of samples in one small patch. by incorporating both\\ncompressive subsampling and the adaptive sot-based representation into the\\ngauss-newton least-squares problem for each fwi iteration, the model\\nperturbation can be recovered after an l1-norm sparsity constraint is applied\\non the sot coefficients. numerical experiments on synthetic models demonstrate\\nthat the sot-based sparsity promoting regularization can provide robust fwi\\nresults with reduced computation.\\n)': False, \"contains(a gaussian process framework for modelling instrumental systematics:\\n  application to transmission spectroscopy\\n  transmission spectroscopy, which consists of measuring the\\nwavelength-dependent absorption of starlight by a planet's atmosphere during a\\ntransit, is a powerful probe of atmospheric composition. however, the expected\\nsignal is typically orders of magnitude smaller than instrumental systematics,\\nand the results are crucially dependent on the treatment of the latter. in this\\npaper, we propose a new method to infer transit parameters in the presence of\\nsystematic noise using gaussian processes, a technique widely used in the\\nmachine learning community for bayesian regression and classification problems.\\nour method makes use of auxiliary information about the state of the\\ninstrument, but does so in a non-parametric manner, without imposing a specific\\ndependence of the systematics on the instrumental parameters, and naturally\\nallows for the correlated nature of the noise. we give an example application\\nof the method to archival nicmos transmission spectroscopy of the hot jupiter\\nhd 189733, which goes some way towards reconciling the controversy surrounding\\nthis dataset in the literature. finally, we provide an appendix giving a\\ngeneral introduction to gaussian processes for regression, in order to\\nencourage their application to a wider range of problems.\\n)\": False, 'contains(deep learning tutorial for denoising\\n  we herein introduce deep learning to seismic noise attenuation. compared with\\ntraditional seismic noise attenuation algorithms that depend on signal models\\nand their corresponding prior assumptions, a deep neural network is trained\\nbased on a large training set, where the inputs are the raw datasets and the\\ncorresponding outputs are the desired clean data. after the completion of\\ntraining, the deep learning method achieves adaptive denoising with no\\nrequirements of (i) accurate modeling of the signal and noise, and (ii) optimal\\nparameters tuning. we call this intelligent denoising. we use a convolutional\\nneural network as the basic tool for deep learning. the training set is\\ngenerated with manually added noise in random and linear noise attenuation, and\\nwith the wave equation in the multiple attenuation. stochastic gradient descent\\nis used to solve the optimal parameters for the convolutional neural network.\\nthe runtime of deep learning on a graphics processing unit for denoising has\\nthe same order as the $f-x$ deconvolutional method. synthetic and field results\\nshow the potential applications of deep learning in the automation of random\\nnoise attenuation with unknown variance, linear noise, and multiples.\\n)': False, 'contains(parametrization and generation of geological models with generative\\n  adversarial networks\\n  one of the main challenges in the parametrization of geological models is the\\nability to capture complex geological structures often observed in the\\nsubsurface. in recent years, generative adversarial networks (gan) were\\nproposed as an efficient method for the generation and parametrization of\\ncomplex data, showing state-of-the-art performances in challenging computer\\nvision tasks such as reproducing natural images (handwritten digits, human\\nfaces, etc.). in this work, we study the application of wasserstein gan for the\\nparametrization of geological models. the effectiveness of the method is\\nassessed for uncertainty propagation tasks using several test cases involving\\ndifferent permeability patterns and subsurface flow problems. results show that\\ngans are able to generate samples that preserve the multipoint statistical\\nfeatures of the geological models both visually and quantitatively. the\\ngenerated samples reproduce both the geological structures and the flow\\nstatistics of the reference geology.\\n)': False, 'contains(using causal discovery to track information flow in spatio-temporal data\\n  - a testbed and experimental results using advection-diffusion simulations\\n  causal discovery algorithms based on probabilistic graphical models have\\nemerged in geoscience applications for the identification and visualization of\\ndynamical processes. the key idea is to learn the structure of a graphical\\nmodel from observed spatio-temporal data, which indicates information flow,\\nthus pathways of interactions, in the observed physical system. studying those\\npathways allows geoscientists to learn subtle details about the underlying\\ndynamical mechanisms governing our planet. initial studies using this approach\\non real-world atmospheric data have shown great potential for scientific\\ndiscovery. however, in these initial studies no ground truth was available, so\\nthat the resulting graphs have been evaluated only by whether a domain expert\\nthinks they seemed physically plausible. this paper seeks to fill this gap. we\\ndevelop a testbed that emulates two dynamical processes dominant in many\\ngeoscience applications, namely advection and diffusion, in a 2d grid. then we\\napply the causal discovery based information tracking algorithms to the\\nsimulation data to study how well the algorithms work for different scenarios\\nand to gain a better understanding of the physical meaning of the graph\\nresults, in particular of instantaneous connections. we make all data sets used\\nin this study available to the community as a benchmark.\\n  keywords: information flow, graphical model, structure learning, causal\\ndiscovery, geoscience.\\n)': False, \"contains(incremental import vector machines for classifying hyperspectral data\\n  in this paper we propose an incremental learning strategy for import vector\\nmachines (ivm), which is a sparse kernel logistic regression approach. we use\\nthe procedure for the concept of self-training for sequential classification of\\nhyperspectral data. the strategy comprises the inclusion of new training\\nsamples to increase the classification accuracy and the deletion of\\nnon-informative samples to be memory- and runtime-efficient. moreover, we\\nupdate the parameters in the incremental ivm model without re-training from\\nscratch. therefore, the incremental classifier is able to deal with large data\\nsets. the performance of the ivm in comparison to support vector machines (svm)\\nis evaluated in terms of accuracy and experiments are conducted to assess the\\npotential of the probabilistic outputs of the ivm. experimental results\\ndemonstrate that the ivm and svm perform similar in terms of classification\\naccuracy. however, the number of import vectors is significantly lower when\\ncompared to the number of support vectors and thus, the computation time during\\nclassification can be decreased. moreover, the probabilities provided by ivm\\nare more reliable, when compared to the probabilistic information, derived from\\nan svm's output. in addition, the proposed self-training strategy can increase\\nthe classification accuracy. overall, the ivm and the its incremental version\\nis worthwhile for the classification of hyperspectral data.\\n)\": False, 'contains(correction of electron back-scattered diffraction datasets using an\\n  evolutionary algorithm\\n  in materials science and particularly electron microscopy, electron\\nback-scatter diffraction (ebsd) is a common and powerful mapping technique for\\ncollecting local crystallographic data at the sub-micron scale. the quality of\\nthe reconstruction of the maps is critical to study the spatial distribution of\\nphases and crystallographic orientation relationships between phases, a key\\ninterest in materials science. however, ebsd data is known to suffer from\\ndistortions that arise from several instrument and detector artifacts. in this\\npaper, we present an unsupervised method that corrects those distortions, and\\nenables or enhances phase differentiation in ebsd data. the method uses a\\nsegmented electron image of the phases of interest (laths, precipitates, voids,\\ninclusions) gathered using detectors that generate less distorted data, of the\\nsame area than the ebsd map, and then searches for the best transformation to\\ncorrect the distortions of the initial ebsd data. to do so, the covariance\\nmatrix adaptation evolution strategy (cma-es) is implemented to distort the\\nebsd until it matches the reference electron image. fast and versatile, this\\nmethod does not require any human annotation and can be applied to large\\ndatasets and wide areas, where the distortions are important. besides, this\\nmethod requires very little assumption concerning the shape of the distortion\\nfunction. some application examples in multiphase materials with feature sizes\\ndown to 1 $\\\\mu$m are presented, including a titanium alloy and a nickel-base\\nsuperalloy.\\n)': False, 'contains(comparison of statistical post-processing methods for probabilistic nwp\\n  forecasts of solar radiation\\n  the increased usage of solar energy places additional importance on forecasts\\nof solar radiation. solar panel power production is primarily driven by the\\namount of solar radiation and it is therefore important to have accurate\\nforecasts of solar radiation. accurate forecasts that also give information on\\nthe forecast uncertainties can help users of solar energy to make better solar\\nradiation based decisions related to the stability of the electrical grid. to\\nachieve this, we apply statistical post-processing techniques that determine\\nrelationships between observations of global radiation (made within the knmi\\nnetwork of automatic weather stations in the netherlands) and forecasts of\\nvarious meteorological variables from the numerical weather prediction (nwp)\\nmodel harmonie-arome (ha) and the atmospheric composition model cams. those\\nrelationships are used to produce probabilistic forecasts of global radiation.\\nwe compare 7 different statistical post-processing methods, consisting of two\\nparametric and five non-parametric methods. we find that all methods are able\\nto generate probabilistic forecasts that improve the raw global radiation\\nforecast from ha according to the root mean squared error (on the median) and\\nthe potential economic value. additionally, we show how important the\\npredictors are in the different regression methods. we also compare the\\nregression methods using various probabilistic scoring metrics, namely the\\ncontinuous ranked probability skill score, the brier skill score and\\nreliability diagrams. we find that quantile regression and generalized random\\nforests generally perform best. in (near) clear sky conditions the\\nnon-parametric methods have more skill than the parametric ones.\\n)': False, 'contains(embodied artificial intelligence through distributed adaptive control:\\n  an integrated framework\\n  in this paper, we argue that the future of artificial intelligence research\\nresides in two keywords: integration and embodiment. we support this claim by\\nanalyzing the recent advances of the field. regarding integration, we note that\\nthe most impactful recent contributions have been made possible through the\\nintegration of recent machine learning methods (based in particular on deep\\nlearning and recurrent neural networks) with more traditional ones (e.g.\\nmonte-carlo tree search, goal babbling exploration or addressable memory\\nsystems). regarding embodiment, we note that the traditional benchmark tasks\\n(e.g. visual classification or board games) are becoming obsolete as\\nstate-of-the-art learning algorithms approach or even surpass human performance\\nin most of them, having recently encouraged the development of first-person 3d\\ngame platforms embedding realistic physics. building upon this analysis, we\\nfirst propose an embodied cognitive architecture integrating heterogenous\\nsub-fields of artificial intelligence into a unified framework. we demonstrate\\nthe utility of our approach by showing how major contributions of the field can\\nbe expressed within the proposed framework. we then claim that benchmarking\\nenvironments need to reproduce ecologically-valid conditions for bootstrapping\\nthe acquisition of increasingly complex cognitive skills through the concept of\\na cognitive arms race between embodied agents.\\n)': False, 'contains(a periodicity-based parallel time series prediction algorithm in cloud\\n  computing environments\\n  in the era of big data, practical applications in various domains continually\\ngenerate large-scale time-series data. among them, some data show significant\\nor potential periodicity characteristics, such as meteorological and financial\\ndata. it is critical to efficiently identify the potential periodic patterns\\nfrom massive time-series data and provide accurate predictions. in this paper,\\na periodicity-based parallel time series prediction (pptsp) algorithm for\\nlarge-scale time-series data is proposed and implemented in the apache spark\\ncloud computing environment. to effectively handle the massive historical\\ndatasets, a time series data compression and abstraction (tsdca) algorithm is\\npresented, which can reduce the data scale as well as accurately extracting the\\ncharacteristics. based on this, we propose a multi-layer time series periodic\\npattern recognition (mtsppr) algorithm using the fourier spectrum analysis\\n(fsa) method. in addition, a periodicity-based time series prediction (ptsp)\\nalgorithm is proposed. data in the subsequent period are predicted based on all\\nprevious period models, in which a time attenuation factor is introduced to\\ncontrol the impact of different periods on the prediction results. moreover, to\\nimprove the performance of the proposed algorithms, we propose a parallel\\nsolution on the apache spark platform, using the streaming real-time computing\\nmodule. to efficiently process the large-scale time-series datasets in\\ndistributed computing environments, distributed streams (dstreams) and\\nresilient distributed datasets (rdds) are used to store and calculate these\\ndatasets. extensive experimental results show that our pptsp algorithm has\\nsignificant advantages compared with other algorithms in terms of prediction\\naccuracy and performance.\\n)': False, \"contains(gap filling in the plant kingdom---trait prediction using hierarchical\\n  probabilistic matrix factorization\\n  plant traits are a key to understanding and predicting the adaptation of\\necosystems to environmental changes, which motivates the try project aiming at\\nconstructing a global database for plant traits and becoming a standard\\nresource for the ecological community. despite its unprecedented coverage, a\\nlarge percentage of missing data substantially constrains joint trait analysis.\\nmeanwhile, the trait data is characterized by the hierarchical phylogenetic\\nstructure of the plant kingdom. while factorization based matrix completion\\ntechniques have been widely used to address the missing data problem,\\ntraditional matrix factorization methods are unable to leverage the\\nphylogenetic structure. we propose hierarchical probabilistic matrix\\nfactorization (hpmf), which effectively uses hierarchical phylogenetic\\ninformation for trait prediction. we demonstrate hpmf's high accuracy,\\neffectiveness of incorporating hierarchical structure and ability to capture\\ntrait correlation through experiments.\\n)\": False, \"contains(memory in memory: a predictive neural network for learning higher-order\\n  non-stationarity from spatiotemporal dynamics\\n  natural spatiotemporal processes can be highly non-stationary in many ways,\\ne.g. the low-level non-stationarity such as spatial correlations or temporal\\ndependencies of local pixel values; and the high-level variations such as the\\naccumulation, deformation or dissipation of radar echoes in precipitation\\nforecasting. from cramer's decomposition, any non-stationary process can be\\ndecomposed into deterministic, time-variant polynomials, plus a zero-mean\\nstochastic term. by applying differencing operations appropriately, we may turn\\ntime-variant polynomials into a constant, making the deterministic component\\npredictable. however, most previous recurrent neural networks for\\nspatiotemporal prediction do not use the differential signals effectively, and\\ntheir relatively simple state transition functions prevent them from learning\\ntoo complicated variations in spacetime. we propose the memory in memory (mim)\\nnetworks and corresponding recurrent blocks for this purpose. the mim blocks\\nexploit the differential signals between adjacent recurrent states to model the\\nnon-stationary and approximately stationary properties in spatiotemporal\\ndynamics with two cascaded, self-renewed memory modules. by stacking multiple\\nmim blocks, we could potentially handle higher-order non-stationarity. the mim\\nnetworks achieve the state-of-the-art results on four spatiotemporal prediction\\ntasks across both synthetic and real-world datasets. we believe that the\\ngeneral idea of this work can be potentially applied to other time-series\\nforecasting tasks.\\n)\": False, 'contains(structure label prediction using similarity-based retrieval and\\n  weakly-supervised label mapping\\n  recently, there has been significant interest in various supervised machine\\nlearning techniques that can help reduce the time and effort consumed by manual\\ninterpretation workflows. however, most successful supervised machine learning\\nalgorithms require huge amounts of annotated training data. obtaining these\\nlabels for large seismic volumes is a very time-consuming and laborious task.\\nwe address this problem by presenting a weakly-supervised approach for\\npredicting the labels of various seismic structures. by having an interpreter\\nselect a very small number of exemplar images for every class of subsurface\\nstructures, we use a novel similarity-based retrieval technique to extract\\nthousands of images that contain similar subsurface structures from the seismic\\nvolume. by assuming that similar images belong to the same class, we obtain\\nthousands of image-level labels for these images; we validate this assumption\\nin our results section. we then introduce a novel weakly-supervised algorithm\\nfor mapping these rough image-level labels into more accurate pixel-level\\nlabels that localize the different subsurface structures within the image. this\\napproach dramatically simplifies the process of obtaining labeled data for\\ntraining supervised machine learning algorithms on seismic interpretation\\ntasks. using our method we generate thousands of automatically-labeled images\\nfrom the netherlands offshore f3 block with reasonably accurate pixel-level\\nlabels. we believe this work will allow for more advances in machine\\nlearning-enabled seismic interpretation.\\n)': False, 'contains(progressively growing generative adversarial networks for high\\n  resolution semantic segmentation of satellite images\\n  machine learning has proven to be useful in classification and segmentation\\nof images. in this paper, we evaluate a training methodology for pixel-wise\\nsegmentation on high resolution satellite images using progressive growing of\\ngenerative adversarial networks. we apply our model to segmenting building\\nrooftops and compare these results to conventional methods for rooftop\\nsegmentation. we present our findings using the spacenet version 2 dataset.\\nprogressive gan training achieved a test accuracy of 93% compared to 89% for\\ntraditional gan training.\\n)': False, 'contains(discovering state-parameter mappings in subsurface models using\\n  generative adversarial networks\\n  a fundamental problem in geophysical modeling is related to the\\nidentification and approximation of causal structures among physical processes.\\nhowever, resolving the bidirectional mappings between physical parameters and\\nmodel state variables (i.e., solving the forward and inverse problems) is\\nchallenging, especially when parameter dimensionality is high. deep learning\\nhas opened a new door toward knowledge representation and complex pattern\\nidentification. in particular, the recently introduced generative adversarial\\nnetworks (gans) hold strong promises in learning cross-domain mappings for\\nimage translation. this study presents a state-parameter identification gan\\n(spid-gan) for simultaneously learning bidirectional mappings between a\\nhigh-dimensional parameter space and the corresponding model state space.\\nspid-gan is demonstrated using a series of representative problems from\\nsubsurface flow modeling. results show that spid-gan achieves satisfactory\\nperformance in identifying the bidirectional state-parameter mappings,\\nproviding a new deep-learning-based, knowledge representation paradigm for a\\nwide array of complex geophysical problems.\\n)': False, 'contains(neuralhydrology - interpreting lstms in hydrology\\n  despite the huge success of long short-term memory networks, their\\napplications in environmental sciences are scarce. we argue that one reason is\\nthe difficulty to interpret the internals of trained networks. in this study,\\nwe look at the application of lstms for rainfall-runoff forecasting, one of the\\ncentral tasks in the field of hydrology, in which the river discharge has to be\\npredicted from meteorological observations. lstms are particularly well-suited\\nfor this problem since memory cells can represent dynamic reservoirs and\\nstorages, which are essential components in state-space modelling approaches of\\nthe hydrological system. on basis of two different catchments, one with snow\\ninfluence and one without, we demonstrate how the trained model can be analyzed\\nand interpreted. in the process, we show that the network internally learns to\\nrepresent patterns that are consistent with our qualitative understanding of\\nthe hydrological system.\\n)': False, \"contains(convolutional lstms for cloud-robust segmentation of remote sensing\\n  imagery\\n  clouds frequently cover the earth's surface and pose an omnipresent challenge\\nto optical earth observation methods. the vast majority of remote sensing\\napproaches either selectively choose single cloud-free observations or employ a\\npre-classification strategy to identify and mask cloudy pixels. we follow a\\ndifferent strategy and treat cloud coverage as noise that is inherent to the\\nobserved satellite data. in prior work, we directly employed a straightforward\\n\\\\emph{convolutional long short-term memory} network for vegetation\\nclassification without explicit cloud filtering and achieved state-of-the-art\\nclassification accuracies. in this work, we investigate this cloud-robustness\\nfurther by visualizing internal cell activations and performing an ablation\\nexperiment on datasets of different cloud coverage. in the visualizations of\\nnetwork states, we identified some cells in which modulation and input gates\\nclosed on cloudy pixels. this indicates that the network has internalized a\\ncloud-filtering mechanism without being specifically trained on cloud labels.\\noverall, our results question the necessity of sophisticated pre-processing\\npipelines for multi-temporal deep learning approaches.\\n)\": False, 'contains(predictive capacity of meteorological data - will it rain tomorrow\\n  with the availability of high precision digital sensors and cheap storage\\nmedium, it is not uncommon to find large amounts of data collected on almost\\nall measurable attributes, both in nature and man-made habitats. weather in\\nparticular has been an area of keen interest for researchers to develop more\\naccurate and reliable prediction models. this paper presents a set of\\nexperiments which involve the use of prevalent machine learning techniques to\\nbuild models to predict the day of the week given the weather data for that\\nparticular day i.e. temperature, wind, rain etc., and test their reliability\\nacross four cities in australia {brisbane, adelaide, perth, hobart}. the\\nresults provide a comparison of accuracy of these machine learning techniques\\nand their reliability to predict the day of the week by analysing the weather\\ndata. we then apply the models to predict weather conditions based on the\\navailable data.\\n)': False, \"contains(automatic salt deposits segmentation: a deep learning approach\\n  one of the most important applications of seismic reflection is the\\nhydrocarbon exploration which is closely related to salt deposits analysis.\\nthis problem is very important even nowadays due to it's non-linear nature.\\ntaking into account the recent developments in deep learning networks tgs-nopec\\ngeophysical company hosted the kaggle competition for salt deposits\\nsegmentation problem in seismic image data. in this paper, we demonstrate the\\ngreat performance of several novel deep learning techniques merged into a\\nsingle neural network which achieved the 27th place (top 1%) in the mentioned\\ncompetition. using a u-net with resnext-50 encoder pre-trained on imagenet as\\nour base architecture, we implemented spatial-channel squeeze & excitation,\\nlovasz loss, coordconv and hypercolumn methods. the source code for our\\nsolution is made publicly available at\\nhttps://github.com/k-mike/automatic-salt-deposits-segmentation.\\n)\": False, \"contains(mapping informal settlements in developing countries using machine\\n  learning and low resolution multi-spectral data\\n  informal settlements are home to the most socially and economically\\nvulnerable people on the planet. in order to deliver effective economic and\\nsocial aid, non-government organizations (ngos), such as the united nations\\nchildren's fund (unicef), require detailed maps of the locations of informal\\nsettlements. however, data regarding informal and formal settlements is\\nprimarily unavailable and if available is often incomplete. this is due, in\\npart, to the cost and complexity of gathering data on a large scale. to address\\nthese challenges, we, in this work, provide three contributions. 1) a brand new\\nmachine learning data-set, purposely developed for informal settlement\\ndetection. 2) we show that it is possible to detect informal settlements using\\nfreely available low-resolution (lr) data, in contrast to previous studies that\\nuse very-high resolution (vhr) satellite and aerial imagery, something that is\\ncost-prohibitive for ngos. 3) we demonstrate two effective classification\\nschemes on our curated data set, one that is cost-efficient for ngos and\\nanother that is cost-prohibitive for ngos, but has additional utility. we\\nintegrate these schemes into a semi-automated pipeline that converts either a\\nlr or vhr satellite image into a binary map that encodes the locations of\\ninformal settlements.\\n)\": False, 'contains(short-term forecasting of italian residential gas demand\\n  natural gas is the most important energy source in italy: it fuels\\nthermoelectric power plants, industrial facilities and domestic heating. gas\\ndemand forecasting is a critical task for any energy provider as it impacts on\\npipe reservation and stock planning. in this paper, the one-day-ahead\\nforecasting of italian daily residential gas demand is studied. five predictors\\nare developed and compared: ridge regression, gaussian process, k-nearest\\nneighbour, artificial neural network, and torus model. preprocessing and\\nfeature selection are also discussed in detail. concerning the prediction\\nerror, a theoretical bound on the best achievable root mean square error is\\nworked out assuming ideal conditions, except for the inaccuracy of\\nmeteorological temperature forecasts, whose effects are properly propagated.\\nthe best predictors, namely the artificial neural network and the gaussian\\nprocess, achieve an rmse which is twice the performance limit, suggesting that\\nprecise predictions of residential gas demand can be achieved at country level.\\n)': False, \"contains(estimating the physical state of a laboratory slow slipping fault from\\n  seismic signals\\n  over the last two decades, strain and gps measurements have shown that slow\\nslip on earthquake faults is a widespread phenomenon. slow slip is also\\ninferred from correlated small amplitude seismic signals known as nonvolcanic\\ntremor and low frequency earthquakes (lfes). slow slip has been reproduced in\\nlaboratory and simulation studies, however the fundamental physics of these\\nphenomena and their relationship to dynamic earthquake rupture remains poorly\\nunderstood. here we show that, in a laboratory setting, continuous seismic\\nwaves are imprinted with fundamental signatures of the fault's physical state.\\nusing machine learning on continuous seismic waves, we can infer several bulk\\ncharacteristics of the fault (friction, shear displacement, gouge thickness),\\nat any time during the slow slip cycle. this analysis also allows us to infer\\nmany properties of the future behavior of the fault, including the time\\nremaining before the next slow slip event. our work suggests that by applying\\nmachine learning approaches to continuous seismic data, new insight into the\\nphysics of slow slip could be obtained in earth.\\n)\": False, 'contains(waveform cross correlation applied to earthquakes in the atlantic ocean\\n  we assess the level of cross correlation between p-waves generated by\\nearthquakes in the atlantic ocean and measured by 22 array stations of the\\ninternational monitoring system (ims). there are 931 events with 6,411 arrivals\\nin 2011 and 2012. station tord was the most sensitive and detected 868 from 931\\nevents. we constructed several 931 by 931 matrices of cross correlation\\ncoefficients (ccs) for individual stations and also for average and cumulative\\nccs. these matrices characterize the detection performance of the involved\\nstations and the ims. sixty earthquakes located in the northern hemisphere were\\nselected as master events for signal detection and building of events\\npopulating a cross correlation standard event list (xsel) for the first halves\\nof 2009 and 2012. high-quality signals (snr>5.0) recorded by 10 most sensitive\\nstations were used as waveform templates. in order to quantitatively estimate\\nthe gain in the completeness and resolution of the xsel we compared it with the\\nreviewed event bulletin (reb) of the international data centre (idc) for the\\nnorth atlantic (na) and with the isc bulletin. machine learning and\\nclassification algorithms were successfully applied to automatically reject\\ninvalid events in the xsel for 2009.\\n)': False, 'contains(a tool for spatio-temporal analysis of social anxiety with twitter data\\n  in this paper, we present a tool for analyzing spatio-temporal distribution\\nof social anxiety. twitter, one of the most popular social network services,\\nhas been chosen as data source for analysis of social anxiety. tweets (posted\\non the twitter) contain various emotions and thus these individual emotions\\nreflect social atmosphere and public opinion, which are often dependent on\\nspatial and temporal factors. the reason why we choose anxiety among various\\nemotions is that anxiety is very important emotion that is useful for observing\\nand understanding social events of communities. we develop a machine learning\\nbased tool to analyze the changes of social atmosphere spatially and\\ntemporally. our tool classifies whether each tweet contains anxious content or\\nnot, and also estimates degree of tweet anxiety. furthermore, it also\\nvisualizes spatio-temporal distribution of anxiety as a form of web\\napplication, which is incorporated with physical map, word cloud, search engine\\nand chart viewer. our tool is applied to a big tweet data in south korea to\\nillustrate its usefulness for exploring social atmosphere and public opinion\\nspatio-temporally.\\n)': False, 'contains(landmine detection using autoencoders on multi-polarization gpr\\n  volumetric data\\n  buried landmines and unexploded remnants of war are a constant threat for the\\npopulation of many countries that have been hit by wars in the past years. the\\nhuge amount of human lives lost due to this phenomenon has been a strong\\nmotivation for the research community toward the development of safe and robust\\ntechniques designed for landmine clearance. nonetheless, being able to detect\\nand localize buried landmines with high precision in an automatic fashion is\\nstill considered a challenging task due to the many different boundary\\nconditions that characterize this problem (e.g., several kinds of objects to\\ndetect, different soils and meteorological conditions, etc.). in this paper, we\\npropose a novel technique for buried object detection tailored to unexploded\\nlandmine discovery. the proposed solution exploits a specific kind of\\nconvolutional neural network (cnn) known as autoencoder to analyze volumetric\\ndata acquired with ground penetrating radar (gpr) using different\\npolarizations. this method works in an anomaly detection framework, indeed we\\nonly train the autoencoder on gpr data acquired on landmine-free areas. the\\nsystem then recognizes landmines as objects that are dissimilar to the soil\\nused during the training step. experiments conducted on real data show that the\\nproposed technique requires little training and no ad-hoc data pre-processing\\nto achieve accuracy higher than 93% on challenging datasets.\\n)': False, 'contains(deep scattering: rendering atmospheric clouds with radiance-predicting\\n  neural networks\\n  we present a technique for efficiently synthesizing images of atmospheric\\nclouds using a combination of monte carlo integration and neural networks. the\\nintricacies of lorenz-mie scattering and the high albedo of cloud-forming\\naerosols make rendering of clouds---e.g. the characteristic silverlining and\\nthe \"whiteness\" of the inner body---challenging for methods based solely on\\nmonte carlo integration or diffusion theory. we approach the problem\\ndifferently. instead of simulating all light transport during rendering, we\\npre-learn the spatial and directional distribution of radiant flux from tens of\\ncloud exemplars. to render a new scene, we sample visible points of the cloud\\nand, for each, extract a hierarchical 3d descriptor of the cloud geometry with\\nrespect to the shading location and the light source. the descriptor is input\\nto a deep neural network that predicts the radiance function for each shading\\nconfiguration. we make the key observation that progressively feeding the\\nhierarchical descriptor into the network enhances the network\\'s ability to\\nlearn faster and predict with high accuracy while using few coefficients. we\\nalso employ a block design with residual connections to further improve\\nperformance. a gpu implementation of our method synthesizes images of clouds\\nthat are nearly indistinguishable from the reference solution within seconds\\ninteractively. our method thus represents a viable solution for applications\\nsuch as cloud design and, thanks to its temporal stability, also for\\nhigh-quality production of animated content.\\n)': False, 'contains(correntropy maximization via admm - application to robust hyperspectral\\n  unmixing\\n  in hyperspectral images, some spectral bands suffer from low signal-to-noise\\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\\ntechniques for the unmixing problem. this paper presents a robust supervised\\nspectral unmixing approach for hyperspectral images. the robustness is achieved\\nby writing the unmixing problem as the maximization of the correntropy\\ncriterion subject to the most commonly used constraints. two unmixing problems\\nare derived: the first problem considers the fully-constrained unmixing, with\\nboth the non-negativity and sum-to-one constraints, while the second one deals\\nwith the non-negativity and the sparsity-promoting of the abundances. the\\ncorresponding optimization problems are solved efficiently using an alternating\\ndirection method of multipliers (admm) approach. experiments on synthetic and\\nreal hyperspectral images validate the performance of the proposed algorithms\\nfor different scenarios, demonstrating that the correntropy-based unmixing is\\nrobust to outlier bands.\\n)': False, 'contains(spatio-temporal crop classification of low-resolution satellite imagery\\n  with capsule layers and distributed attention\\n  land use classification of low resolution spatial imagery is one of the most\\nextensively researched fields in remote sensing. despite significant\\nadvancements in satellite technology, high resolution imagery lacks global\\ncoverage and can be prohibitively expensive to procure for extended time\\nperiods. accurately classifying land use change without high resolution imagery\\noffers the potential to monitor vital aspects of global development agenda\\nincluding climate smart agriculture, drought resistant crops, and sustainable\\nland management. utilizing a combination of capsule layers and long-short term\\nmemory layers with distributed attention, the present paper achieves\\nstate-of-the-art accuracy on temporal crop type classification at a 30x30m\\nresolution with sentinel 2 imagery.\\n)': False, \"contains(reproducing kernel functions: a general framework for discrete variable\\n  representation\\n  since its introduction, the discrete variable representation (dvr) basis set\\nhas become an invaluable representation of state vectors and hermitian\\noperators in non-relativistic quantum dynamics and spectroscopy calculations.\\non the other hand reproducing kernel (positive definite) functions have been\\nwidely employed for a long time to a wide variety of disciplines: detection and\\nestimation problems in signal processing; data analysis in statistics;\\ngenerating observational models in machine learning; solving inverse problems\\nin geophysics and tomography in general; and in quantum mechanics.\\n  in this article it was demonstrated that, starting with the axiomatic\\ndefinition of dvr provided by littlejohn [1], it is possible to show that the\\nspace upon which the projection operator, defined in ref [1], projects is a\\nreproducing kernel hilbert space (rkhs) whose associated reproducing kernel\\nfunction can be used to generate dvr points and their corresponding dvr\\nfunctions on any domain manifold (curved or not). it is illustrated how, with\\nthis idea, one may be able to `neatly' address the long-standing challenge of\\nbuilding multidimensional dvr basis functions defined on curved manifolds.\\n)\": False, 'contains(slum segmentation and change detection : a deep learning approach\\n  more than one billion people live in slums around the world. in some\\ndeveloping countries, slum residents make up for more than half of the\\npopulation and lack reliable sanitation services, clean water, electricity,\\nother basic services. thus, slum rehabilitation and improvement is an important\\nglobal challenge, and a significant amount of effort and resources have been\\nput into this endeavor. these initiatives rely heavily on slum mapping and\\nmonitoring, and it is essential to have robust and efficient methods for\\nmapping and monitoring existing slum settlements. in this work, we introduce an\\napproach to segment and map individual slums from satellite imagery, leveraging\\nregional convolutional neural networks for instance segmentation using transfer\\nlearning. in addition, we also introduce a method to perform change detection\\nand monitor slum change over time. we show that our approach effectively learns\\nslum shape and appearance, and demonstrates strong quantitative results,\\nresulting in a maximum ap of 80.0.\\n)': False, 'contains(construction safety risk modeling and simulation\\n  by building on a recently introduced genetic-inspired attribute-based\\nconceptual framework for safety risk analysis, we propose a novel methodology\\nto compute construction univariate and bivariate construction safety risk at a\\nsituational level. our fully data-driven approach provides construction\\npractitioners and academicians with an easy and automated way of extracting\\nvaluable empirical insights from databases of unstructured textual injury\\nreports. by applying our methodology on an attribute and outcome dataset\\ndirectly obtained from 814 injury reports, we show that the frequency-magnitude\\ndistribution of construction safety risk is very similar to that of natural\\nphenomena such as precipitation or earthquakes. motivated by this observation,\\nand drawing on state-of-the-art techniques in hydroclimatology and insurance,\\nwe introduce univariate and bivariate nonparametric stochastic safety risk\\ngenerators, based on kernel density estimators and copulas. these generators\\nenable the user to produce large numbers of synthetic safety risk values\\nfaithfully to the original data, allowing safetyrelated decision-making under\\nuncertainty to be grounded on extensive empirical evidence. just like the\\naccurate modeling and simulation of natural phenomena such as wind or\\nstreamflow is indispensable to successful structure dimensioning or water\\nreservoir management, we posit that improving construction safety calls for the\\naccurate modeling, simulation, and assessment of safety risk. the underlying\\nassumption is that like natural phenomena, construction safety may benefit from\\nbeing studied in an empirical and quantitative way rather than qualitatively\\nwhich is the current industry standard. finally, a side but interesting finding\\nis that attributes related to high energy levels and to human error emerge as\\nstrong risk shapers on the dataset we used to illustrate our methodology.\\n)': False, \"contains(intercomparison of machine learning methods for statistical downscaling:\\n  the case of daily and extreme precipitation\\n  statistical downscaling of global climate models (gcms) allows researchers to\\nstudy local climate change effects decades into the future. a wide range of\\nstatistical models have been applied to downscaling gcms but recent advances in\\nmachine learning have not been explored. in this paper, we compare four\\nfundamental statistical methods, bias correction spatial disaggregation (bcsd),\\nordinary least squares, elastic-net, and support vector machine, with three\\nmore advanced machine learning methods, multi-task sparse structure learning\\n(mssl), bcsd coupled with mssl, and convolutional neural networks to downscale\\ndaily precipitation in the northeast united states. metrics to evaluate of each\\nmethod's ability to capture daily anomalies, large scale climate shifts, and\\nextremes are analyzed. we find that linear methods, led by bcsd, consistently\\noutperform non-linear approaches. the direct application of state-of-the-art\\nmachine learning methods to statistical downscaling does not provide\\nimprovements over simpler, longstanding approaches.\\n)\": False, 'contains(a multi-channel approach for automatic microseismic event localization\\n  using ransac-based arrival time event clustering(ratec)\\n  in the presence of background noise and interference, arrival times picked\\nfrom a surface microseismic data set usually include a number of false picks\\nwhich lead to uncertainty in location estimation. to eliminate false picks and\\nimprove the accuracy of location estimates, we develop a classification\\nalgorithm (ratec) that clusters picked arrival times into event groups based on\\nrandom sampling and fitting moveout curves that approximate hyperbolas. arrival\\ntimes far from the fitted hyperbolas are classified as false picks and removed\\nfrom the data set prior to location estimation. simulations of synthetic data\\nfor a 1-d linear array show that ratec is robust under different noise\\nconditions and generally applicable to various types of media. by generalizing\\nthe underlying moveout model, ratec is extended to the case of a 2-d surface\\nmonitoring array. the effectiveness of event location for the 2-d case is\\ndemonstrated using a data set collected by a 5200-element dense 2-d array\\ndeployed for microearthquake monitoring.\\n)': False, 'contains(constructing an efficient machine learning model for tornado prediction\\n  tornado prediction methods and main mechanisms of tornado genesis were\\nanalyzed. a model, based on the superposition principle, has been built. for\\nefficiency evaluation, the constructed model has been tested on real-life data\\nobtained from the university of oklahoma (usa). it is shown that the\\nconstructed tornado prediction model is more efficient than all previous\\nmodels.\\n)': False, 'contains(ensemble-based kernel learning for a class of data assimilation problems\\n  with imperfect forward simulators\\n  simulator imperfection, often known as model error, is ubiquitous in\\npractical data assimilation problems. despite the enormous efforts dedicated to\\naddressing this problem, properly handling simulator imperfection in data\\nassimilation remains to be a challenging task. in this work, we propose an\\napproach to dealing with simulator imperfection from a point of view of\\nfunctional approximation that can be implemented through a certain machine\\nlearning method, such as kernel-based learning adopted in the current work. to\\nthis end, we start from considering a class of supervised learning problems,\\nand then identify similarities between supervised learning and variational data\\nassimilation. these similarities found the basis for us to develop an\\nensemble-based learning framework to tackle supervised learning problems, while\\nachieving various advantages of ensemble-based methods over the variational\\nones. after establishing the ensemble-based learning framework, we proceed to\\ninvestigate the integration of ensemble-based learning into an ensemble-based\\ndata assimilation framework to handle simulator imperfection. in the course of\\nour investigations, we also develop a strategy to tackle the issue of\\nmulti-modality in supervised-learning problems, and transfer this strategy to\\ndata assimilation problems to help improve assimilation performance. for\\ndemonstration, we apply the ensemble-based learning framework and the\\nintegrated, ensemble-based data assimilation framework to a supervised learning\\nproblem and a data assimilation problem with an imperfect forward simulator,\\nrespectively. the experiment results indicate that both frameworks achieve good\\nperformance in relevant case studies, and that functional approximation through\\nmachine learning may serve as a viable way to account for simulator\\nimperfection in data assimilation problems.\\n)': False, \"contains(automatic seismic salt interpretation with deep convolutional neural\\n  networks\\n  one of the most crucial tasks in seismic reflection imaging is to identify\\nthe salt bodies with high precision. traditionally, this is accomplished by\\nvisually picking the salt/sediment boundaries, which requires a great amount of\\nmanual work and may introduce systematic bias. with recent progress of deep\\nlearning algorithm and growing computational power, a great deal of efforts\\nhave been made to replace human effort with machine power in salt body\\ninterpretation. currently, the method of convolutional neural networks (cnn) is\\nrevolutionizing the computer vision field and has been a hot topic in the image\\nanalysis. in this paper, the benefits of cnn-based classification are\\ndemonstrated by using a state-of-art network structure u-net, along with the\\nresidual learning framework resnet, to delineate salt body with high precision.\\nnetwork adjustments, including the exponential linear units (elu) activation\\nfunction, the lov\\\\'{a}sz-softmax loss function, and stratified $k$-fold\\ncross-validation, have been deployed to further improve the prediction\\naccuracy. the preliminary result using seg advanced modeling (seam) data shows\\ngood agreement between the predicted salt body and manually interpreted salt\\nbody, especially in areas with weak reflections. this indicates the great\\npotential of applying cnn for salt-related interpretations.\\n)\": False, 'contains(the error is the feature: how to forecast lightning using a model\\n  prediction error\\n  despite the progress within the last decades, weather forecasting is still a\\nchallenging and computationally expensive task. current satellite-based\\napproaches to predict thunderstorms are usually based on the analysis of the\\nobserved brightness temperatures in different spectral channels and emit a\\nwarning if a critical threshold is reached. recent progress in data science\\nhowever demonstrates that machine learning can be successfully applied to many\\nresearch fields in science, especially in areas dealing with large datasets. we\\ntherefore present a new approach to the problem of predicting thunderstorms\\nbased on machine learning. the core idea of our work is to use the error of\\ntwo-dimensional optical flow algorithms applied to images of meteorological\\nsatellites as a feature for machine learning models. we interpret that optical\\nflow error as an indication of convection potentially leading to thunderstorms\\nand lightning. to factor in spatial proximity we use various manual convolution\\nsteps. we also consider effects such as the time of day or the geographic\\nlocation. we train different tree classifier models as well as a neural network\\nto predict lightning within the next few hours (called nowcasting in\\nmeteorology) based on these features. in our evaluation section we compare the\\npredictive power of the different models and the impact of different features\\non the classification result. our results show a high accuracy of 96% for\\npredictions over the next 15 minutes which slightly decreases with increasing\\nforecast period but still remains above 83% for forecasts of up to five hours.\\nthe high false positive rate of nearly 6% however needs further investigation\\nto allow for an operational use of our approach.\\n)': False, 'contains(an unsupervised method for estimating the global horizontal irradiance\\n  from photovoltaic power measurements\\n  in this paper, we present a method to determine the global horizontal\\nirradiance (ghi) from the power measurements of one or more pv systems, located\\nin the same neighborhood. the method is completely unsupervised and is based on\\na physical model of a pv plant. the precise assessment of solar irradiance is\\npivotal for the forecast of the electric power generated by photovoltaic (pv)\\nplants. however, on-ground measurements are expensive and are generally not\\nperformed for small and medium-sized pv plants. satellite-based services\\nrepresent a valid alternative to on site measurements, but their space-time\\nresolution is limited. results from two case studies located in switzerland are\\npresented. the performance of the proposed method at assessing ghi is compared\\nwith that of free and commercial satellite services. our results show that the\\npresented method is generally better than satellite-based services, especially\\nat high temporal resolutions.\\n)': False, 'contains(kernel embedding approaches to orbit determination of spacecraft\\n  clusters\\n  this paper presents a novel formulation and solution of orbit determination\\nover finite time horizons as a learning problem. we present an approach to\\norbit determination under very broad conditions that are satisfied for n-body\\nproblems. these weak conditions allow us to perform orbit determination with\\nnoisy and highly non-linear observations such as those presented by range-rate\\nonly (doppler only) observations. we show that domain generalization and\\ndistribution regression techniques can learn to estimate orbits of a group of\\nsatellites and identify individual satellites especially with prior\\nunderstanding of correlations between orbits and provide asymptotic convergence\\nconditions. the approach presented requires only visibility and observability\\nof the underlying state from observations and is particularly useful for\\nautonomous spacecraft operations using low-cost ground stations or sensors. we\\nvalidate the orbit determination approach using observations of two spacecraft\\n(grifex and mcubed-2) along with synthetic datasets of multiple spacecraft\\ndeployments and lunar orbits. we also provide a comparison with the standard\\ntechniques (ekf) under highly noisy conditions.\\n)': False, \"contains(deep learning and data assimilation for real-time production prediction\\n  in natural gas wells\\n  the prediction of the gas production from mature gas wells, due to their\\ncomplex end-of-life behavior, is challenging and crucial for operational\\ndecision making. in this paper, we apply a modified deep lstm model for\\nprediction of the gas flow rates in mature gas wells, including the\\nuncertainties in input parameters. additionally, due to changes in the system\\nin time and in order to increase the accuracy and robustness of the prediction,\\nthe ensemble kalman filter (enkf) is used to update the flow rate predictions\\nbased on new observations. the developed approach was tested on the data from\\ntwo mature gas production wells in which their production is highly dynamic and\\nsuffering from salt deposition. the results show that the flow predictions\\nusing the enkf updated model leads to better jeffreys' j-divergences than the\\npredictions without the enkf model updating scheme.\\n)\": False, 'contains(a mapreduce based big-data framework for object extraction from mosaic\\n  satellite images\\n  we propose a framework stitching of vector representations of large scale\\nraster mosaic images in distributed computing model. in this way, the negative\\neffect of the lack of resources of the central system and scalability problem\\ncan be eliminated. the product obtained by this study can be used in\\napplications requiring spatial and temporal analysis on big satellite map\\nimages. this study also shows that big data frameworks are not only used in\\napplications of text-based data mining and machine learning algorithms, but\\nalso used in applications of algorithms in image processing. the effectiveness\\nof the product realized with this project is also going to be proven by\\nscalability and performance tests performed on real world landsat-8 satellite\\nimages.\\n)': False, 'contains(stacked autoencoders based machine learning for noise reduction and\\n  signal reconstruction in geophysical data\\n  autoencoders are neural network formulations where the input and output of\\nthe network are identical and the goal is to identify the hidden representation\\nin the provided datasets. generally, autoencoders project the data nonlinearly\\nonto a lower dimensional hidden space, where the important features get\\nhighlighted and interpretation of the data becomes easier. recent studies have\\nshown that even in the presence of noise in the input data, autoencoders can be\\ntrained to reconstruct the noisefree component of the data from the\\nreduced-dimensional hidden space.\\n  in this paper, we explore the application of autoencoders within the scope of\\ndenoising geophysical datasets using a data-driven methodology. the autoencoder\\nformulation is discussed, and a stacked variant of deep autoencoders is\\nproposed. the proposed method involves locally training the weights first using\\nbasic autoencoders, each comprising a single hidden layer. using these\\ninitialized weights as starting points in the optimization model, the full\\nautoencoder network is then trained in the second step. the applicability of\\ndenoising autoencoders has been demonstrated on a basic mathematical example\\nand several geophysical examples. for all the cases, autoencoders are found to\\nsignificantly reduce the noise in the input data.\\n)': False, 'contains(water preservation in soan river basin using deep learning techniques\\n  water supplies are crucial for the development of living beings. however,\\nchange in the hydrological process i.e. climate and land usage are the key\\nissues. sustaining water level and accurate estimating for dynamic conditions\\nis a critical job for hydrologists, but predicting hydrological extremes is an\\nopen issue. in this paper, we proposed two deep learning techniques and three\\nmachine learning algorithms to predict stream flow, given the present climate\\nconditions. the results showed that the recurrent neural network (rnn) or long\\nshort-term memory (lstm), an artificial neural network based method, outperform\\nother conventional and machine-learning algorithms for predicting stream flow.\\nfurthermore, we analyzed that stream flow is directly affected by\\nprecipitation, land usage, and temperature. these indexes are critical, which\\ncan be used by hydrologists to identify the potential for stream flow. we make\\nthe dataset publicly available (https://github.com/sadaqat007/dataset) so that\\nothers should be able to replicate and build upon the results published.\\n)': False, 'contains(predicting food security outcomes using convolutional neural networks\\n  (cnns) for satellite tasking\\n  obtaining reliable data describing local food security metrics (fsm) at a\\ngranularity that is informative to policy-makers requires expensive and\\nlogistically difficult surveys, particularly in the developing world. we train\\na cnn on publicly available satellite data describing land cover classification\\nand use both transfer learning and direct training to build a model for fsm\\nprediction purely from satellite imagery data. we then propose efficient\\ntasking algorithms for high resolution satellite assets via transfer learning,\\nmarkovian search algorithms, and bayesian networks.\\n)': False, 'contains(a machine learning framework to forecast wave conditions\\n  a~machine learning framework is developed to estimate ocean-wave conditions.\\nby supervised training of machine learning models on many thousands of\\niterations of a physics-based wave model, accurate representations of\\nsignificant wave heights and period can be used to predict ocean conditions. a\\nmodel of monterey bay was used as the example test site; it was forced by\\nmeasured wave conditions, ocean-current nowcasts, and reported winds. these\\ninput data along with model outputs of spatially variable wave heights and\\ncharacteristic period were aggregated into supervised learning training and\\ntest data sets, which were supplied to machine learning models. these machine\\nlearning models replicated wave heights with a root-mean-squared error of 9cm\\nand correctly identify over 90% of the characteristic periods for the test-data\\nsets. impressively, transforming model inputs to outputs through matrix\\noperations requires only a fraction (<1/1,000) of the computation time compared\\nto forecasting with the physics-based model.\\n)': False, 'contains(seismic data denoising and deblending using deep learning\\n  an important step of seismic data processing is removing noise, including\\ninterference due to simultaneous and blended sources, from the recorded data.\\ntraditional methods are time-consuming to apply as they often require manual\\nchoosing of parameters to obtain good results. we use deep learning, with a\\nu-net model incorporating a resnet architecture pretrained on imagenet and\\nfurther trained on synthetic seismic data, to perform this task. the method is\\napplied to common offset gathers, with adjacent offset gathers of the gather\\nbeing denoised provided as additional input channels. here we show that this\\napproach leads to a method that removes noise from several datasets recorded in\\ndifferent parts of the world with moderate success. we find that providing\\nthree adjacent offset gathers on either side of the gather being denoised is\\nmost effective. as this method does not require parameters to be chosen, it is\\nmore automated than traditional methods.\\n)': False, 'contains(from bach to the beatles: the simulation of human tonal expectation\\n  using ecologically-trained predictive models\\n  tonal structure is in part conveyed by statistical regularities between\\nmusical events, and research has shown that computational models reflect tonal\\nstructure in music by capturing these regularities in schematic constructs like\\npitch histograms. of the few studies that model the acquisition of perceptual\\nlearning from musical data, most have employed self-organizing models that\\nlearn a topology of static descriptions of musical contexts. also, the stimuli\\nused to train these models are often symbolic rather than acoustically faithful\\nrepresentations of musical material. in this work we investigate whether\\nsequential predictive models of musical memory (specifically, recurrent neural\\nnetworks), trained on audio from commercial cd recordings, induce tonal\\nknowledge in a similar manner to listeners (as shown in behavioral studies in\\nmusic perception). our experiments indicate that various types of recurrent\\nneural networks produce musical expectations that clearly convey tonal\\nstructure. furthermore, the results imply that although implicit knowledge of\\ntonal structure is a necessary condition for accurate musical expectation, the\\nmost accurate predictive models also use other cues beyond the tonal structure\\nof the musical context.\\n)': False, 'contains(structured priors for sparse-representation-based hyperspectral image classification\\n\\n-pixel-wise classification, where each pixel is assigned to a predefined class, is one of the most important procedures in hyperspectral image (hsi) analysis. by representing a test pixel as 4 a linear combination of a small subset of labeled pixels, a sparse 1 representation classifier (src) gives rather plausible results 0 compared with that of traditional classifiers such as the support 2 vector machine (svm). recently, by incorporating additional n structured sparsity priors, the second generation srcs have a appeared in the literature and are reported to further improve j the performance of hsi. these priors are based on exploiting 61 itnhheersepnattiasltrduecptuernedeonfcitehse bdeticwteioennartyh,e onreibgohtbho.riinng tphiixselps,aptehre,\\n)': False, 'contains(scalable and efficient hypothesis testing with random forests\\n  throughout the last decade, random forests have established themselves as\\namong the most accurate and popular supervised learning methods. while their\\nblack-box nature has made their mathematical analysis difficult, recent work\\nhas established important statistical properties like consistency and\\nasymptotic normality by considering subsampling in lieu of bootstrapping.\\nthough such results open the door to traditional inference procedures, all\\nformal methods suggested thus far place severe restrictions on the testing\\nframework and their computational overhead precludes their practical scientific\\nuse. here we propose a permutation-style testing approach to formally assess\\nfeature significance. we establish asymptotic validity of the test via\\nexchangeability arguments and show that the test maintains high power with\\norders of magnitude fewer computations. as importantly, the procedure scales\\neasily to big data settings where large training and testing sets may be\\nemployed without the need to construct additional models. simulations and\\napplications to ecological data where random forests have recently shown\\npromise are provided.\\n)': False, 'contains(predicting hurricane trajectories using a recurrent neural network\\n  hurricanes are cyclones circulating about a defined center whose closed wind\\nspeeds exceed 75 mph originating over tropical and subtropical waters. at\\nlandfall, hurricanes can result in severe disasters. the accuracy of predicting\\ntheir trajectory paths is critical to reduce economic loss and save human\\nlives. given the complexity and nonlinearity of weather data, a recurrent\\nneural network (rnn) could be beneficial in modeling hurricane behavior. we\\npropose the application of a fully connected rnn to predict the trajectory of\\nhurricanes. we employed the rnn over a fine grid to reduce typical truncation\\nerrors. we utilized their latitude, longitude, wind speed, and pressure\\npublicly provided by the national hurricane center (nhc) to predict the\\ntrajectory of a hurricane at 6-hour intervals. results show that this proposed\\ntechnique is competitive to methods currently employed by the nhc and can\\npredict up to approximately 120 hours of hurricane path.\\n)': False, 'contains(model selection techniques -- an overview\\n  in the era of big data, analysts usually explore various statistical models\\nor machine learning methods for observed data in order to facilitate scientific\\ndiscoveries or gain predictive power. whatever data and fitting procedures are\\nemployed, a crucial step is to select the most appropriate model or method from\\na set of candidates. model selection is a key ingredient in data analysis for\\nreliable and reproducible statistical inference or prediction, and thus central\\nto scientific studies in fields such as ecology, economics, engineering,\\nfinance, political science, biology, and epidemiology. there has been a long\\nhistory of model selection techniques that arise from researches in statistics,\\ninformation theory, and signal processing. a considerable number of methods\\nhave been proposed, following different philosophies and exhibiting varying\\nperformances. the purpose of this article is to bring a comprehensive overview\\nof them, in terms of their motivation, large sample performance, and\\napplicability. we provide integrated and practically relevant discussions on\\ntheoretical properties of state-of- the-art model selection approaches. we also\\nshare our thoughts on some controversial views on the practice of model\\nselection.\\n)': False, 'contains(deepflow: history matching in the space of deep generative models\\n  the calibration of a reservoir model with observed transient data of fluid\\npressures and rates is a key task in obtaining a predictive model of the flow\\nand transport behaviour of the earth\\'s subsurface. the model calibration task,\\ncommonly referred to as \"history matching\", can be formalised as an ill-posed\\ninverse problem where we aim to find the underlying spatial distribution of\\npetrophysical properties that explain the observed dynamic data. we use a\\ngenerative adversarial network pretrained on geostatistical object-based models\\nto represent the distribution of rock properties for a synthetic model of a\\nhydrocarbon reservoir. the dynamic behaviour of the reservoir fluids is\\nmodelled using a transient two-phase incompressible darcy formulation. we\\ninvert for the underlying reservoir properties by first modeling property\\ndistributions using the pre-trained generative model then using the adjoint\\nequations of the forward problem to perform gradient descent on the latent\\nvariables that control the output of the generative model. in addition to the\\ndynamic observation data, we include well rock-type constraints by introducing\\nan additional objective function. our contribution shows that for a synthetic\\ntest case, we are able to obtain solutions to the inverse problem by optimising\\nin the latent variable space of a deep generative model, given a set of\\ntransient observations of a non-linear forward problem.\\n)': False, 'contains(bootstrapping robotic ecological perception from a limited set of\\n  hypotheses through interactive perception\\n  to solve its task, a robot needs to have the ability to interpret its\\nperceptions. in vision, this interpretation is particularly difficult and\\nrelies on the understanding of the structure of the scene, at least to the\\nextent of its task and sensorimotor abilities. a robot with the ability to\\nbuild and adapt this interpretation process according to its own tasks and\\ncapabilities would push away the limits of what robots can achieve in a non\\ncontrolled environment. a solution is to provide the robot with processes to\\nbuild such representations that are not specific to an environment or a\\nsituation. a lot of works focus on objects segmentation, recognition and\\nmanipulation. defining an object solely on the basis of its visual appearance\\nis challenging given the wide range of possible objects and environments.\\ntherefore, current works make simplifying assumptions about the structure of a\\nscene. such assumptions reduce the adaptivity of the object extraction process\\nto the environments in which the assumption holds. to limit such assumptions,\\nwe introduce an exploration method aimed at identifying moveable elements in a\\nscene without considering the concept of object. by using the interactive\\nperception framework, we aim at bootstrapping the acquisition process of a\\nrepresentation of the environment with a minimum of context specific\\nassumptions. the robotic system builds a perceptual map called relevance map\\nwhich indicates the moveable parts of the current scene. a classifier is\\ntrained online to predict the category of each region (moveable or\\nnon-moveable). it is also used to select a region with which to interact, with\\nthe goal of minimizing the uncertainty of the classification. a specific\\nclassifier is introduced to fit these needs: the collaborative mixture models\\nclassifier. the method is tested on a set of scenarios of increasing\\ncomplexity, using both simulations and a pr2 robot.\\n)': False, 'contains(digital ecosystems: optimisation by a distributed intelligence\\n  can intelligence optimise digital ecosystems? how could a distributed\\nintelligence interact with the ecosystem dynamics? can the software components\\nthat are part of genetic selection be intelligent in themselves, as in an\\nadaptive technology? we consider the effect of a distributed intelligence\\nmechanism on the evolutionary and ecological dynamics of our digital ecosystem,\\nwhich is the digital counterpart of a biological ecosystem for evolving\\nsoftware services in a distributed network. we investigate neural networks and\\nsupport vector machine for the learning based pattern recognition functionality\\nof our distributed intelligence. simulation results imply that the digital\\necosystem performs better with the application of a distributed intelligence,\\nmarginally more effectively when powered by support vector machine than neural\\nnetworks, and suggest that it can contribute to optimising the operation of our\\ndigital ecosystem.\\n)': False, 'contains(using machine learning to parameterize moist convection: potential for\\n  modeling of climate, climate change and extreme events\\n  the parameterization of moist convection contributes to uncertainty in\\nclimate modeling and numerical weather prediction. machine learning (ml) can be\\nused to learn new parameterizations directly from high-resolution model output,\\nbut it remains poorly understood how such parameterizations behave when fully\\ncoupled in a general circulation model (gcm) and whether they are useful for\\nsimulations of climate change or extreme events. here, we focus on these issues\\nusing idealized tests in which an ml-based parameterization is trained on\\noutput from a conventional parameterization and its performance is assessed in\\nsimulations with a gcm. we use an ensemble of decision trees (random forest) as\\nthe ml algorithm, and this has the advantage that it automatically ensures\\nconservation of energy and non-negativity of surface precipitation. the gcm\\nwith the ml convective parameterization runs stably and accurately captures\\nimportant climate statistics including precipitation extremes without the need\\nfor special training on extremes. climate change between a control climate and\\na warm climate is not captured if the ml parameterization is only trained on\\nthe control climate, but it is captured if the training includes samples from\\nboth climates. remarkably, climate change is also captured when training only\\non the warm climate, and this is because the extratropics of the warm climate\\nprovides training samples for the tropics of the control climate. in addition\\nto being potentially useful for the simulation of climate, we show that ml\\nparameterizations can be interrogated to provide diagnostics of the interaction\\nbetween convection and the large-scale environment.\\n)': False, 'contains(spatial-spectral regularized local scaling cut for dimensionality\\n  reduction in hyperspectral image classification\\n  dimensionality reduction (dr) methods have attracted extensive attention to\\nprovide discriminative information and reduce the computational burden of the\\nhyperspectral image (hsi) classification. however, the dr methods face many\\nchallenges due to limited training samples with high dimensional spectra. to\\naddress this issue, a graph-based spatial and spectral regularized local\\nscaling cut (ssrlsc) for dr of hsi data is proposed. the underlying idea of the\\nproposed method is to utilize the information from both the spectral and\\nspatial domains to achieve better classification accuracy than its spectral\\ndomain counterpart. in ssrlsc, a guided filter is initially used to smoothen\\nand homogenize the pixels of the hsi data in order to preserve the pixel\\nconsistency. this is followed by generation of between-class and within-class\\ndissimilarity matrices in both spectral and spatial domains by regularized\\nlocal scaling cut (rlsc) and neighboring pixel local scaling cut (nplsc)\\nrespectively. finally, we obtain the projection matrix by optimizing the\\nupdated spatial-spectral between-class and total-class dissimilarity. the\\neffectiveness of the proposed dr algorithm is illustrated with two popular\\nreal-world hsi datasets.\\n)': False, 'contains(generating realistic geology conditioned on physical measurements with\\n  generative adversarial networks\\n  an important problem in geostatistics is to build models of the subsurface of\\nthe earth given physical measurements at sparse spatial locations. typically,\\nthis is done using spatial interpolation methods or by reproducing patterns\\nfrom a reference image. however, these algorithms fail to produce realistic\\npatterns and do not exhibit the wide range of uncertainty inherent in the\\nprediction of geology. in this paper, we show how semantic inpainting with\\ngenerative adversarial networks can be used to generate varied realizations of\\ngeology which honor physical measurements while matching the expected\\ngeological patterns. in contrast to other algorithms, our method scales well\\nwith the number of data points and mimics a distribution of patterns as opposed\\nto a single pattern or image. the generated conditional samples are state of\\nthe art.\\n)': False, 'contains(sea surface temperature prediction and reconstruction using patch-level\\n  neural network representations\\n  the forecasting and reconstruction of ocean and atmosphere dynamics from\\nsatellite observation time series are key challenges. while model-driven\\nrepresentations remain the classic approaches, data-driven representations\\nbecome more and more appealing to benefit from available large-scale\\nobservation and simulation datasets. in this work we investigate the relevance\\nof recently introduced bilinear residual neural network representations, which\\nmimic numerical integration schemes such as runge-kutta, for the forecasting\\nand assimilation of geophysical fields from satellite-derived remote sensing\\ndata. as a case-study, we consider satellite-derived sea surface temperature\\ntime series off south africa, which involves intense and complex upper ocean\\ndynamics. our numerical experiments demonstrate that the proposed patch-level\\nneural-network-based representations outperform other data-driven models,\\nincluding analog schemes, both in terms of forecasting and missing data\\ninterpolation performance with a relative gain up to 50\\\\% for highly dynamic\\nareas.\\n)': False, 'contains(feedback neural network for weakly supervised geo-semantic segmentation\\n  learning from weakly-supervised data is one of the main challenges in machine\\nlearning and computer vision, especially for tasks such as image semantic\\nsegmentation where labeling is extremely expensive and subjective. in this\\npaper, we propose a novel neural network architecture to perform\\nweakly-supervised learning by suppressing irrelevant neuron activations. it\\nlocalizes objects of interest by learning from image-level categorical labels\\nin an end-to-end manner. we apply this algorithm to a practical challenge of\\ntransforming satellite images into a map of settlements and individual\\nbuildings. experimental results show that the proposed algorithm achieves\\nsuperior performance and efficiency when compared with various baseline models.\\n)': False, 'contains(geology prediction based on operation data of tbm: comparison between\\n  deep neural network and statistical learning methods\\n  tunnel boring machine (tbm) is a complex engineering system widely used for\\ntunnel construction. in view of the complicated construction environments, it\\nis necessary to predict geology conditions prior to excavation. in recent\\nyears, massive operation data of tbm has been recorded, and mining these data\\ncan provide important references and useful information for designers and\\noperators of tbm. in this work, a geology prediction approach is proposed based\\non deep neural network and operation data. it can provide relatively accurate\\ngeology prediction results ahead of the tunnel face compared with the other\\nprediction models based on statistical learning methods. the application case\\nstudy on a tunnel in china shows that the proposed approach can accurately\\nestimate the geological conditions prior to excavation, especially for the\\nshort range ahead of training data. this work can be regarded as a good\\ncomplement to the geophysical prospecting approach during the construction of\\ntunnels, and also highlights the applicability and potential of deep neural\\nnetworks for other data mining tasks of tbms.\\n)': False, 'contains(a novel prestack sparse azimuthal avo inversion\\n  in this paper we demonstrate a new algorithm for sparse prestack azimuthal\\navo inversion. a novel euclidean prior model is developed to at once respect\\nsparseness in the layered earth and smoothness in the model of reflectivity.\\nrecognizing that methods of artificial intelligence and bayesian computation\\nare finding an every increasing role in augmenting the process of\\ninterpretation and analysis of geophysical data, we derive a generalized\\nmatrix-variate model of reflectivity in terms of orthogonal basis functions,\\nsubject to sparse constraints. this supports a direct application of machine\\nlearning methods, in a way that can be mapped back onto the physical principles\\nknown to govern reflection seismology. as a demonstration we present an\\napplication of these methods to the marcellus shale. attributes extracted using\\nthe azimuthal inversion are clustered using an unsupervised learning algorithm.\\ninterpretation of the clusters is performed in the context of the ruger model\\nof azimuthal avo.\\n)': False, 'contains(forward amortized inference for likelihood-free variational\\n  marginalization\\n  in this paper, we introduce a new form of amortized variational inference by\\nusing the forward kl divergence in a joint-contrastive variational loss. the\\nresulting forward amortized variational inference is a likelihood-free method\\nas its gradient can be sampled without bias and without requiring any\\nevaluation of either the model joint distribution or its derivatives. we prove\\nthat our new variational loss is optimized by the exact posterior marginals in\\nthe fully factorized mean-field approximation, a property that is not shared\\nwith the more conventional reverse kl inference. furthermore, we show that\\nforward amortized inference can be easily marginalized over large families of\\nlatent variables in order to obtain a marginalized variational posterior. we\\nconsider two examples of variational marginalization. in our first example we\\ntrain a bayesian forecaster for predicting a simplified chaotic model of\\natmospheric convection. in the second example we train an amortized variational\\napproximation of a bayesian optimal classifier by marginalizing over the model\\nspace. the result is a powerful meta-classification network that can solve\\narbitrary classification problems without further training.\\n)': False, 'contains(a supervised geometry-aware mapping approach for classification of\\n  hyperspectral images\\n  the lack of proper class discrimination among the hyperspectral (hs) data\\npoints poses a potential challenge in hs classification. to address this issue,\\nthis paper proposes an optimal geometry-aware transformation for enhancing the\\nclassification accuracy. the underlying idea of this method is to obtain a\\nlinear projection matrix by solving a nonlinear objective function based on the\\nintrinsic geometrical structure of the data. the objective function is\\nconstructed to quantify the discrimination between the points from dissimilar\\nclasses on the projected data space. then the obtained projection matrix is\\nused to linearly map the data to more discriminative space. the effectiveness\\nof the proposed transformation is illustrated with three benchmark real-world\\nhs data sets. the experiments reveal that the classification and dimensionality\\nreduction methods on the projected discriminative space outperform their\\ncounterpart in the original space.\\n)': False, 'contains(removing clouds and recovering ground observations in satellite image\\n  sequences via temporally contiguous robust matrix completion\\n  we consider the problem of removing and replacing clouds in satellite image\\nsequences, which has a wide range of applications in remote sensing. our\\napproach first detects and removes the cloud-contaminated part of the image\\nsequences. it then recovers the missing scenes from the clean parts using the\\nproposed \"tecromac\" (temporally contiguous robust matrix completion) objective.\\nthe objective function balances temporal smoothness with a low rank solution\\nwhile staying close to the original observations. the matrix whose the rows are\\npixels and columnsare days corresponding to the image, has low-rank because the\\npixels reflect land-types such as vegetation, roads and lakes and there are\\nrelatively few variations as a result. we provide efficient optimization\\nalgorithms for tecromac, so we can exploit images containing millions of\\npixels. empirical results on real satellite image sequences, as well as\\nsimulated data, demonstrate that our approach is able to recover underlying\\nimages from heavily cloud-contaminated observations.\\n)': False, 'contains(penobscot dataset: fostering machine learning development for seismic\\n  interpretation\\n  we have seen in the past years the flourishing of machine and deep learning\\nalgorithms in several applications such as image classification and\\nsegmentation, object detection and recognition, among many others. this was\\nonly possible, in part, because datasets like imagenet -- with +14 million\\nlabeled images -- were created and made publicly available, providing\\nresearches with a common ground to compare their advances and extend the\\nstate-of-the-art. although we have seen an increasing interest in machine\\nlearning in geosciences as well, we will only be able to achieve a significant\\nimpact in our community if we collaborate to build such a common basis. this is\\neven more difficult when it comes to the oil&gas industry, in which\\nconfidentiality and commercial interests often hinder the sharing of datasets\\nwith others. in this letter, we present the penobscot interpretation dataset,\\nour contribution to the development of machine learning in geosciences, more\\nspecifically in seismic interpretation. the penobscot 3d seismic dataset was\\nacquired in the scotian shelf, offshore nova scotia, canada. the data is\\npublicly available and comprises pre- and pos-stack data, 5 horizons and well\\nlogs of 2 wells. however, for the dataset to be of practical use for our tasks,\\nwe had to reinterpret the seismic, generating 7 horizons separating different\\nseismic facies intervals. the interpreted horizons were used to generated\\n+100,000 labeled images for inlines and crosslines. to demonstrate the utility\\nof our dataset, results of two experiments are presented.\\n)': False, 'contains(fusion of hyperspectral and ground penetrating radar to estimate soil\\n  moisture\\n  in this contribution, we investigate the potential of hyperspectral data\\ncombined with either simulated ground penetrating radar (gpr) or simulated\\n(sensor-like) soil-moisture data to estimate soil moisture. we propose two\\nsimulation approaches to extend a given multi-sensor dataset which contains\\nsparse gpr data. in the first approach, simulated gpr data is generated either\\nby an interpolation along the time axis or by a machine learning model. the\\nsecond approach includes the simulation of soil-moisture along the gpr profile.\\nthe soil-moisture estimation is improved significantly by the fusion of\\nhyperspectral and gpr data. in contrast, the combination of simulated,\\nsensor-like soil-moisture values and hyperspectral data achieves the worst\\nregression performance. in conclusion, the estimation of soil moisture with\\nhyperspectral and gpr data engages further investigations.\\n)': False, \"contains(learning ensembles of anomaly detectors on synthetic data\\n  the main aim of this work is to develop and implement an automatic anomaly\\ndetection algorithm for meteorological time-series. to achieve this goal we\\ndevelop an approach to constructing an ensemble of anomaly detectors in\\ncombination with adaptive threshold selection based on artificially generated\\nanomalies. we demonstrate the efficiency of the proposed method by integrating\\nthe corresponding implementation into ``minimax-94'' road weather information\\nsystem.\\n)\": False, 'contains(leak event identification in water systems using high order crf\\n  today, detection of anomalous events in civil infrastructures (e.g. water\\npipe breaks and leaks) is time consuming and often takes hours or days. pipe\\nbreakage as one of the most frequent types of failure of water networks often\\ncauses community disruptions ranging from temporary interruptions in services\\nto extended loss of business and relocation of residents. in this project, we\\ndesign and implement a two-phase approach for leak event identification, which\\nleverages dynamic data from multiple information sources including iot sensing\\ndata (pressure values and/or flow rates), geophysical data (water systems), and\\nhuman inputs (tweets posted on twitter). in the approach, a high order\\nconditional random field (crf) is constructed that enforces predictions based\\non iot observations consistent with human inputs to improve the performance of\\nevent identifications.\\n  considering the physical water network as a graph, a crf model is built and\\nlearned by the structured support vector machine (ssvm) using node features\\nsuch as water pressure and flow rate. after that, we built the high order crf\\nsystem by enforcing twitter leakage detection information. an optimal inference\\nalgorithm is proposed for the adapted high order crf model. experimental\\nresults show the effectiveness of our system.\\n)': False, 'contains(meteorological time series forecasting based on mlp modelling using\\n  heterogeneous transfer functions\\n  in this paper, we propose to study four meteorological and seasonal time\\nseries coupled with a multi-layer perceptron (mlp) modeling. we chose to\\ncombine two transfer functions for the nodes of the hidden layer, and to use a\\ntemporal indicator (time index as input) in order to take into account the\\nseasonal aspect of the studied time series. the results of the prediction\\nconcern two years of measurements and the learning step, eight independent\\nyears. we show that this methodology can improve the accuracy of meteorological\\ndata estimation compared to a classical mlp modelling with a homogenous\\ntransfer function.\\n)': False, 'contains(a machine learning nowcasting method based on real-time reanalysis data\\n  despite marked progress over the past several decades, convective storm\\nnowcasting remains a challenge because most nowcasting systems are based on\\nlinear extrapolation of radar reflectivity without much consideration for other\\nmeteorological fields. the variational doppler radar analysis system (vdras) is\\nan advanced convective-scale analysis system capable of providing analysis of\\n3-d wind, temperature, and humidity by assimilating doppler radar observations.\\nalthough potentially useful, it is still an open question as to how to use\\nthese fields to improve nowcasting. in this study, we present results from our\\nfirst attempt at developing a support vector machine (svm) box-based nowcasting\\n(sbow) method under the machine learning framework using vdras analysis data.\\nthe key design points of sbow are as follows: 1) the study domain is divided\\ninto many position-fixed small boxes and the nowcasting problem is transformed\\ninto one question, i.e., will a radar echo > 35 dbz appear in a box in 30\\nminutes? 2) box-based temporal and spatial features, which include time trends\\nand surrounding environmental information, are elaborately constructed, and 3)\\nthe box-based constructed features are used to first train the svm classifier,\\nand then the trained classifier is used to make predictions. compared with\\ncomplicated and expensive expert systems, the above design of sbow allows the\\nsystem to be small, compact, straightforward, and easy to maintain and expand\\nat low cost. the experimental results show that, although no complicated\\ntracking algorithm is used, sbow can predict the storm movement trend and storm\\ngrowth with reasonable skill.\\n)': False, 'contains(deepsat - a learning framework for satellite imagery\\n  satellite image classification is a challenging problem that lies at the\\ncrossroads of remote sensing, computer vision, and machine learning. due to the\\nhigh variability inherent in satellite data, most of the current object\\nclassification approaches are not suitable for handling satellite datasets. the\\nprogress of satellite image analytics has also been inhibited by the lack of a\\nsingle labeled high-resolution dataset with multiple class labels. the\\ncontributions of this paper are twofold - (1) first, we present two new\\nsatellite datasets called sat-4 and sat-6, and (2) then, we propose a\\nclassification framework that extracts features from an input image, normalizes\\nthem and feeds the normalized feature vectors to a deep belief network for\\nclassification. on the sat-4 dataset, our best network produces a\\nclassification accuracy of 97.95% and outperforms three state-of-the-art object\\nrecognition algorithms, namely - deep belief networks, convolutional neural\\nnetworks and stacked denoising autoencoders by ~11%. on sat-6, it produces a\\nclassification accuracy of 93.9% and outperforms the other algorithms by ~15%.\\ncomparative studies with a random forest classifier show the advantage of an\\nunsupervised learning approach over traditional supervised learning techniques.\\na statistical analysis based on distribution separability criterion and\\nintrinsic dimensionality estimation substantiates the effectiveness of our\\napproach in learning better representations for satellite imagery.\\n)': False, 'contains(searching for hot subdwarf stars from the lamost spectra. iii.\\n  classifying the hot subdwarf stars from lamost dr4 using deep learning method\\n  hot subdwarf stars are core he burning stars located at the blue end of the\\nhorizontal branch, also known as the extreme horizontal branch. the properties\\nof hot subdwarf stars are important for our understanding of the stellar\\nastrophysics, globular clusters and galaxies. the spectra of hot subdwarf stars\\ncan provide us with the detailed information of the stellar atmospheric\\nparameters (such as effective temperature, gravity, and helium abundances),\\nwhich is important to clarify the astrophysical and statistical properties of\\nhot subdwarf stars. these properties can provide important constraint on the\\ntheoretical models of hot subdwarf stars. searching for hot subdwarf stars from\\nthe spectra data obtained by the large sky area multi-object fiber\\nspectroscopic telescope (lamost) can significantly enlarge the sample size of\\nhot subdwarf stars, and help us better study the nature of hot subdwarf stars.\\nin this paper we study a new method of searching for hot subdwarf stars from\\nlamost spectra using convolutional neural networks and support vector machine\\n(cnn+svm). the experiment on the spectra from lamost dr4 shows that cnn+svm can\\nclassify the hot subdwarf stars accurately: the accuracy is 88.98$\\\\%$ and the\\nrecall is 94.38 $\\\\%$. our research provides a new machine learning tool for\\nsearching for hot subdwarf stars in large spectroscopic surveys.\\n)': False, 'contains(seeing the wind: visual wind speed prediction with a coupled\\n  convolutional and recurrent neural network\\n  wind energy resource quantification, air pollution monitoring, and weather\\nforecasting all rely on rapid, accurate measurement of local wind conditions.\\nvisual observations of the effects of wind---the swaying of trees and flapping\\nof flags, for example---encode information regarding local wind conditions that\\ncan potentially be leveraged for visual anemometry that is inexpensive and\\nubiquitous. here, we demonstrate a coupled convolutional neural network and\\nrecurrent neural network architecture that extracts the wind speed encoded in\\nvisually recorded flow-structure interactions of a flag in naturally occurring\\nwind. predictions for wind speeds ranging from 0.75-11 m/s showed agreement\\nwith measurements from a cup anemometer on site, with a root-mean-square error\\napproaching the natural wind speed variability due to atmospheric turbulence.\\ngeneralizability of the network was demonstrated by successful prediction of\\nwind speed based on recordings of other flags in the field and in a controlled\\nin wind tunnel test. furthermore, physics-based scaling of the flapping\\ndynamics accurately predicts the dependence of the network performance on the\\nvideo frame rate and duration.\\n)': False, 'contains(phase 2: dcl system using deep learning approaches for land-based or\\n  ship-based real-time recognition and localization of marine mammals - machine\\n  learning detection algorithms\\n  overarching goals for this work aim to advance the state of the art for\\ndetection, classification and localization (dcl) in the field of bioacoustics.\\nthis goal is primarily achieved by building a generic framework for\\ndetection-classification (dc) using a fast, efficient and scalable\\narchitecture, demonstrating the capabilities of this system using on a variety\\nof low-frequency mid-frequency cetacean sounds. two primary goals are to\\ndevelop transferable technologies for detection and classification in, one: the\\narea of advanced algorithms, such as deep learning and other methods; and two:\\nadvanced systems, capable of real-time and archival processing. for each key\\narea, we will focus on producing publications from this work and providing\\ntools and software to the community where/when possible. currently massive\\namounts of acoustic data are being collected by various institutions,\\ncorporations and national defense agencies. the long-term goal is to provide\\ntechnical capability to analyze the data using automatic algorithms for (dc)\\nbased on machine intelligence. the goal of the automation is to provide\\neffective and efficient mechanisms by which to process large acoustic datasets\\nfor understanding the bioacoustic behaviors of marine mammals. this capability\\nwill provide insights into the potential ecological impacts and influences of\\nanthropogenic ocean sounds. this work focuses on building technologies using a\\nmaturity model based on darpa 6.1 and 6.2 processes, for basic and applied\\nresearch, respectively.\\n)': False, 'contains(orders-of-magnitude speedup in atmospheric chemistry modeling through\\n  neural network-based emulation\\n  chemical transport models (ctms), which simulate air pollution transport,\\ntransformation, and removal, are computationally expensive, largely because of\\nthe computational intensity of the chemical mechanisms: systems of coupled\\ndifferential equations representing atmospheric chemistry. here we investigate\\nthe potential for machine learning to reproduce the behavior of a chemical\\nmechanism, yet with reduced computational expense. we create a 17-layer\\nresidual multi-target regression neural network to emulate the carbon bond\\nmechanism z (cbm-z) gas-phase chemical mechanism. we train the network to match\\ncbm-z predictions of changes in concentrations of 77 chemical species after one\\nhour, given a range of chemical and meteorological input conditions, which it\\nis able to do with root-mean-square error (rmse) of less than 1.97 ppb (median\\nrmse = 0.02 ppb), while achieving a 250x computational speedup. an additional\\n17x speedup (total 4250x speedup) is achieved by running the neural network on\\na graphics-processing unit (gpu). the neural network is able to reproduce the\\nemergent behavior of the chemical system over diurnal cycles using euler\\nintegration, but additional work is needed to constrain the propagation of\\nerrors as simulation time progresses.\\n)': False, 'contains(seismic-net: a deep densely connected neural network to detect seismic\\n  events\\n  one of the risks of large-scale geologic carbon sequestration is the\\npotential migration of fluids out of the storage formations. accurate and fast\\ndetection of this fluids migration is not only important but also challenging,\\ndue to the large subsurface uncertainty and complex governing physics.\\ntraditional leakage detection and monitoring techniques rely on geophysical\\nobservations including seismic. however, the resulting accuracy of these\\nmethods is limited because of indirect information they provide requiring\\nexpert interpretation, therefore yielding in-accurate estimates of leakage\\nrates and locations. in this work, we develop a novel machine-learning\\ndetection package, named \"seismic-net\", which is based on the deep densely\\nconnected neural network. to validate the performance of our proposed leakage\\ndetection method, we employ our method to a natural analog site at chimay\\\\\\'o,\\nnew mexico. the seismic events in the data sets are generated because of the\\neruptions of geysers, which is due to the leakage of $\\\\mathrm{co}_\\\\mathrm{2}$.\\nin particular, we demonstrate the efficacy of our seismic-net by formulating\\nour detection problem as an event detection problem with time series data. a\\nfixed-length window is slid throughout the time series data and we build a deep\\ndensely connected network to classify each window to determine if a geyser\\nevent is included. through our numerical tests, we show that our model achieves\\nprecision/recall as high as 0.889/0.923. therefore, our seismic-net has a great\\npotential for detection of $\\\\mathrm{co}_\\\\mathrm{2}$ leakage.\\n)': False, 'contains(geographically and temporally weighted neural networks for\\n  satellite-based mapping of ground-level pm2.5\\n  the integration of satellite-derived aerosol optical depth (aod) and\\nstation-measured pm2.5 provides a promising approach for obtaining spatial\\npm2.5 data. several spatiotemporal models, which considered spatial and\\ntemporal heterogeneities of aod-pm2.5 relationship, have been widely adopted\\nfor pm2.5 estimation. however, they generally described the complex aod-pm2.5\\nrelationship based on a linear hypothesis. previous machine learning models\\nyielded great superiorities for fitting the nonlinear aod-pm2.5 relationship,\\nbut seldom allowed for its spatiotemporal variations. to simultaneously\\nconsider the nonlinearity and spatiotemporal heterogeneities of aod-pm2.5\\nrelationship, geographically and temporally weighted neural networks (gtwnns)\\nwere developed for satellite-based estimation of ground-level pm2.5 in this\\nstudy. using satellite aod products, ndvi data, and meteorological factors over\\nchina as input, gtwnns were set up with station pm2.5 measurements. then the\\nspatial pm2.5 data of those locations with no ground stations could be\\nobtained. the proposed gtwnns have achieved a better performance compared with\\nprevious spatiotemporal models, i.e., daily geographically weighted regression\\nand geographically and temporally weighted regression. the sample-based and\\nsite-based cross-validation r2 values of gtwnns are 0.80 and 0.79,\\nrespectively. on this basis, the spatial pm2.5 data with a resolution of 0.1\\ndegree were generated in china. this study implemented the combination of\\ngeographical law and neural networks, and improved the accuracy of\\nsatellite-based pm2.5 estimation.\\n)': False, 'contains(source localization on graphs via l1 recovery and spectral graph theory\\n  we cast the problem of source localization on graphs as the simultaneous\\nproblem of sparse recovery and diffusion kernel learning. an l1 regularization\\nterm enforces the sparsity constraint while we recover the sources of diffusion\\nfrom a single snapshot of the diffusion process. the diffusion kernel is\\nestimated by assuming the process to be as generic as the standard heat\\ndiffusion. we show with synthetic data that we can concomitantly learn the\\ndiffusion kernel and the sources, given an estimated initialization. we\\nvalidate our model with cholera mortality and atmospheric tracer diffusion\\ndata, showing also that the accuracy of the solution depends on the\\nconstruction of the graph from the data points.\\n)': False, 'contains(comparison of deep neural networks and deep hierarchical models for\\n  spatio-temporal data\\n  spatio-temporal data are ubiquitous in the agricultural, ecological, and\\nenvironmental sciences, and their study is important for understanding and\\npredicting a wide variety of processes. one of the difficulties with modeling\\nspatial processes that change in time is the complexity of the dependence\\nstructures that must describe how such a process varies, and the presence of\\nhigh-dimensional complex data sets and large prediction domains. it is\\nparticularly challenging to specify parameterizations for nonlinear dynamic\\nspatio-temporal models (dstms) that are simultaneously useful scientifically\\nand efficient computationally. statisticians have developed deep hierarchical\\nmodels that can accommodate process complexity as well as the uncertainties in\\nthe predictions and inference. however, these models can be expensive and are\\ntypically application specific. on the other hand, the machine learning\\ncommunity has developed alternative \"deep learning\" approaches for nonlinear\\nspatio-temporal modeling. these models are flexible yet are typically not\\nimplemented in a probabilistic framework. the two paradigms have many things in\\ncommon and suggest hybrid approaches that can benefit from elements of each\\nframework. this overview paper presents a brief introduction to the deep\\nhierarchical dstm (dh-dstm) framework, and deep models in machine learning,\\nculminating with the deep neural dstm (dn-dstm). recent approaches that combine\\nelements from dh-dstms and echo state network dn-dstms are presented as\\nillustrations.\\n)': False, \"contains(quantification in-the-wild: data-sets and baselines\\n  quantification is the task of estimating the class-distribution of a\\ndata-set. while typically considered as a parameter estimation problem with\\nstrict assumptions on the data-set shift, we consider quantification\\nin-the-wild, on two large scale data-sets from marine ecology: a survey of\\ncaribbean coral reefs, and a plankton time series from martha's vineyard\\ncoastal observatory. we investigate several quantification methods from the\\nliterature and indicate opportunities for future work. in particular, we show\\nthat a deep neural network can be fine-tuned on a very limited amount of data\\n(25 - 100 samples) to outperform alternative methods.\\n)\": False, 'contains(a citizen-science approach to muon events in imaging atmospheric\\n  cherenkov telescope data: the muon hunter\\n  event classification is a common task in gamma-ray astrophysics. it can be\\ntreated with rapidly-advancing machine learning algorithms, which have the\\npotential to outperform traditional analysis methods. however, a major\\nchallenge for machine learning models is extracting reliably labelled training\\nexamples from real data. citizen science offers a promising approach to tackle\\nthis challenge.\\n  we present \"muon hunter\", a citizen science project hosted on the zooniverse\\nplatform, where veritas data are classified multiple times by individual users\\nin order to select and parameterize muon events, a product from cosmic ray\\ninduced showers. we use this dataset to train and validate a convolutional\\nneural-network model to identify muon events for use in monitoring and\\ncalibration. the results of this work and our experience of using the\\nzooniverse are presented.\\n)': False, \"contains(identifying solar flare precursors using time series of sdo/hmi images and sharp parameters\\n\\nwe adopt deep learning algorithms that take time series of active regions as input, instead of using only stationary features at xed time points, to perform solar are classi cations (strong ares versus weak ares). two sets of models are trained: classi cation of are versus non- are events, and classi cation of strong are events from weak are events. our results represent a signi cant improvement over previous work on similar tasks. we use machine learning algorithms to extract features directly from magnetogram images. the extracted features can be used for prediction and classi cation purposes and have been shown to perform almost as well as when we use standard active region parameters that are calculated based on experts' knowledge about the physics behind solar are events, given by sharp parameters. we build a thorough and exible data pre-processing pipeline to clean and prepare data (from goes, sdo/jsoc) for the machine learning tasks. we demonstrate the e ectiveness of the proposed algorithms in identifying precursors for strong solar are events using out-of-sample prediction tasks: four representative active regions (tracked for more than 100 hours, with at least one strong are event) are chosen and tested to predict strong are events on tted classication models which are trained with other active regions. our results show that we can construct precursors of solar are events e ciently using time series of sharp parameters. this shows promising directions towards accurate online solar are predictions, which we will address in followup work. furthermore, the physical meaning and interpretations of the modeling results are partially covered in this paper, more thorough investigations will follow.\\n)\": False, \"contains(simultaneous shot inversion for nonuniform geometries using fast data\\n  interpolation\\n  stochastic optimization is key to efficient inversion in pde-constrained\\noptimization. using 'simultaneous shots', or random superposition of source\\nterms, works very well in simple acquisition geometries where all sources see\\nall receivers, but this rarely occurs in practice. we develop an approach that\\ninterpolates data to an ideal acquisition geometry while solving the inverse\\nproblem using simultaneous shots. the approach is formulated as a joint inverse\\nproblem, combining ideas from low-rank interpolation with full-waveform\\ninversion. results using synthetic experiments illustrate the flexibility and\\nefficiency of the approach.\\n)\": False, 'contains(a mixture of experts model for predicting persistent weather patterns\\n  weather and atmospheric patterns are often persistent. the simplest weather\\nforecasting method is the so-called persistence model, which assumes that the\\nfuture state of a system will be similar (or equal) to the present state.\\nmachine learning (ml) models are widely used in different weather forecasting\\napplications, but they need to be compared to the persistence model to analyse\\nwhether they provide a competitive solution to the problem at hand. in this\\npaper, we devise a new model for predicting low-visibility in airports using\\nthe concepts of mixture of experts. visibility level is coded as two different\\nordered categorical variables: cloud height and runway visual height. the\\nunderlying system in this application is stagnant approximately in 90% of the\\ncases, and standard ml models fail to improve on the performance of the\\npersistence model. because of this, instead of trying to simply beat the\\npersistence model using ml, we use this persistence as a baseline and learn an\\nordinal neural network model that refines its results by focusing on learning\\nweather fluctuations. the results show that the proposal outperforms\\npersistence and other ordinal autoregressive models, especially for longer time\\nhorizon predictions and for the runway visual height variable.\\n)': False, 'contains(tiny-inception-resnet-v2: using deep learning for eliminating bonded\\n  labors of brick kilns in south asia\\n  this paper proposes to employ a inception-resnet inspired deep learning\\narchitecture called tiny-inception-resnet-v2 to eliminate bonded labor by\\nidentifying brick kilns within \"brick-kiln-belt\" of south asia. the framework\\nis developed by training a network on the satellite imagery consisting of 11\\ndifferent classes of south asian region. the dataset developed during the\\nprocess includes the geo-referenced images of brick kilns, houses, roads,\\ntennis courts, farms, sparse trees, dense trees, orchards, parking lots, parks\\nand barren lands. the dataset is made publicly available for further research.\\nour proposed network architecture with very fewer learning parameters\\noutperforms all state-of-the-art architectures employed for recognition of\\nbrick kilns. our proposed solution would enable regional monitoring and\\nevaluation mechanisms for the sustainable development goals.\\n)': False, 'contains(detecting spacecraft anomalies using lstms and nonparametric dynamic\\n  thresholding\\n  as spacecraft send back increasing amounts of telemetry data, improved\\nanomaly detection systems are needed to lessen the monitoring burden placed on\\noperations engineers and reduce operational risk. current spacecraft monitoring\\nsystems only target a subset of anomaly types and often require costly expert\\nknowledge to develop and maintain due to challenges involving scale and\\ncomplexity. we demonstrate the effectiveness of long short-term memory (lstms)\\nnetworks, a type of recurrent neural network (rnn), in overcoming these issues\\nusing expert-labeled telemetry anomaly data from the soil moisture active\\npassive (smap) satellite and the mars science laboratory (msl) rover,\\ncuriosity. we also propose a complementary unsupervised and nonparametric\\nanomaly thresholding approach developed during a pilot implementation of an\\nanomaly detection system for smap, and offer false positive mitigation\\nstrategies along with other key improvements and lessons learned during\\ndevelopment.\\n)': False, \"contains(flarecast: an i4.0 technology for space weather using satellite data\\n  'flare likelihood and region eruption forecasting (flarecast)' is a horizon\\n2020 project, which realized a technological platform for machine learning\\nalgorithms, with the objective of providing the space weather community with a\\nprediction service for solar flares. this paper describes the flarecast service\\nand shows how the methods implemented in the platform allow both flare\\nprediction and a quantitative assessment of how the information contained in\\nthe space data utilized in the analysis may impact the forecasting process.\\n)\": False, 'contains(bayesian prediction of future street scenes using synthetic likelihoods\\n  for autonomous agents to successfully operate in the real world, the ability\\nto anticipate future scene states is a key competence. in real-world scenarios,\\nfuture states become increasingly uncertain and multi-modal, particularly on\\nlong time horizons. dropout based bayesian inference provides a computationally\\ntractable, theoretically well grounded approach to learn likely\\nhypotheses/models to deal with uncertain futures and make predictions that\\ncorrespond well to observations -- are well calibrated. however, it turns out\\nthat such approaches fall short to capture complex real-world scenes, even\\nfalling behind in accuracy when compared to the plain deterministic approaches.\\nthis is because the used log-likelihood estimate discourages diversity. in this\\nwork, we propose a novel bayesian formulation for anticipating future scene\\nstates which leverages synthetic likelihoods that encourage the learning of\\ndiverse models to accurately capture the multi-modal nature of future scene\\nstates. we show that our approach achieves accurate state-of-the-art\\npredictions and calibrated probabilities through extensive experiments for\\nscene anticipation on cityscapes dataset. moreover, we show that our approach\\ngeneralizes across diverse tasks such as digit generation and precipitation\\nforecasting.\\n)': False, 'contains(hybrid integration of multilayer perceptrons and parametric models for\\n  reliability forecasting in the smart grid\\n  the reliable power system operation is a major goal for electric utilities,\\nwhich requires the accurate reliability forecasting to minimize the duration of\\npower interruptions. since weather conditions are usually the leading causes\\nfor power interruptions in the smart grid, especially for its distribution\\nnetworks, this paper comprehensively investigates the combined effect of\\nvarious weather parameters on the reliability performance of distribution\\nnetworks. specially, a multilayer perceptron (mlp) based framework is proposed\\nto forecast the daily numbers of sustained and momentary power interruptions in\\none distribution management area using time series of common weather data.\\nfirst, the parametric regression models are implemented to analyze the\\nrelationship between the daily numbers of power interruptions and various\\ncommon weather parameters, such as temperature, precipitation, air pressure,\\nwind speed, and lightning. the selected weather parameters and corresponding\\nparametric models are then integrated as inputs to formulate a mlp neural\\nnetwork model to predict the daily numbers of power interruptions. a modified\\nextreme learning machine (elm) based hierarchical learning algorithm is\\nintroduced for training the formulated model using realtime reliability data\\nfrom an electric utility in florida and common weather data from national\\nclimatic data center (ncdc). in addition, the sensitivity analysis is\\nimplemented to determine the various impacts of different weather parameters on\\nthe daily numbers of power interruptions.\\n)': False, 'contains(searching for hot subdwarf stars in lamost dr1-ii. pure spectroscopic\\n  identification method for hot subdwarfs\\n  employing a new machine learning method, named hierarchical extreme learning\\nmachine (helm) algorithm, we identified 56 hot subdwarf stars in the first data\\nrelease (dr1) of the large sky area multi-object fibre spectroscopic telescope\\n(lamost) survey. the atmospheric parameters of the stars are obtained by\\nfitting the profiles of hydrogen (h) balmer lines and helium (he) lines with\\nsynthetic spectra calculated from non-local thermodynamic equilibrium (nlte)\\nmodel atmospheres. five he-rich hot subdwarf stars were found in our sample\\nwith their log(nhe/nh) > -1 , while 51 stars are he-poor sdb, sdo and sdob\\nstars. we also confirmed the two he sequences of hot subdwarf stars found by\\nedelmann et al. (2003) in teff - log(nhe/nh) diagram. the helm algorithm works\\ndirectly on the observed spectroscopy and is able to filter out spectral\\nproperties without supplementary photometric data. the results presented in\\nthis study demonstrate that the helm algorithm is a reliable method to search\\nfor hot subdwarf stars after a suitable training is performed, and it is also\\nsuitable to search for other objects which have obvious features in their\\nspectra or images.\\n)': False, 'contains(spatial projection of multiple climate variables using hierarchical\\n  multitask learning\\n  future projection of climate is typically obtained by combining outputs from\\nmultiple earth system models (esms) for several climate variables such as\\ntemperature and precipitation. while ipcc has traditionally used a simple model\\noutput average, recent work has illustrated potential advantages of using a\\nmultitask learning (mtl) framework for projections of individual climate\\nvariables. in this paper we introduce a framework for hierarchical multitask\\nlearning (hmtl) with two levels of tasks such that each super-task, i.e., task\\nat the top level, is itself a multitask learning problem over sub-tasks. for\\nclimate projections, each super-task focuses on projections of specific climate\\nvariables spatially using an mtl formulation. for the proposed hmtl approach, a\\ngroup lasso regularization is added to couple parameters across the\\nsuper-tasks, which in the climate context helps exploit relationships among the\\nbehavior of different climate variables at a given spatial location. we show\\nthat some recent works on mtl based on learning task dependency structures can\\nbe viewed as special cases of hmtl. experiments on synthetic and real climate\\ndata show that hmtl produces better results than decoupled mtl methods applied\\nseparately on the super-tasks and hmtl significantly outperforms baselines for\\nclimate projection.\\n)': False, \"contains(quantifying uncertainty in discrete-continuous and skewed data with\\n  bayesian deep learning\\n  deep learning (dl) methods have been transforming computer vision with\\ninnovative adaptations to other domains including climate change. for dl to\\npervade science and engineering (s&e) applications where risk management is a\\ncore component, well-characterized uncertainty estimates must accompany\\npredictions. however, s&e observations and model-simulations often follow\\nheavily skewed distributions and are not well modeled with dl approaches, since\\nthey usually optimize a gaussian, or euclidean, likelihood loss. recent\\ndevelopments in bayesian deep learning (bdl), which attempts to capture\\nuncertainties from noisy observations, aleatoric, and from unknown model\\nparameters, epistemic, provide us a foundation. here we present a\\ndiscrete-continuous bdl model with gaussian and lognormal likelihoods for\\nuncertainty quantification (uq). we demonstrate the approach by developing uq\\nestimates on `deepsd', a super-resolution based dl model for statistical\\ndownscaling (sd) in climate applied to precipitation, which follows an\\nextremely skewed distribution. we find that the discrete-continuous models\\noutperform a basic gaussian distribution in terms of predictive accuracy and\\nuncertainty calibration. furthermore, we find that the lognormal distribution,\\nwhich can handle skewed distributions, produces quality uncertainty estimates\\nat the extremes. such results may be important across s&e, as well as other\\ndomains such as finance and economics, where extremes are often of significant\\ninterest. furthermore, to our knowledge, this is the first uq model in sd where\\nboth aleatoric and epistemic uncertainties are characterized.\\n)\": False, 'contains(deep belief networks based feature generation and regression for\\n  predicting wind power\\n  wind energy forecasting helps to manage power production, and hence, reduces\\nenergy cost. deep neural networks (dnn) mimics hierarchical learning in the\\nhuman brain and thus possesses hierarchical, distributed, and multi-task\\nlearning capabilities. based on aforementioned characteristics, we report deep\\nbelief network (dbn) based forecast engine for wind power prediction because of\\nits good generalization and unsupervised pre-training attributes. the proposed\\ndbn-wp forecast engine, which exhibits stochastic feature generation\\ncapabilities and is composed of multiple restricted boltzmann machines,\\ngenerates suitable features for wind power prediction using atmospheric\\nproperties as input. dbn-wp, due to its unsupervised pre-training of rbm layers\\nand generalization capabilities, is able to learn the fluctuations in the\\nmeteorological properties and thus is able to perform effective mapping of the\\nwind power. in the deep network, a regression layer is appended at the end to\\npredict sort-term wind power. it is experimentally shown that the deep learning\\nand unsupervised pre-training capabilities of dbn based model has comparable\\nand in some cases better results than hybrid and complex learning techniques\\nproposed for wind power prediction. the proposed prediction system based on\\ndbn, achieves mean values of rmse, mae and sde as 0.124, 0.083 and 0.122,\\nrespectively. statistical analysis of several independent executions of the\\nproposed dbn-wp wind power prediction system demonstrates the stability of the\\nsystem. the proposed dbn-wp architecture is easy to implement and offers\\ngeneralization as regards the change in location of the wind farm is concerned.\\n)': False, 'contains(precision annealing monte carlo methods for statistical data\\n  assimilation and machine learning\\n  in statistical data assimilation (sda) and supervised machine learning (ml),\\nwe wish to transfer information from observations to a model of the processes\\nunderlying those observations. for sda, the model consists of a set of\\ndifferential equations that describe the dynamics of a physical system. for ml,\\nthe model is usually constructed using other strategies. in this paper, we\\ndevelop a systematic formulation based on monte carlo sampling to achieve such\\ninformation transfer. following the derivation of an appropriate target\\ndistribution, we present the formulation based on the standard\\nmetropolis-hasting (mh) procedure and the hamiltonian monte carlo (hmc) method\\nfor performing the high dimensional integrals that appear. to the extensive\\nliterature on mh and hmc, we add (1) an annealing method using a hyperparameter\\nthat governs the precision of the model to identify and explore the highest\\nprobability regions of phase space dominating those integrals, and (2) a\\nstrategy for initializing the state space search. the efficacy of the proposed\\nformulation is demonstrated using a nonlinear dynamical model with chaotic\\nsolutions widely used in geophysics.\\n)': False, 'contains(water bathymetry mapping from uav imagery based on machine learning\\n\\nthe determination of accurate bathymetric information is a key element for near offshore activities, hydrological studies such as coastal engineering applications, sedimentary processes, hydrographic surveying as well as archaeological mapping and biological research. uav imagery processed with structure from motion (sfm) and multi view stereo (mvs) techniques can provide a low-cost alternative to established shallow seabed mapping techniques offering as well the important visual information. nevertheless, water refraction poses significant challenges on depth determination. till now, this problem has been addressed through customized image-based refraction correction algorithms or by modifying the collinearity equation. in this paper, in order to overcome the water refraction errors, we employ machine learning tools that are able to learn the systematic underestimation of the estimated depths. in the proposed approach, based on known depth observations from bathymetric lidar surveys, an svr model was developed able to estimate more accurately the real depths of point clouds derived from sfm-mvs procedures. experimental results over two test sites along with the performed quantitative validation indicated the high potential of the developed approach.\\n)': False, 'contains(ecological data analysis based on machine learning algorithms\\n  classification is an important supervised machine learning method, which is\\nnecessary and challenging issue for ecological research. it offers a way to\\nclassify a dataset into subsets that share common patterns. notably, there are\\nmany classification algorithms to choose from, each making certain assumptions\\nabout the data and about how classification should be formed. in this paper, we\\napplied eight machine learning classification algorithms such as decision\\ntrees, random forest, artificial neural network, support vector machine, linear\\ndiscriminant analysis, k-nearest neighbors, logistic regression and naive bayes\\non ecological data. the goal of this study is to compare different machine\\nlearning classification algorithms in ecological dataset. in this analysis we\\nhave checked the accuracy test among the algorithms. in our study we conclude\\nthat linear discriminant analysis and k-nearest neighbors are the best methods\\namong all other methods\\n)': False, \"contains(temporal neural networks for downscaling climate variability * and extremes\\n\\nthis paper presents an application of temporal neural networks for downscaling global climate models (gcms) output. because of computational constraints, gcms are usually run at coarse grid resolution (in the order of 100s of kilometres) and as a result they are inherently unable to present local sub-grid scale features and dynamics. consequently, outputs from these models cannot be used directly in many climate change impact studies. this research explored the issues of 'downscaling' the outputs of gcms using a temporal neural network (tnn) approach. the method is proposed for downscaling daily precipitation and temperature series for a region in northern quebec, canada. the downscaling models are developed and validated using large-scale predictor variables derived from the national center for environmental prediction (ncep) reanalysis data set. the performance of the temporal neural network downscaling model is also compared to a regression-based statistical downscaling model with emphasis on their ability in reproducing the observed climate variability and extremes. the downscaling results for the base period (1961-2000) suggest that the tnn is an efficient method for downscaling both daily precipitation as well as daily maximum and minimum temperature series. furthermore, the different model test results indicate that the tnn model mostly outperforms the statistical models for the downscaling of daily precipitation extremes and variability. q 2006 elsevier ltd. all rights reserved. * this work was made possible through a grant from the canadian climate change action fund, environment canada, and a grant from the natural sciences and engineering research council of canada. the authors would like to thank the aluminum company of canada (alcan) for providing the experiment data. * corresponding author. tel.: c1 905 525 9140x23354; fax: c1 905 546 0463. e-mail address: couliba@mcmaster.ca (p. coulibaly).\\n)\": False, 'contains(a novel universal solar energy predictor\\n  solar energy is one of the most economical and clean sustainable energy\\nsources on the planet. however, the solar energy throughput is highly\\nunpredictable due to its dependency on a plethora of conditions including\\nweather, seasons, and other ecological/environmental conditions. thus, the\\nsolar energy prediction is an inevitable necessity to optimize solar energy and\\nalso to improve the efficiency of solar energy systems. conventionally, the\\noptimization of the solar energy is undertaken by subject matter experts using\\ntheir domain knowledge; although it is impractical for even the experts to tune\\nthe solar systems on a continuous basis. we strongly believe that the power of\\nmachine learning can be harnessed to better optimize the solar energy\\nproduction by learning the correlation between various conditions and solar\\nenergy production from historical data which is typically readily available.\\nfor this use, this paper predicts the daily total energy generation of an\\ninstalled solar program using the naive bayes classifier. in the forecast\\nprocedure, one year historical dataset including daily moderate temperatures,\\ndaily total sunshine duration, daily total global solar radiation and daily\\ntotal photovoltaic energy generation parameters are used as the categorical\\nvalued features. by way of this naive bayes program the sensitivity and the\\nprecision measures are improved for the photovoltaic energy prediction and also\\nthe consequences of other solar characteristics on the solar energy production\\nhave been assessed.\\n)': False, 'contains(cobweb - a toolbox for automatic tomographic image analysis based on\\n  machine learning techniques: application and examples\\n  in this study, we introduce cobweb 1.0 which is a graphical user interface\\ntailored explicitly for accurate image segmentation and representative\\nelementary volume analysis of digital rock images derived from high resolution\\ntomography. the cobweb code is a work package deployed as a series of windows\\nexecutable binaries which use image processing and machine learning libraries\\nof matlab. the user-friendly interface enables image segmentation and\\ncross-validation employing k-means, fuzzy c-means, least square support vector\\nmachine, and ensemble classification (bragging and boosting) segmentation\\ntechniques. a quick region of interest analysis including relative porosity\\ntrends, pore size distribution, and volume fraction of different phases can be\\nperformed on different geomaterials. data can be exported to paraview, dsi\\nstudio (.fib), microsoft excel and matlab for further visualisation and\\nstatistical analysis. the efficiency of the new tool was verified using gas\\nhydrate-bearing sediment samples and berea sandstone, both from synchrotron\\ntomography datasets, as well as grosmont carbonate rock x-ray micro-tomographic\\ndataset. despite its high sub-micrometer resolution, the gas hydrate dataset\\nwas suffering from edge enhancement artefacts. these artefacts were primarily\\nnormalized by the dual filtering approach using both non-local means and\\nanisotropic diffusion filtering. the desired automatic segmentation of the\\nphases (brine, sand, and gas hydrate) was thus successfully achieved using the\\ndual clustering approach.\\n)': False, 'contains(automatic large-scale classification of bird sounds is strongly improved\\n  by unsupervised feature learning\\n  automatic species classification of birds from their sound is a computational\\ntool of increasing importance in ecology, conservation monitoring and vocal\\ncommunication studies. to make classification useful in practice, it is crucial\\nto improve its accuracy while ensuring that it can run at big data scales. many\\napproaches use acoustic measures based on spectrogram-type data, such as the\\nmel-frequency cepstral coefficient (mfcc) features which represent a\\nmanually-designed summary of spectral information. however, recent work in\\nmachine learning has demonstrated that features learnt automatically from data\\ncan often outperform manually-designed feature transforms. feature learning can\\nbe performed at large scale and \"unsupervised\", meaning it requires no manual\\ndata labelling, yet it can improve performance on \"supervised\" tasks such as\\nclassification. in this work we introduce a technique for feature learning from\\nlarge volumes of bird sound recordings, inspired by techniques that have proven\\nuseful in other domains. we experimentally compare twelve different feature\\nrepresentations derived from the mel spectrum (of which six use this\\ntechnique), using four large and diverse databases of bird vocalisations, with\\na random forest classifier. we demonstrate that mfccs are of limited power in\\nthis context, leading to worse performance than the raw mel spectral data.\\nconversely, we demonstrate that unsupervised feature learning provides a\\nsubstantial boost over mfccs and mel spectra without adding computational\\ncomplexity after the model has been trained. the boost is particularly notable\\nfor single-label classification tasks at large scale. the spectro-temporal\\nactivations learned through our procedure resemble spectro-temporal receptive\\nfields calculated from avian primary auditory forebrain.\\n)': False, \"contains(text classification of the precursory accelerating seismicity corpus:\\n  inference on some theoretical trends in earthquake predictability research\\n  from 1988 to 2018\\n  text analytics based on supervised machine learning classifiers has shown\\ngreat promise in a multitude of domains, but has yet to be applied to\\nseismology. we test various standard models (naive bayes, k-nearest neighbors,\\nsupport vector machines, and random forests) on a seismological corpus of 100\\narticles related to the topic of precursory accelerating seismicity, spanning\\nfrom 1988 to 2010. this corpus was labelled in mignan (2011) with the precursor\\nwhether explained by critical processes (i.e., cascade triggering) or by other\\nprocesses (such as signature of main fault loading). we investigate rather the\\nclassification process can be automatized to help analyze larger corpora in\\norder to better understand trends in earthquake predictability research. we\\nfind that the naive bayes model performs best, in agreement with the machine\\nlearning literature for the case of small datasets, with cross-validation\\naccuracies of 86% for binary classification. for a refined multiclass\\nclassification ('non-critical process' < 'agnostic' < 'critical process\\nassumed' < 'critical process demonstrated'), we obtain up to 78% accuracy.\\nprediction on a dozen of articles published since 2011 shows however a weak\\ngeneralization with a f1-score of 60%, only slightly better than a random\\nclassifier, which can be explained by a change of authorship and use of\\ndifferent terminologies. yet, the model shows f1-scores greater than 80% for\\nthe two multiclass extremes ('non-critical process' versus 'critical process\\ndemonstrated') while it falls to random classifier results (around 25%) for\\npapers labelled 'agnostic' or 'critical process assumed'. those results are\\nencouraging in view of the small size of the corpus and of the high degree of\\nabstraction of the labelling. domain knowledge engineering remains essential\\nbut can be made transparent by an investigation of naive bayes keyword\\nposterior probabilities.\\n)\": False, 'contains(bayesian heatmaps: probabilistic classification with multiple unreliable\\n  information sources\\n  unstructured data from diverse sources, such as social media and aerial\\nimagery, can provide valuable up-to-date information for intelligent situation\\nassessment. mining these different information sources could bring major\\nbenefits to applications such as situation awareness in disaster zones and\\nmapping the spread of diseases. such applications depend on classifying the\\nsituation across a region of interest, which can be depicted as a spatial\\n\"heatmap\". annotating unstructured data using crowdsourcing or automated\\nclassifiers produces individual classifications at sparse locations that\\ntypically contain many errors. we propose a novel bayesian approach that models\\nthe relevance, error rates and bias of each information source, enabling us to\\nlearn a spatial gaussian process classifier by aggregating data from multiple\\nsources with varying reliability and relevance. our method does not require\\ngold-labelled data and can make predictions at any location in an area of\\ninterest given only sparse observations. we show empirically that our approach\\ncan handle noisy and biased data sources, and that simultaneously inferring\\nreliability and transferring information between neighbouring reports leads to\\nmore accurate predictions. we demonstrate our method on two real-world problems\\nfrom disaster response, showing how our approach reduces the amount of\\ncrowdsourced data required and can be used to generate valuable heatmap\\nvisualisations from sms messages and satellite images.\\n)': False, \"contains(graph-valued regression\\n  undirected graphical models encode in a graph $g$ the dependency structure of\\na random vector $y$. in many applications, it is of interest to model $y$ given\\nanother random vector $x$ as input. we refer to the problem of estimating the\\ngraph $g(x)$ of $y$ conditioned on $x=x$ as ``graph-valued regression.'' in\\nthis paper, we propose a semiparametric method for estimating $g(x)$ that\\nbuilds a tree on the $x$ space just as in cart (classification and regression\\ntrees), but at each leaf of the tree estimates a graph. we call the method\\n``graph-optimized cart,'' or go-cart. we study the theoretical properties of\\ngo-cart using dyadic partitioning trees, establishing oracle inequalities on\\nrisk minimization and tree partition consistency. we also demonstrate the\\napplication of go-cart to a meteorological dataset, showing how graph-valued\\nregression can provide a useful tool for analyzing complex data.\\n)\": False, 'contains(predicting global variations in outdoor pm2.5 concentrations using\\n  satellite images and deep convolutional neural networks\\n  here we present a new method of estimating global variations in outdoor\\npm$_{2.5}$ concentrations using satellite images combined with ground-level\\nmeasurements and deep convolutional neural networks. specifically, new deep\\nlearning models were trained over the global pm$_{2.5}$ concentration range\\n($<$1-436 $\\\\mu$g/m$^3$) using a large database of satellite images paired with\\nground level pm$_{2.5}$ measurements available from the world health\\norganization. final model selection was based on a systematic evaluation of\\nwell-known architectures for the convolutional base including inceptionv3,\\nxception, and vgg16. the xception architecture performed best and the final\\nglobal model had a root mean square error (rmse) value of 13.01 $\\\\mu$g/m$^3$\\n(r$^2$=0.75) in the disjoint test set. the predictive performance of our new\\nglobal model (called image-pm$_{2.5}$) is similar to the current\\nstate-of-the-art model used in the global burden of disease study but relies\\nonly on satellite images as input. as a result, the image-pm$_{2.5}$ model\\noffers a fast, cost-effective means of estimating global variations in\\nlong-term average pm$_{2.5}$ concentrations and may be particularly useful for\\nregions without ground monitoring data or detailed emissions inventories. the\\nimage-pm$_{2.5}$ model can be used as a stand-alone method of global exposure\\nestimation or incorporated into more complex hierarchical model structures.\\n)': False, 'contains(supervised machine learning for analysing spectra of exoplanetary\\n  atmospheres\\n  the use of machine learning is becoming ubiquitous in astronomy, but remains\\nrare in the study of the atmospheres of exoplanets. given the spectrum of an\\nexoplanetary atmosphere, a multi-parameter space is swept through in real time\\nto find the best-fit model. known as atmospheric retrieval, it is a technique\\nthat originates from the earth and planetary sciences. such methods are very\\ntime-consuming and by necessity there is a compromise between physical and\\nchemical realism versus computational feasibility. machine learning has\\npreviously been used to determine which molecules to include in the model, but\\nthe retrieval itself was still performed using standard methods. here, we\\nreport an adaptation of the random forest method of supervised machine\\nlearning, trained on a pre-computed grid of atmospheric models, which retrieves\\nfull posterior distributions of the abundances of molecules and the cloud\\nopacity. the use of a pre-computed grid allows a large part of the\\ncomputational burden to be shifted offline. we demonstrate our technique on a\\ntransmission spectrum of the hot gas-giant exoplanet wasp-12b using a\\nfive-parameter model (temperature, a constant cloud opacity and the volume\\nmixing ratios or relative abundance by number of water, ammonia and hydrogen\\ncyanide). we obtain results consistent with the standard nested-sampling\\nretrieval method. additionally, we can estimate the sensitivity of the measured\\nspectrum to constraining the model parameters and we can quantify the\\ninformation content of the spectrum. our method can be straightforwardly\\napplied using more sophisticated atmospheric models and also to interpreting an\\nensemble of spectra without having to retrain the random forest.\\n)': False, 'contains(orbit: ordering based information transfer across space and time for\\n  global surface water monitoring\\n  many earth science applications require data at both high spatial and\\ntemporal resolution for effective monitoring of various ecosystem resources.\\ndue to practical limitations in sensor design, there is often a trade-off in\\ndifferent resolutions of spatio-temporal datasets and hence a single sensor\\nalone cannot provide the required information. various data fusion methods have\\nbeen proposed in the literature that mainly rely on individual timesteps when\\nboth datasets are available to learn a mapping between features values at\\ndifferent resolutions using local relationships between pixels. earth\\nobservation data is often plagued with spatially and temporally correlated\\nnoise, outliers and missing data due to atmospheric disturbances which pose a\\nchallenge in learning the mapping from a local neighborhood at individual\\ntimesteps. in this paper, we aim to exploit time-independent global\\nrelationships between pixels for robust transfer of information across\\ndifferent scales. specifically, we propose a new framework, orbit (ordering\\nbased information transfer) that uses relative ordering constraint among pixels\\nto transfer information across both time and scales. the effectiveness of the\\nframework is demonstrated for global surface water monitoring using both\\nsynthetic and real-world datasets.\\n)': False, 'contains(time-space tradeoff in deep learning models for crop classification on\\n  satellite multi-spectral image time series\\n  in this article, we investigate several structured deep learning models for\\ncrop type classification on multi-spectral time series. in particular, our aim\\nis to assess the respective importance of spatial and temporal structures in\\nsuch data. with this objective, we consider several designs of convolutional,\\nrecurrent, and hybrid neural networks, and assess their performance on a large\\ndataset of freely available sentinel-2 imagery. we find that the\\nbest-performing approaches are hybrid configurations for which most of the\\nparameters (up to 90%) are allocated to modeling the temporal structure of the\\ndata. our results thus constitute a set of guidelines for the design of bespoke\\ndeep learning models for crop type classification.\\n)': False, 'contains(poverty mapping using convolutional neural networks trained on high and\\n  medium resolution satellite images, with an application in mexico\\n  mapping the spatial distribution of poverty in developing countries remains\\nan important and costly challenge. these \"poverty maps\" are key inputs for\\npoverty targeting, public goods provision, political accountability, and impact\\nevaluation, that are all the more important given the geographic dispersion of\\nthe remaining bottom billion severely poor individuals. in this paper we train\\nconvolutional neural networks (cnns) to estimate poverty directly from high and\\nmedium resolution satellite images. we use both planet and digital globe\\nimagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively,\\ncovering all 2 million sq. km. of mexico. benchmark poverty estimates come from\\nthe 2014 mcs-enigh combined with the 2015 intercensus and are used to estimate\\npoverty rates for 2,456 mexican municipalities. cnns are trained using the 896\\nmunicipalities in the 2014 mcs-enigh. we experiment with several architectures\\n(googlenet, vgg) and use googlenet as a final architecture where weights are\\nfine-tuned from imagenet. we find that 1) the best models, which incorporate\\nsatellite-estimated land use as a predictor, explain approximately 57% of the\\nvariation in poverty in a validation sample of 10 percent of mcs-enigh\\nmunicipalities; 2) across all mcs-enigh municipalities explanatory power\\nreduces to 44% in a cnn prediction and landcover model; 3) predicted poverty\\nfrom the cnn predictions alone explains 47% of the variation in poverty in the\\nvalidation sample, and 37% over all mcs-enigh municipalities; 4) in urban areas\\nwe see slight improvements from using digital globe versus planet imagery,\\nwhich explain 61% and 54% of poverty variation respectively. we conclude that\\ncnns can be trained end-to-end on satellite imagery to estimate poverty,\\nalthough there is much work to be done to understand how the training process\\ninfluences out of sample validation.\\n)': False, \"contains(a dynamic network and representation learningapproach for quantifying\\n  economic growth fromsatellite imagery\\n  quantifying the improvement in human living standard, as well as the city\\ngrowth in developing countries, is a challenging problem due to the lack of\\nreliable economic data. therefore, there is a fundamental need for alternate,\\nlargely unsupervised, computational methods that can estimate the economic\\nconditions in the developing regions. to this end, we propose a new network\\nscience- and representation learning-based approach that can quantify economic\\nindicators and visualize the growth of various regions. more precisely, we\\nfirst create a dynamic network drawn out of high-resolution nightlight\\nsatellite images. we then demonstrate that using representation learning to\\nmine the resulting network, our proposed approach can accurately predict\\nspatial gross economic expenditures over large regions. our method, which\\nrequires only nightlight images and limited survey data, can capture\\ncity-growth, as well as how people's living standard is changing; this can\\nultimately facilitate the decision makers' understanding of growth without\\nheavily relying on expensive and time-consuming surveys.\\n)\": False, 'contains(a unified view of generative models for networks: models, methods,\\n  opportunities, and challenges\\n  research on probabilistic models of networks now spans a wide variety of\\nfields, including physics, sociology, biology, statistics, and machine\\nlearning. these efforts have produced a diverse ecology of models and methods.\\ndespite this diversity, many of these models share a common underlying\\nstructure: pairwise interactions (edges) are generated with probability\\nconditional on latent vertex attributes. differences between models generally\\nstem from different philosophical choices about how to learn from data or\\ndifferent empirically-motivated goals. the highly interdisciplinary nature of\\nwork on these generative models, however, has inhibited the development of a\\nunified view of their similarities and differences. for instance, novel\\ntheoretical models and optimization techniques developed in machine learning\\nare largely unknown within the social and biological sciences, which have\\ninstead emphasized model interpretability. here, we describe a unified view of\\ngenerative models for networks that draws together many of these disparate\\nthreads and highlights the fundamental similarities and differences that span\\nthese fields. we then describe a number of opportunities and challenges for\\nfuture work that are revealed by this view.\\n)': False, \"contains(stochastic seismic waveform inversion using generative adversarial\\n  networks as a geological prior\\n  we present an application of deep generative models in the context of\\npartial-differential equation (pde) constrained inverse problems. we combine a\\ngenerative adversarial network (gan) representing an a priori model that\\ncreates subsurface geological structures and their petrophysical properties,\\nwith the numerical solution of the pde governing the propagation of acoustic\\nwaves within the earth's interior. we perform bayesian inversion using an\\napproximate metropolis-adjusted langevin algorithm (mala) to sample from the\\nposterior given seismic observations. gradients with respect to the model\\nparameters governing the forward problem are obtained by solving the adjoint of\\nthe acoustic wave equation. gradients of the mismatch with respect to the\\nlatent variables are obtained by leveraging the differentiable nature of the\\ndeep neural network used to represent the generative model. we show that\\napproximate mala sampling allows efficient bayesian inversion of model\\nparameters obtained from a prior represented by a deep generative model,\\nobtaining a diverse set of realizations that reflect the observed seismic\\nresponse.\\n)\": False, 'contains(machine learning approach to earthquake rupture dynamics\\n  simulating dynamic rupture propagation is challenging due to the\\nuncertainties involved in the underlying physics of fault slip, stress\\nconditions, and frictional properties of the fault. a trial and error approach\\nis often used to determine the unknown parameters describing rupture, but\\nrunning many simulations usually requires human review to determine how to\\nadjust parameter values and is thus not very efficient. to reduce the\\ncomputational cost and improve our ability to determine reasonable stress and\\nfriction parameters, we take advantage of the machine learning approach. we\\ndevelop two models for earthquake rupture propagation using the artificial\\nneural network (ann) and the random forest (rf) algorithms to predict if a\\nrupture can break a geometric heterogeneity on a fault. we train the models\\nusing a database of 1600 dynamic rupture simulations computed numerically.\\nfault geometry, stress conditions, and friction parameters vary in each\\nsimulation. we cross-validate and test the predictive power of the models using\\nan additional 400 simulated ruptures, respectively. both rf and ann models\\npredict rupture propagation with more than 81% accuracy, and model parameters\\ncan be used to infer the underlying factors most important for rupture\\npropagation. both of the models are computationally efficient such that the 400\\ntestings require a fraction of a second, leading to potential applications of\\ndynamic rupture that have previously not been possible due to the computational\\ndemands of physics-based rupture simulations.\\n)': False, 'contains(pbbfmm3d: a parallel black-box fast multipole method for non-oscillatory\\n  kernels\\n  this paper presents pbbfmm3d: a parallel black-box fast multipole method that\\naccelerates kernel matrix-vector multiplications where the kernel is a\\nnon-oscillatory function in three dimensions. such problems arise from a wide\\nrange of fields, \\\\emph{e.g.,} computational mechanics, geosciences and machine\\nlearning. while a naive direct evaluation has an $o(n^2)$ complexity in time\\nand storage, which is prohibitive for large-scale applications, pbbfmm3d\\nreduces the costs to $o(n)$. in contrast to other fast methods that require the\\nknowledge of the explicit kernel formula, pbbfmm3d requires only the ability to\\nevaluate the kernel. to further accelerate the computation on shared-memory\\nmachines, the parallelism in pbbfmm3d was analyzed and implemented using\\nopenmp. we show numerical experiments on the accuracy and the parallel\\nscalability of pbbfmm3d, as well as its applications to covariance matrix\\ncomputations that are heavily used in parameter estimation techniques, such as\\nkriging and kalman filtering.\\n)': False, 'contains(soilgrids250m: global gridded soil information based on machine learning\\n\\nthis paper describes the technical development and accuracy assessment of the most recent and improved version of the soilgrids system at 250m resolution (june 2016 update). soilgrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, cation exchange capacity (cec), ph, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the world reference base (wrb) and usda classification systems (ca. 280 raster layers in total). predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from modis land products, srtm dem derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods√∞random forest and gradient boosting and/or multinomial logistic regression√∞as implemented in the r packages ranger, xgboost, nnet and caret. the results of 10¬±fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (ph) of variation with an overall average of 61%. improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of soilgrids at 1 km spatial resolution, range from 60 to 230%. improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. further development of soilgrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. another area of future research is the development of methods for multiscale merging of soilgrids predictions with local and/or national gridded soil products (e.g. up to\\n)': False, \"contains(interpretable time series classification using all-subsequence learning\\n  and symbolic representations in time and frequency domains\\n  the time series classification literature has expanded rapidly over the last\\ndecade, with many new classification approaches published each year. the\\nresearch focus has mostly been on improving the accuracy and efficiency of\\nclassifiers, while their interpretability has been somewhat neglected.\\nclassifier interpretability has become a critical constraint for many\\napplication domains and the introduction of the 'right to explanation' gdpr eu\\nlegislation in may 2018 is likely to further emphasize the importance of\\nexplainable learning algorithms. in this work we analyse the state-of-the-art\\nfor time series classification, and propose new algorithms that aim to maintain\\nthe classifier accuracy and efficiency, but keep interpretability as a key\\ndesign constraint. we present new time series classification algorithms that\\nadvance the state-of-the-art by implementing the following three key ideas: (1)\\nmultiple resolutions of symbolic approximations: we combine symbolic\\nrepresentations obtained using different parameters; (2) multiple domain\\nrepresentations: we combine symbolic approximations in time (e.g., sax) and\\nfrequency (e.g., sfa) domains; (3) efficient navigation of a huge\\nsymbolic-words space: we adapt a symbolic sequence classifier named seql, to\\nmake it work with multiple domain representations (e.g., sax-seql, sfa-seql),\\nand use its greedy feature selection strategy to effectively filter the best\\nfeatures for each representation. we show that a multi-resolution multi-domain\\nlinear classifier, sax-sfa-seql, achieves a similar accuracy to the\\nstate-of-the-art cote ensemble, and to a recent deep learning method (fcn), but\\nuses a fraction of the time required by either cote or fcn. we discuss the\\naccuracy, efficiency and interpretability of our proposed algorithms. to\\nfurther analyse the interpretability aspect of our classifiers, we present a\\ncase study on an ecology benchmark.\\n)\": False, 'contains(embedding geographic locations for modelling the natural environment\\n  using flickr tags and structured data\\n  meta-data from photo-sharing websites such as flickr can be used to obtain\\nrich bag-of-words descriptions of geographic locations, which have proven\\nvaluable, among others, for modelling and predicting ecological features. one\\nimportant insight from previous work is that the descriptions obtained from\\nflickr tend to be complementary to the structured information that is available\\nfrom traditional scientific resources. to better integrate these two diverse\\nsources of information, in this paper we consider a method for learning vector\\nspace embeddings of geographic locations. we show experimentally that this\\nmethod improves on existing approaches, especially in cases where structured\\ninformation is available.\\n)': False, 'contains(tsallis regularized optimal transport and ecological inference\\n  optimal transport is a powerful framework for computing distances between\\nprobability distributions. we unify the two main approaches to optimal\\ntransport, namely monge-kantorovitch and sinkhorn-cuturi, into what we define\\nas tsallis regularized optimal transport (\\\\trot). \\\\trot~interpolates a rich\\nfamily of distortions from wasserstein to kullback-leibler, encompassing as\\nwell pearson, neyman and hellinger divergences, to name a few. we show that\\nmetric properties known for sinkhorn-cuturi generalize to \\\\trot, and provide\\nefficient algorithms for finding the optimal transportation plan with formal\\nconvergence proofs. we also present the first application of optimal transport\\nto the problem of ecological inference, that is, the reconstruction of joint\\ndistributions from their marginals, a problem of large interest in the social\\nsciences. \\\\trot~provides a convenient framework for ecological inference by\\nallowing to compute the joint distribution --- that is, the optimal\\ntransportation plan itself --- when side information is available, which is\\n\\\\textit{e.g.} typically what census represents in political science.\\nexperiments on data from the 2012 us presidential elections display the\\npotential of \\\\trot~in delivering a faithful reconstruction of the joint\\ndistribution of ethnic groups and voter preferences.\\n)': False, 'contains(reliable real-time seismic signal/noise discrimination with machine\\n  learning\\n  in earthquake early warning (eew), every sufficiently impulsive signal is\\npotentially the first evidence for an unfolding large earthquake. more often\\nthan not, however, impulsive signals are mere nuisance signals. one of the most\\nfundamental - and difficult - tasks in eew is to rapidly and reliably\\ndiscriminate real local earthquake signals from all other signals. this\\ndiscrimination is necessarily based on very little information, typically a few\\nseconds worth of seismic waveforms from a small number of stations. as a\\nresult, current eew systems struggle to avoid discrimination errors, and suffer\\nfrom false and missed alerts. in this study we show how modern machine learning\\nclassifiers can strongly improve real-time signal/noise discrimination. we\\ndevelop and compare a series of non-linear classifiers with variable\\narchitecture depths, including fully connected, convolutional (cnn) and\\nrecurrent neural networks, and a model that combines a generative adversarial\\nnetwork with a random forest (gan+rf). we train all classifiers on the same\\ndata set, which includes 374k local earthquake records (m3.0-9.1) and 946k\\nimpulsive noise signals. we find that all classifiers outperform existing\\nsimple linear classifiers, and that complex models trained directly on the raw\\nsignals yield the greatest degree of improvement. using 3s long waveform\\nsnippets, the cnn and the gan+rf classifiers both reach 99.5% precision and\\n99.3% recall on an independent validation data set. most misclassifications\\nstem from impulsive teleseismic records, and from incorrectly labeled records\\nin the data set. our results suggest that machine learning classifiers can\\nstrongly improve the reliability and speed of eew alerts.\\n)': False, 'contains(automatic differentiation in machine learning: a survey\\n  derivatives, mostly in the form of gradients and hessians, are ubiquitous in\\nmachine learning. automatic differentiation (ad), also called algorithmic\\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\\nmore general than backpropagation for efficiently and accurately evaluating\\nderivatives of numeric functions expressed as computer programs. ad is a small\\nbut established field with applications in areas including computational fluid\\ndynamics, atmospheric sciences, and engineering design optimization. until very\\nrecently, the fields of machine learning and ad have largely been unaware of\\neach other and, in some cases, have independently discovered each other\\'s\\nresults. despite its relevance, general-purpose ad has been missing from the\\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\\nunder the names \"dynamic computational graphs\" and \"differentiable\\nprogramming\". we survey the intersection of ad and machine learning, cover\\napplications where ad has direct relevance, and address the main implementation\\ntechniques. by precisely defining the main differentiation techniques and their\\ninterrelationships, we aim to bring clarity to the usage of the terms\\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\\nthese are encountered more and more in machine learning settings.\\n)': False, \"contains(deep learning in remote sensing: a review\\n  standing at the paradigm shift towards data-intensive science, machine\\nlearning techniques are becoming increasingly important. in particular, as a\\nmajor breakthrough in the field, deep learning has proven as an extremely\\npowerful tool in many fields. shall we embrace deep learning as the key to all?\\nor, should we resist a 'black-box' solution? there are controversial opinions\\nin the remote sensing community. in this article, we analyze the challenges of\\nusing deep learning for remote sensing data analysis, review the recent\\nadvances, and provide resources to make deep learning in remote sensing\\nridiculously simple to start with. more importantly, we advocate remote sensing\\nscientists to bring their expertise into deep learning, and use it as an\\nimplicit general model to tackle unprecedented large-scale influential\\nchallenges, such as climate change and urbanization.\\n)\": False, \"contains(machine learning for graph-based representations of three-dimensional\\n  discrete fracture networks\\n  structural and topological information play a key role in modeling flow and\\ntransport through fractured rock in the subsurface. discrete fracture network\\n(dfn) computational suites such as dfnworks are designed to simulate flow and\\ntransport in such porous media. flow and transport calculations reveal that a\\nsmall backbone of fractures exists, where most flow and transport occurs.\\nrestricting the flowing fracture network to this backbone provides a\\nsignificant reduction in the network's effective size. however, the particle\\ntracking simulations needed to determine the reduction are computationally\\nintensive. such methods may be impractical for large systems or for robust\\nuncertainty quantification of fracture networks, where thousands of forward\\nsimulations are needed to bound system behavior.\\n  in this paper, we develop an alternative network reduction approach to\\ncharacterizing transport in dfns, by combining graph theoretical and machine\\nlearning methods. we consider a graph representation where nodes signify\\nfractures and edges denote their intersections. using random forest and support\\nvector machines, we rapidly identify a subnetwork that captures the flow\\npatterns of the full dfn, based primarily on node centrality features in the\\ngraph. our supervised learning techniques train on particle-tracking backbone\\npaths found by dfnworks, but run in negligible time compared to those\\nsimulations. we find that our predictions can reduce the network to\\napproximately 20% of its original size, while still generating breakthrough\\ncurves consistent with those of the original network.\\n)\": False, 'contains(neural machine translation between herbal prescriptions and diseases\\n  the current study applies deep learning to herbalism. toward the goal, we\\nacquired the de-identified health insurance reimbursements that were claimed in\\na 10-year period from 2004 to 2013 in the national health insurance database of\\ntaiwan, the total number of reimbursement records equaling 340 millions. two\\nartificial intelligence techniques were applied to the dataset: residual\\nconvolutional neural network multitask classifier and attention-based recurrent\\nneural network. the former works to translate from herbal prescriptions to\\ndiseases; and the latter from diseases to herbal prescriptions. analysis of the\\nclassification results indicates that herbal prescriptions are specific to:\\nanatomy, pathophysiology, sex and age of the patient, and season and year of\\nthe prescription. further analysis identifies temperature and gross domestic\\nproduct as the meteorological and socioeconomic factors that are associated\\nwith herbal prescriptions. analysis of the neural machine transitional result\\nindicates that the recurrent neural network learnt not only syntax but also\\nsemantics of diseases and herbal prescriptions.\\n)': False, 'contains(exploring urban air quality with maps: mobile air pollution sensing\\n  mobile and ubiquitous sensing of urban air quality (aq) has received\\nincreased attention as an economically and operationally viable means to survey\\natmospheric environment with high spatial-temporal resolution. a necessary and\\nvalue-added step towards data-driven sustainable urban management is\\nfine-granular aq inference, which estimates grid-level pollutant concentrations\\nat every instance of time using aq data collected from fixed-location and\\nmobile sensors. we present the mobile air pollution sensing (maps) framework,\\nwhich consists of data preprocessing, urban feature extraction, and aq\\ninference. this is applied to a case study in beijing (3,025 square km, 19 june\\n- 16 july 2018), where pm2.5 concentrations measured by 28 fixed monitoring\\nstations and 15 vehicles are fused to infer hourly pm2.5 concentrations in\\n3,025 1km-by-1km grids. two machine learning structures, namely deep feature\\nspatial-temporal tree (dfeast-tree) and deep feature spatial-temporal network\\n(dfeast-net), are proposed to infer pm2.5 concentrations supported by 62 types\\nof urban data that encompass geography, land use, traffic, public, and\\nmeteorology. this allows us to infer fine-granular pm2.5 concentrations based\\non sparse aq measurements (less than 5% coverage) with good accuracy\\n(smape<15%, r-square>0.9), while accounting for the regional transport of air\\npollutants outside the study area. in-depth discussions are provided on the\\nheterogeneity of fixed and mobile data sources, spatial coverage of mobile\\nsensing, and importance of urban features for inferring pm2.5 concentrations.\\n)': False, 'contains(stochastic parameterization identification using ensemble kalman\\n  filtering combined with expectation-maximization and newton-raphson maximum\\n  likelihood methods\\n  for modelling geophysical systems, large-scale processes are described\\nthrough a set of coarse-grained dynamical equations while small-scale processes\\nare represented via parameterizations. this work proposes a method for\\nidentifying the best possible stochastic parameterization from noisy data.\\nstate-the-art sequential estimation methods such as kalman and particle filters\\ndo not achieve this goal succesfully because both suffer from the collapse of\\nthe parameter posterior distribution. to overcome this intrinsic limitation, we\\npropose two statistical learning methods. they are based on the combination of\\ntwo methodologies: the maximization of the likelihood via\\nexpectation-maximization (em) and newton-raphson (nr) algorithms which are\\nmainly applied in the statistic and machine learning communities, and the\\nensemble kalman filter (enkf). the methods are derived using a bayesian\\napproach for a hidden markov model. they are applied to infer deterministic and\\nstochastic physical parameters from noisy observations in coarse-grained\\ndynamical models. numerical experiments are conducted using the lorenz-96\\ndynamical system with one and two scales as a proof-of-concept. the imperfect\\ncoarse-grained model is modelled through a one-scale lorenz-96 system in which\\na stochastic parameterization is incorpored to represent the small-scale\\ndynamics. the algorithms are able to identify an optimal stochastic\\nparameterization with a good accuracy under moderate observational noise. the\\nproposed enkf-em and enkf-nr are promising statistical learning methods for\\ndeveloping stochastic parameterizations in high-dimensional geophysical models.\\n)': False, 'contains(biogeography based satellite image classification\\n  biogeography is the study of the geographical distribution of biological\\norganisms. the mindset of the engineer is that we can learn from nature.\\nbiogeography based optimization is a burgeoning nature inspired technique to\\nfind the optimal solution of the problem. satellite image classification is an\\nimportant task because it is the only way we can know about the land cover map\\nof inaccessible areas. though satellite images have been classified in past by\\nusing various techniques, the researchers are always finding alternative\\nstrategies for satellite image classification so that they may be prepared to\\nselect the most appropriate technique for the feature extraction task in hand.\\nthis paper is focused on classification of the satellite image of a particular\\nland cover using the theory of biogeography based optimization. the original\\nbbo algorithm does not have the inbuilt property of clustering which is\\nrequired during image classification. hence modifications have been proposed to\\nthe original algorithm and the modified algorithm is used to classify the\\nsatellite image of a given region. the results indicate that highly accurate\\nland cover features can be extracted effectively when the proposed algorithm is\\nused.\\n)': False, 'contains(understanding the 2016 us presidential election using ecological\\n  inference and distribution regression with census microdata\\n  we combine fine-grained spatially referenced census data with the vote\\noutcomes from the 2016 us presidential election. using this dataset, we perform\\necological inference using distribution regression (flaxman et al, kdd 2015)\\nwith a multinomial-logit regression so as to model the vote outcome trump,\\nclinton, other / didn\\'t vote as a function of demographic and socioeconomic\\nfeatures. ecological inference allows us to estimate \"exit poll\" style results\\nlike what was trump\\'s support among white women, but for entirely novel\\ncategories. we also perform exploratory data analysis to understand which\\ncensus variables are predictive of voting for trump, voting for clinton, or not\\nvoting for either. all of our methods are implemented in python and r and are\\navailable online for replication.\\n)': False, 'contains(performance evaluation and hyperparameter tuning of statistical and\\n  machine-learning models using spatial data\\n  machine-learning algorithms have gained popularity in recent years in the\\nfield of ecological modeling due to their promising results in predictive\\nperformance of classification problems. while the application of such\\nalgorithms has been highly simplified in the last years due to their\\nwell-documented integration in commonly used statistical programming languages\\nsuch as r, there are several practical challenges in the field of ecological\\nmodeling related to unbiased performance estimation, optimization of algorithms\\nusing hyperparameter tuning and spatial autocorrelation. we address these\\nissues in the comparison of several widely used machine-learning algorithms\\nsuch as boosted regression trees (brt), k-nearest neighbor (wknn), random\\nforest (rf) and support vector machine (svm) to traditional parametric\\nalgorithms such as logistic regression (glm) and semi-parametric ones like\\ngeneralized additive models (gam). different nested cross-validation methods\\nincluding hyperparameter tuning methods are used to evaluate model performances\\nwith the aim to receive bias-reduced performance estimates. as a case study the\\nspatial distribution of forest disease diplodia sapinea in the basque country\\nin spain is investigated using common environmental variables such as\\ntemperature, precipitation, soil or lithology as predictors. results show that\\ngam and rf (mean auroc estimates 0.708 and 0.699) outperform all other methods\\nin predictive accuracy. the effect of hyperparameter tuning saturates at around\\n50 iterations for this data set. the auroc differences between the bias-reduced\\n(spatial cross-validation) and overoptimistic (non-spatial cross-validation)\\nperformance estimates of the gam and rf are 0.167 (24%) and 0.213 (30%),\\nrespectively. it is recommended to also use spatial partitioning for\\ncross-validation hyperparameter tuning of spatial data.\\n)': False, 'contains(an ensemble quadratic echo state network for nonlinear spatio-temporal\\n  forecasting\\n  spatio-temporal data and processes are prevalent across a wide variety of\\nscientific disciplines. these processes are often characterized by nonlinear\\ntime dynamics that include interactions across multiple scales of spatial and\\ntemporal variability. the data sets associated with many of these processes are\\nincreasing in size due to advances in automated data measurement, management,\\nand numerical simulator output. non- linear spatio-temporal models have only\\nrecently seen interest in statistics, but there are many classes of such models\\nin the engineering and geophysical sciences. tradi- tionally, these models are\\nmore heuristic than those that have been presented in the statistics\\nliterature, but are often intuitive and quite efficient computationally. we\\nshow here that with fairly simple, but important, enhancements, the echo state\\nnet- work (esn) machine learning approach can be used to generate long-lead\\nforecasts of nonlinear spatio-temporal processes, with reasonable uncertainty\\nquantification, and at only a fraction of the computational expense of a\\ntraditional parametric nonlinear spatio-temporal models.\\n)': False, \"contains(evaluating compositionality in sentence embeddings\\n  an important challenge for human-like ai is compositional semantics. recent\\nresearch has attempted to address this by using deep neural networks to learn\\nvector space embeddings of sentences, which then serve as input to other tasks.\\nwe present a new dataset for one such task, `natural language inference' (nli),\\nthat cannot be solved using only word-level knowledge and requires some\\ncompositionality. we find that the performance of state of the art sentence\\nembeddings (infersent; conneau et al., 2017) on our new dataset is poor. we\\nanalyze the decision rules learned by infersent and find that they are\\nconsistent with simple heuristics that are ecologically valid in its training\\ndataset. further, we find that augmenting training with our dataset improves\\ntest performance on our dataset without loss of performance on the original\\ntraining dataset. this highlights the importance of structured datasets in\\nbetter understanding and improving ai systems.\\n)\": False, 'contains(the census and the second law: an entropic approach to optimal\\n  apportionment for the u.s. house of representatives\\n  the constitutionally mandated task of assigning congressional seats to the\\nvarious u.s. states proportional to their represented populations (\"according\\nto their numbers\") has engendered much contention, but rather less consensus.\\nusing the same principles of entropic inference that underlie the foundations\\nof information theory and statistical thermodynamics, and also enjoy fruitful\\napplication in image processing, spectral analysis, machine learning,\\neconometrics, bioinformatics, and a growing number of other fields, we motivate\\nand explore a method for congressional apportionment based on minimizing\\nrelative entropy (also known as kullback-leibler divergence), or, equivalently,\\nmaximizing shannon entropy. in terms of communication theory, we might say that\\nthe entropic apportionment gives each constituent as equal a voice as possible.\\nif we view representational weight as a finite resource to be distributed\\namongst the represented population, the entropic measure is identical with the\\ntheil index long employed in economics to measure inequality in the\\ndistribution of wealth or income, or in ecology to measure the distribution of\\nbiomass or reproductive fitness. besides congressional apportionment, the\\nmethod is also directly applicable to other multi-regional or\\nmulti-constituency legislatures, to party-list proportional voting systems used\\nin various parliamentary elections, and similar settings, where the task is to\\nallocate a discrete number of seats or other resources, and the primary goal is\\none of maximal proportionality or equity. in addition, the same entropic\\nfigure-of-merit can be used in parallel to compare different choices for the\\ntotal number of representatives, and then subsequently to assess different\\ncongressional district sizes, after seats are assigned and proposed district\\nboundaries drawn.\\n)': False, 'contains(decadal climate predictions using sequential learning algorithms\\n  ensembles of climate models are commonly used to improve climate predictions\\nand assess the uncertainties associated with them. weighting the models\\naccording to their performances holds the promise of further improving their\\npredictions. here, we use an ensemble of decadal climate predictions to\\ndemonstrate the ability of sequential learning algorithms (slas) to reduce the\\nforecast errors and reduce the uncertainties. three different slas are\\nconsidered, and their performances are compared with those of an equally\\nweighted ensemble, a linear regression and the climatology. predictions of four\\ndifferent variables--the surface temperature, the zonal and meridional wind,\\nand pressure--are considered. the spatial distributions of the performances are\\npresented, and the statistical significance of the improvements achieved by the\\nslas is tested. based on the performances of the slas, we propose one to be\\nhighly suitable for the improvement of decadal climate predictions.\\n)': False, \"contains(the evolution of precipitate crystal structures in an al-mg-si(-cu)\\n  alloy studied by a combined haadf-stem and sped approach\\n  this work presents a detailed investigation into the effect of a low cu\\naddition (0.01 at.%) on precipitation in an al-0.80mg-0.85si alloy during\\nageing. the precipitate crystal structures were assessed by scanning\\ntransmission electron microscopy combined with a novel scanning precession\\nelectron diffraction approach, which includes machine learning. the combination\\nof techniques enabled evaluation of the atomic arrangement within individual\\nprecipitates, as well as an improved estimate of precipitate phase fractions at\\neach ageing condition, through analysis of a statistically significant number\\nof precipitates. based on the obtained results, the total amount of solute\\natoms locked inside precipitates could be approximated. it was shown that even\\nwith a cu content close to impurity levels, the al-mg-si system precipitation\\nwas significantly affected with overageing. the principal change was due to a\\ngradually increasing phase fraction of the cu-containing q'-phase, which\\neventually was seen to dominate the precipitate structures. the structural\\novertake could be explained based on a continuous formation of the thermally\\nstable q'-phase, with cu atomic columns incorporating less cu than what could\\npotentially be accommodated.\\n)\": False, 'contains(aboveground biomass mapping in french guiana by combining remote\\n  sensing, forest inventories and environmental data\\n  mapping forest aboveground biomass (agb) has become an important task,\\nparticularly for the reporting of carbon stocks and changes. agb can be mapped\\nusing synthetic aperture radar data (sar) or passive optical data. however,\\nthese data are insensitive to high agb levels (\\\\textgreater{}150 mg/ha, and\\n\\\\textgreater{}300 mg/ha for p-band), which are commonly found in tropical\\nforests. studies have mapped the rough variations in agb by combining optical\\nand environmental data at regional and global scales. nevertheless, these maps\\ncannot represent local variations in agb in tropical forests. in this paper, we\\nhypothesize that the problem of misrepresenting local variations in agb and agb\\nestimation with good precision occurs because of both methodological limits\\n(signal saturation or dilution bias) and a lack of adequate calibration data in\\nthis range of agb values. we test this hypothesis by developing a calibrated\\nregression model to predict variations in high agb values (mean\\n\\\\textgreater{}300 mg/ha) in french guiana by a methodological approach for\\nspatial extrapolation with data from the optical geoscience laser altimeter\\nsystem (glas), forest inventories, radar, optics, and environmental variables\\nfor spatial inter-and extrapolation. given their higher point count, glas data\\nallow a wider coverage of agb values. we find that the metrics from glas\\nfootprints are correlated with field agb estimations (r 2 =0.54, rmse=48.3\\nmg/ha) with no bias for high values. first, predictive models, including\\nremote-sensing, environmental variables and spatial correlation functions,\\nallow us to obtain \"wall-to-wall\" agb maps over french guiana with an rmse for\\nthe in situ agb estimates of ~51 mg/ha and r${}^2$=0.48 at a 1-km grid size. we\\nconclude that a calibrated regression model based on glas with dependent\\nenvironmental data can produce good agb predictions even for high agb values if\\nthe calibration data fit the agb range. we also demonstrate that small temporal\\nand spatial mismatches between field data and glas footprints are not a problem\\nfor regional and global calibrated regression models because field data aim to\\npredict large and deep tendencies in agb variations from environmental\\ngradients and do not aim to represent high but stochastic and temporally\\nlimited variations from forest dynamics. thus, we advocate including a greater\\nvariety of data, even if less precise and shifted, to better represent high agb\\nvalues in global models and to improve the fitting of these models for high\\nvalues.\\n)': False, 'contains(seismic inversion by newtonian machine learning\\n  we present a wave-equation inversion method that inverts skeletonized data\\nfor the subsurface velocity model. the skeletonized representation of the\\nseismic traces consists of the low-rank latent-space variables predicted by a\\nwell-trained autoencoder neural network. the input to the autoencoder is the\\nrecorded common shot gathers, and the implicit function theorem is used to\\ndetermine the perturbation of the skeletonized data with respect to the\\nvelocity perturbation. the final velocity model is the one that best predicts\\nthe observed latent-space parameters. empirical results suggest that the\\ncycle-skipping problem is largely mitigated compared to the conventional full\\nwaveform inversion (fwi) method by replacing the waveform differences by those\\nof the latent-space parameters. the advantage of this method over other\\nskeletonized data methods is that no manual picking of important features is\\nrequired because the skeletal data are automatically selected by the\\nautoencoder. the most significant contribution of this paper is that it\\nprovides a general framework for using solutions to the governing pde to invert\\nskeletal data generated by any type of a neural network. the governing equation\\ncan be that for gravity, seismic waves, electromagnetic fields, and magnetic\\nfields. the input data can be the records from different types of data and\\ntheir skeletal features, as long as the model parameters are sensitive to their\\nperturbations. the skeletal data can be the latent space variables of an\\nautoencoder, a variational autoencoder, or a feature map from a convolutional\\nneural network (cnn), or principal component analysis (pca) features. in other\\nwords, we have combined the best features of newtonian physics and the pattern\\nmatching capabilities of machine learning to invert seismic data by newtonian\\nmachine learning.\\n)': False, \"contains(three dimensional deep learning approach for remote sensing image\\n  classification\\n  recently, a variety of approaches has been enriching the field of remote\\nsensing (rs) image processing and analysis. unfortunately, existing methods\\nremain limited faced to the rich spatio-spectral content of today's large\\ndatasets. it would seem intriguing to resort to deep learning (dl) based\\napproaches at this stage with regards to their ability to offer accurate\\nsemantic interpretation of the data. however, the specificity introduced by the\\ncoexistence of spectral and spatial content in the rs datasets widens the scope\\nof the challenges presented to adapt dl methods to these contexts. therefore,\\nthe aim of this paper is firstly to explore the performance of dl architectures\\nfor the rs hyperspectral dataset classification and secondly to introduce a new\\nthree-dimensional dl approach that enables a joint spectral and spatial\\ninformation process. a set of three-dimensional schemes is proposed and\\nevaluated. experimental results based on well knownhyperspectral datasets\\ndemonstrate that the proposed method is able to achieve a better classification\\nrate than state of the art methods with lower computational costs.\\n)\": False, 'contains(satellite image forgery detection and localization using gan and\\n  one-class classifier\\n  current satellite imaging technology enables shooting high-resolution\\npictures of the ground. as any other kind of digital images, overhead pictures\\ncan also be easily forged. however, common image forensic techniques are often\\ndeveloped for consumer camera images, which strongly differ in their nature\\nfrom satellite ones (e.g., compression schemes, post-processing, sensors,\\netc.). therefore, many accurate state-of-the-art forensic algorithms are bound\\nto fail if blindly applied to overhead image analysis. development of novel\\nforensic tools for satellite images is paramount to assess their authenticity\\nand integrity. in this paper, we propose an algorithm for satellite image\\nforgery detection and localization. specifically, we consider the scenario in\\nwhich pixels within a region of a satellite image are replaced to add or remove\\nan object from the scene. our algorithm works under the assumption that no\\nforged images are available for training. using a generative adversarial\\nnetwork (gan), we learn a feature representation of pristine satellite images.\\na one-class support vector machine (svm) is trained on these features to\\ndetermine their distribution. finally, image forgeries are detected as\\nanomalies. the proposed algorithm is validated against different kinds of\\nsatellite images containing forgeries of different size and shape.\\n)': False, 'contains(emergence of structured behaviors from curiosity-based intrinsic\\n  motivation\\n  infants are experts at playing, with an amazing ability to generate novel\\nstructured behaviors in unstructured environments that lack clear extrinsic\\nreward signals. we seek to replicate some of these abilities with a neural\\nnetwork that implements curiosity-driven intrinsic motivation. using a simple\\nbut ecologically naturalistic simulated environment in which the agent can move\\nand interact with objects it sees, the agent learns a world model predicting\\nthe dynamic consequences of its actions. simultaneously, the agent learns to\\ntake actions that adversarially challenge the developing world model, pushing\\nthe agent to explore novel and informative interactions with its environment.\\nwe demonstrate that this policy leads to the self-supervised emergence of a\\nspectrum of complex behaviors, including ego motion prediction, object\\nattention, and object gathering. moreover, the world model that the agent\\nlearns supports improved performance on object dynamics prediction and\\nlocalization tasks. our results are a proof-of-principle that computational\\nmodels of intrinsic motivation might account for key features of developmental\\nvisuomotor learning in infants.\\n)': False, \"contains(poincar√© inequalities on intervals -- application to sensitivity\\n  analysis\\n  the development of global sensitivity analysis of numerical model outputs has\\nrecently raised new issues on 1-dimensional poincar\\\\'e inequalities. typically\\ntwo kind of sensitivity indices are linked by a poincar\\\\'e type inequality,\\nwhich provide upper bounds of the most interpretable index by using the other\\none, cheaper to compute. this allows performing a low-cost screening of\\nunessential variables. the efficiency of this screening then highly depends on\\nthe accuracy of the upper bounds in poincar\\\\'e inequalities. the novelty in the\\nquestions concern the wide range of probability distributions involved, which\\nare often truncated on intervals. after providing an overview of the existing\\nknowledge and techniques, we add some theory about poincar\\\\'e constants on\\nintervals, with improvements for symmetric intervals. then we exploit the\\nspectral interpretation for computing exact value of poincar\\\\'e constants of\\nany admissible distribution on a given interval. we give semi-analytical\\nresults for some frequent distributions (truncated exponential, triangular,\\ntruncated normal), and present a numerical method in the general case. finally,\\nan application is made to a hydrological problem, showing the benefits of the\\nnew results in poincar\\\\'e inequalities to sensitivity analysis.\\n)\": False, \"contains(spectral image visualization using generative adversarial networks\\n  spectral images captured by satellites and radio-telescopes are analyzed to\\nobtain information about geological compositions distributions, distant asters\\nas well as undersea terrain. spectral images usually contain tens to hundreds\\nof continuous narrow spectral bands and are widely used in various fields. but\\nthe vast majority of those image signals are beyond the visible range, which\\ncalls for special visualization technique. the visualizations of spectral\\nimages shall convey as much information as possible from the original signal\\nand facilitate image interpretation. however, most of the existing visualizatio\\nmethods display spectral images in false colors, which contradict with human's\\nexperience and expectation. in this paper, we present a novel visualization\\ngenerative adversarial network (gan) to display spectral images in natural\\ncolors. to achieve our goal, we propose a loss function which consists of an\\nadversarial loss and a structure loss. the adversarial loss pushes our solution\\nto the natural image distribution using a discriminator network that is trained\\nto differentiate between false-color images and natural-color images. we also\\nuse a cycle loss as the structure constraint to guarantee structure\\nconsistency. experimental results show that our method is able to generate\\nstructure-preserved and natural-looking visualizations.\\n)\": False, 'contains(machine learning predicts laboratory earthquakes\\n  forecasting fault failure is a fundamental but elusive goal in earthquake\\nscience. here we show that by listening to the acoustic signal emitted by a\\nlaboratory fault, machine learning can predict the time remaining before it\\nfails with great accuracy. these predictions are based solely on the\\ninstantaneous physical characteristics of the acoustical signal, and do not\\nmake use of its history. surprisingly, machine learning identifies a signal\\nemitted from the fault zone previously thought to be low-amplitude noise that\\nenables failure forecasting throughout the laboratory quake cycle. we\\nhypothesize that applying this approach to continuous seismic data may lead to\\nsignificant advances in identifying currently unknown signals, in providing new\\ninsights into fault physics, and in placing bounds on fault failure times.\\n)': False, 'contains(driving digital rock towards machine learning: predicting permeability\\n  with gradient boosting and deep neural networks\\n  we present a research study aimed at testing of applicability of machine\\nlearning techniques for prediction of permeability of digitized rock samples.\\nwe prepare a training set containing 3d images of sandstone samples imaged with\\nx-ray microtomography and corresponding permeability values simulated with pore\\nnetwork approach. we also use minkowski functionals and deep learning-based\\ndescriptors of 3d images and 2d slices as input features for predictive model\\ntraining and prediction. we compare predictive power of various feature sets\\nand methods. the later include gradient boosting and various architectures of\\ndeep neural networks (dnn). the results demonstrate applicability of machine\\nlearning for image-based permeability prediction and open a new area of digital\\nrock research.\\n)': False, 'contains(ionospheric activity prediction using convolutional recurrent neural\\n  networks\\n  the ionosphere electromagnetic activity is a major factor of the quality of\\nsatellite telecommunications, global navigation satellite systems (gnss) and\\nother vital space applications. being able to forecast globally the total\\nelectron content (tec) would enable a better anticipation of potential\\nperformance degradations. a few studies have proposed models able to predict\\nthe tec locally, but not worldwide for most of them. thanks to a large record\\nof past tec maps publicly available, we propose a method based on deep neural\\nnetworks (dnn) to forecast a sequence of global tec maps consecutive to an\\ninput sequence of tec maps, without introducing any prior knowledge other than\\nearth rotation periodicity. by combining several state-of-the-art\\narchitectures, the proposed approach is competitive with previous works on tec\\nforecasting while predicting the tec globally.\\n)': False, 'contains(public decision support for low population density areas: an\\n  imbalance-aware hyper-ensemble for spatio-temporal crime prediction\\n  crime events are known to reveal spatio-temporal patterns, which can be used\\nfor predictive modeling and subsequent decision support. while the focus has\\nhitherto been placed on areas with high population density, we address the\\nchallenging undertaking of predicting crime hotspots in regions with low\\npopulation densities and highly unequally-distributed crime.this results in a\\nsevere sparsity (i.e., class imbalance) of the outcome variable, which impedes\\npredictive modeling. to alleviate this, we develop machine learning models for\\nspatio-temporal prediction that are specifically adjusted for an imbalanced\\ndistribution of the class labels and test them in an actual setting with\\nstate-of-the-art predictors (i.e., socio-economic, geographical, temporal,\\nmeteorological, and crime variables in fine resolution). the proposed\\nimbalance-aware hyper-ensemble increases the hit ratio considerably from 18.1%\\nto 24.6% when aiming for the top 5% of hotspots, and from 53.1% to 60.4% when\\naiming for the top 20% of hotspots.\\n)': False, 'contains(analytic expressions for stochastic distances between relaxed complex\\n  wishart distributions\\n  the scaled complex wishart distribution is a widely used model for multilook\\nfull polarimetric sar data whose adequacy has been attested in the literature.\\nclassification, segmentation, and image analysis techniques which depend on\\nthis model have been devised, and many of them employ some type of\\ndissimilarity measure. in this paper we derive analytic expressions for four\\nstochastic distances between relaxed scaled complex wishart distributions in\\ntheir most general form and in important particular cases. using these\\ndistances, inequalities are obtained which lead to new ways of deriving the\\nbartlett and revised wishart distances. the expressiveness of the four analytic\\ndistances is assessed with respect to the variation of parameters. such\\ndistances are then used for deriving new tests statistics, which are proved to\\nhave asymptotic chi-square distribution. adopting the test size as a comparison\\ncriterion, a sensitivity study is performed by means of monte carlo experiments\\nsuggesting that the bhattacharyya statistic outperforms all the others. the\\npower of the tests is also assessed. applications to actual data illustrate the\\ndiscrimination and homogeneity identification capabilities of these distances.\\n)': False, 'contains(data set from molisan regional seismic network events\\n  after the earthquake occurred in molise (central italy) on 31st october 2002\\n(ml 5.4, 29 people dead), the local servizio regionale per la protezione civile\\nto ensure a better analysis of local seismic data, through a convention with\\nthe istituto nazionale di geofisica e vulcanologia (ingv), promoted the design\\nof the regional seismic network (rmsm) and funded its implementation. the 5\\nstations of rmsm worked since 2007 to 2013 collecting a large amount of seismic\\ndata and giving an important contribution to the study of seismic sources\\npresent in the region and the surrounding territory. this work reports about\\nthe dataset containing all triggers collected by rmsm since july 2007 to march\\n2009, including actual seismic events; among them, all earthquakes events\\nrecorded in coincidence to rete sismica nazionale centralizzata (rsnc) of ingv\\nhave been marked with s and p arrival timestamps. every trigger has been\\nassociated to a spectrogram defined into a recorded time vs. frequency domain.\\nthe main aim of this structured dataset is to be used for further analysis with\\ndata mining and machine learning techniques on image patterns associated to the\\nwaveforms.\\n)': False, 'contains(transfer learning from deep features for remote sensing and poverty\\n  mapping\\n  the lack of reliable data in developing countries is a major obstacle to\\nsustainable development, food security, and disaster relief. poverty data, for\\nexample, is typically scarce, sparse in coverage, and labor-intensive to\\nobtain. remote sensing data such as high-resolution satellite imagery, on the\\nother hand, is becoming increasingly available and inexpensive. unfortunately,\\nsuch data is highly unstructured and currently no techniques exist to\\nautomatically extract useful insights to inform policy decisions and help\\ndirect humanitarian efforts. we propose a novel machine learning approach to\\nextract large-scale socioeconomic indicators from high-resolution satellite\\nimagery. the main challenge is that training data is very scarce, making it\\ndifficult to apply modern techniques such as convolutional neural networks\\n(cnn). we therefore propose a transfer learning approach where nighttime light\\nintensities are used as a data-rich proxy. we train a fully convolutional cnn\\nmodel to predict nighttime lights from daytime imagery, simultaneously learning\\nfeatures that are useful for poverty prediction. the model learns filters\\nidentifying different terrains and man-made structures, including roads,\\nbuildings, and farmlands, without any supervision beyond nighttime lights. we\\ndemonstrate that these learned features are highly informative for poverty\\nmapping, even approaching the predictive performance of survey data collected\\nin the field.\\n)': False, 'contains(unsupervised discovery of el nino using causal feature learning on\\n  microlevel climate data\\n  we show that the climate phenomena of el nino and la nina arise naturally as\\nstates of macro-variables when our recent causal feature learning framework\\n(chalupka 2015, chalupka 2016) is applied to micro-level measures of zonal wind\\n(zw) and sea surface temperatures (sst) taken over the equatorial band of the\\npacific ocean. the method identifies these unusual climate states on the basis\\nof the relation between zw and sst patterns without any input about past\\noccurrences of el nino or la nina. the simpler alternatives of (i) clustering\\nthe sst fields while disregarding their relationship with zw patterns, or (ii)\\nclustering the joint zw-sst patterns, do not discover el nino. we discuss the\\ndegree to which our method supports a causal interpretation and use a\\nlow-dimensional toy example to explain its success over other clustering\\napproaches. finally, we propose a new robust and scalable alternative to our\\noriginal algorithm (chalupka 2016), which circumvents the need for\\nhigh-dimensional density learning.\\n)': False, 'contains(evolving spatially aggregated features from satellite imagery for\\n  regional modeling\\n  satellite imagery and remote sensing provide explanatory variables at\\nrelatively high resolutions for modeling geospatial phenomena, yet regional\\nsummaries are often desirable for analysis and actionable insight. in this\\npaper, we propose a novel method of inducing spatial aggregations as a\\ncomponent of the machine learning process, yielding regional model features\\nwhose construction is driven by model prediction performance rather than prior\\nassumptions. our results demonstrate that genetic programming is particularly\\nwell suited to this type of feature construction because it can automatically\\nsynthesize appropriate aggregations, as well as better incorporate them into\\npredictive models compared to other regression methods we tested. in our\\nexperiments we consider a specific problem instance and real-world dataset\\nrelevant to predicting snow properties in high-mountain asia.\\n)': False, 'contains(conditional chow-liu tree structures for modeling discrete-valued vector\\n  time series\\n  we consider the problem of modeling discrete-valued vector time series data\\nusing extensions of chow-liu tree models to capture both dependencies across\\ntime and dependencies across variables. conditional chow-liu tree models are\\nintroduced, as an extension to standard chow-liu trees, for modeling\\nconditional rather than joint densities. we describe learning algorithms for\\nsuch models and show how they can be used to learn parsimonious representations\\nfor the output distributions in hidden markov models. these models are applied\\nto the important problem of simulating and forecasting daily precipitation\\noccurrence for networks of rain stations. to demonstrate the effectiveness of\\nthe models, we compare their performance versus a number of alternatives using\\nhistorical precipitation data from southwestern australia and the western\\nunited states. we illustrate how the structure and parameters of the models can\\nbe used to provide an improved meteorological interpretation of such data.\\n)': False, 'contains(joint gaussian processes for biophysical parameter retrieval\\n  solving inverse problems is central to geosciences and remote sensing.\\nradiative transfer models (rtms) represent mathematically the physical laws\\nwhich govern the phenomena in remote sensing applications (forward models). the\\nnumerical inversion of the rtm equations is a challenging and computationally\\ndemanding problem, and for this reason, often the application of a nonlinear\\nstatistical regression is preferred. in general, regression models predict the\\nbiophysical parameter of interest from the corresponding received radiance.\\nhowever, this approach does not employ the physical information encoded in the\\nrtms. an alternative strategy, which attempts to include the physical\\nknowledge, consists in learning a regression model trained using data simulated\\nby an rtm code. in this work, we introduce a nonlinear nonparametric regression\\nmodel which combines the benefits of the two aforementioned approaches. the\\ninversion is performed taking into account jointly both real observations and\\nrtm-simulated data. the proposed joint gaussian process (jgp) provides a solid\\nframework for exploiting the regularities between the two types of data. the\\njgp automatically detects the relative quality of the simulated and real data,\\nand combines them accordingly. this occurs by learning an additional\\nhyper-parameter w.r.t. a standard gp model, and fitting parameters through\\nmaximizing the pseudo-likelihood of the real observations. the resulting scheme\\nis both simple and robust, i.e., capable of adapting to different scenarios.\\nthe advantages of the jgp method compared to benchmark strategies are shown\\nconsidering rtm-simulated and real observations in different experiments.\\nspecifically, we consider leaf area index (lai) retrieval from landsat data\\ncombined with simulated data generated by the prosail model.\\n)': False, 'contains(a stackelberg game perspective on the conflict between machine learning\\n  and data obfuscation\\n  data is the new oil; this refrain is repeated extensively in the age of\\ninternet tracking, machine learning, and data analytics. as data collection\\nbecomes more personal and pervasive, however, public pressure is mounting for\\nprivacy protection. in this atmosphere, developers have created applications to\\nadd noise to user attributes visible to tracking algorithms. this creates a\\nstrategic interaction between trackers and users when incentives to maintain\\nprivacy and improve accuracy are misaligned. in this paper, we conceptualize\\nthis conflict through an n+1-player, augmented stackelberg game. first a\\nmachine learner declares a privacy protection level, and then users respond by\\nchoosing their own perturbation amounts. we use the general frameworks of\\ndifferential privacy and empirical risk minimization to quantify the utility\\ncomponents due to privacy and accuracy, respectively. in equilibrium, each user\\nperturbs her data independently, which leads to a high net loss in accuracy. to\\nremedy this scenario, we show that the learner improves his utility by\\nproactively perturbing the data himself. while other work in this area has\\nstudied privacy markets and mechanism design for truthful reporting of user\\ninformation, we take a different viewpoint by considering both user and learner\\nperturbation.\\n)': False, 'contains(phaselink: a deep learning approach to seismic phase association\\n  seismic phase association is a fundamental task in seismology that pertains\\nto linking together phase detections on different sensors that originate from a\\ncommon earthquake. it is widely employed to detect earthquakes on permanent and\\ntemporary seismic networks, and underlies most seismicity catalogs produced\\naround the world. this task can be challenging because the number of sources is\\nunknown, events frequently overlap in time, or can occur simultaneously in\\ndifferent parts of a network. we present phaselink, a framework based on recent\\nadvances in deep learning for grid-free earthquake phase association. our\\napproach learns to link phases together that share a common origin, and is\\ntrained entirely on tens of millions of synthetic sequences of p- and s-wave\\narrival times generated using a simple 1d velocity model. our approach is\\nsimple to implement for any tectonic regime, suitable for real-time processing,\\nand can naturally incorporate errors in arrival time picks. rather than tuning\\na set of ad hoc hyperparameters to improve performance, phaselink can be\\nimproved by simply adding examples of problematic cases to the training\\ndataset. we demonstrate the state-of-the-art performance of phaselink on a\\nchallenging recent sequence from southern california, and synthesized sequences\\nfrom japan designed to test the point at which the method fails. for the\\nexamined datasets, phaselink can precisely associate p- and s-picks to events\\nthat are separated by ~12 seconds in origin time. this approach is expected to\\nimprove the resolution of seismicity catalogs, add stability to real-time\\nseismic monitoring, and streamline automated processing of large seismic\\ndatasets.\\n)': False, 'contains(travel time tomography with adaptive dictionaries\\n  we develop a 2d travel time tomography method which regularizes the inversion\\nby modeling groups of slowness pixels from discrete slowness maps, called\\npatches, as sparse linear combinations of atoms from a dictionary. we propose\\nto use dictionary learning during the inversion to adapt dictionaries to\\nspecific slowness maps. this patch regularization, called the local model, is\\nintegrated into the overall slowness map, called the global model. the local\\nmodel considers small-scale variations using a sparsity constraint and the\\nglobal model considers larger-scale features constrained using $\\\\ell_2$\\nregularization. this strategy in a locally-sparse travel time tomography (lst)\\napproach enables simultaneous modeling of smooth and discontinuous slowness\\nfeatures. this is in contrast to conventional tomography methods, which\\nconstrain models to be exclusively smooth or discontinuous. we develop a\\n$\\\\textit{maximum a posteriori}$ formulation for lst and exploit the sparsity of\\nslowness patches using dictionary learning. the lst approach compares favorably\\nwith smoothness and total variation regularization methods on densely, but\\nirregularly sampled synthetic slowness maps.\\n)': False, 'contains(model parameter estimation using coherent structure coloring\\n  lagrangian data assimilation is a complex problem in oceanic and atmospheric\\nmodeling. tracking drifters in large-scale geophysical flows can involve\\nuncertainty in drifter location, complex inertial effects, and other factors\\nwhich make comparing them to simulated lagrangian trajectories from numerical\\nmodels extremely challenging. temporal and spatial discretization, factors\\nnecessary in modeling large scale flows, also contribute to separation between\\nreal and simulated drifter trajectories. the chaotic advection inherent in\\nthese turbulent flows tends to separate even closely spaced tracer particles,\\nmaking error metrics based solely on drifter displacements unsuitable for\\nestimating model parameters. we propose to instead use error in the coherent\\nstructure coloring (csc) field to assess model skill. the csc field provides a\\nspatial representation of the underlying coherent patterns in the flow, and we\\nshow that it is a more robust metric for assessing model accuracy. through the\\nuse of two test cases, one considering spatial uncertainty in particle\\ninitialization, and one examining the influence of stochastic error along a\\ntrajectory and temporal discretization, we show that error in the coherent\\nstructure coloring field can be used to accurately determine single or multiple\\nsimultaneously unknown model parameters, whereas a conventional error metric\\nbased on error in drifter displacement fails. because the csc field enhances\\nthe difference in error between correct and incorrect model parameters, error\\nminima in model parameter sweeps become more distinct. the effectiveness and\\nrobustness of this method for single and multi-parameter estimation in\\nanalytical flows suggests that lagrangian data assimilation for real oceanic\\nand atmospheric models would benefit from a similar approach.\\n)': False, \"contains(a scalable machine learning system for pre-season agriculture yield\\n  forecast\\n  yield forecast is essential to agriculture stakeholders and can be obtained\\nwith the use of machine learning models and data coming from multiple sources.\\nmost solutions for yield forecast rely on ndvi (normalized difference\\nvegetation index) data, which is time-consuming to be acquired and processed.\\nto bring scalability for yield forecast, in the present paper we describe a\\nsystem that incorporates satellite-derived precipitation and soil properties\\ndatasets, seasonal climate forecasting data from physical models and other\\nsources to produce a pre-season prediction of soybean/maize yield---with no\\nneed of ndvi data. this system provides significantly useful results by the\\nexempting the need for high-resolution remote-sensing data and allowing farmers\\nto prepare for adverse climate influence on the crop cycle. in our studies, we\\nforecast the soybean and maize yields for brazil and usa, which corresponded to\\n44% of the world's grain production in 2016. results show the error metrics for\\nsoybean and maize yield forecasts are comparable to similar systems that only\\nprovide yield forecast information in the first weeks to months of the crop\\ncycle.\\n)\": False, 'contains(evaluating aleatoric and epistemic uncertainties of time series deep\\n  learning models for soil moisture predictions\\n  soil moisture is an important variable that determines floods, vegetation\\nhealth, agriculture productivity, and land surface feedbacks to the atmosphere,\\netc. accurately modeling soil moisture has important implications in both\\nweather and climate models. the recently available satellite-based observations\\ngive us a unique opportunity to build data-driven models to predict soil\\nmoisture instead of using land surface models, but previously there was no\\nuncertainty estimate. we tested monte carlo dropout (mcd) with an aleatoric\\nterm for our long short-term memory models for this problem, and asked if the\\nuncertainty terms behave as they were argued to. we show that the method\\nsuccessfully captures the predictive error after tuning a hyperparameter on a\\nrepresentative training dataset. we show the mcd uncertainty estimate, as\\npreviously argued, does detect dissimilarity.\\n)': False, 'contains(identifying solar flare precursors using time series of sdo/hmi images\\n  and sharp parameters\\n  we present several methods towards construction of precursors, which show\\ngreat promise towards early predictions, of solar flare events in this paper. a\\ndata pre-processing pipeline is built to extract useful data from multiple\\nsources (geostationary operational environmental satellites (goes) and solar\\ndynamics observatory (sdo)/helioseismic and magnetic imager (hmi) to prepare\\ninputs for machine learning algorithms. two classification models are\\npresented: classification of flares from quiet times for active regions and\\nclassification of strong versus weak flare events. we adopt deep learning\\nalgorithms to capture both the spatial and temporal information from hmi\\nmagnetogram data. effective feature extraction and feature selection with raw\\nmagnetogram data using deep learning and statistical algorithms enable us to\\ntrain classification models to achieve almost as good performance as using\\nactive region parameters provided in hmi/space-weather hmi-active region patch\\n(sharp) data files. the results show great promise towards accurate, reliable,\\nand timely predictions of solar flare events. the use of atmospheric imaging\\nassembly (aia) data will be the topic of future studies.\\n)': False, \"contains(hyperspectral data analysis in r: the hsdar package\\n  hyperspectral remote sensing is a promising tool for a variety of\\napplications including ecology, geology, analytical chemistry and medical\\nresearch. this article presents the new \\\\hsdar package for r statistical\\nsoftware, which performs a variety of analysis steps taken during a typical\\nhyperspectral remote sensing approach. the package introduces a new class for\\nefficiently storing large hyperspectral datasets such as hyperspectral cubes\\nwithin r. the package includes several important hyperspectral analysis tools\\nsuch as continuum removal, normalized ratio indices and integrates two widely\\nused radiation transfer models. in addition, the package provides methods to\\ndirectly use the functionality of the caret package for machine learning tasks.\\ntwo case studies demonstrate the package's range of functionality: first, plant\\nleaf chlorophyll content is estimated and second, cancer in the human larynx is\\ndetected from hyperspectral data.\\n)\": False, 'contains(stratospheric aerosol injection as a deep reinforcement learning problem\\n  as global greenhouse gas emissions continue to rise, the use of stratospheric\\naerosol injection (sai), a form of solar geoengineering, is increasingly\\nconsidered in order to artificially mitigate climate change effects. however,\\ninitial research in simulation suggests that naive sai can have catastrophic\\nregional consequences, which may induce serious geostrategic conflicts. current\\ngeo-engineering research treats sai control in low-dimensional approximation\\nonly. we suggest treating sai as a high-dimensional control problem, with\\npolicies trained according to a context-sensitive reward function within the\\ndeep reinforcement learning (drl) paradigm. in order to facilitate training in\\nsimulation, we suggest to emulate hadcm3, a widely used general circulation\\nmodel, using deep learning techniques. we believe this is the first application\\nof drl to the climate sciences.\\n)': False, 'contains(petrophysical property estimation from seismic data using recurrent\\n  neural networks\\n  reservoir characterization involves the estimation petrophysical properties\\nfrom well-log data and seismic data. estimating such properties is a\\nchallenging task due to the non-linearity and heterogeneity of the subsurface.\\nvarious attempts have been made to estimate petrophysical properties using\\nmachine learning techniques such as feed-forward neural networks and support\\nvector regression (svr). recent advances in machine learning have shown\\npromising results for recurrent neural networks (rnn) in modeling complex\\nsequential data such as videos and speech signals. in this work, we propose an\\nalgorithm for property estimation from seismic data using recurrent neural\\nnetworks. an applications of the proposed workflow to estimate density and\\np-wave impedance using seismic data shows promising results compared to\\nfeed-forward neural networks.\\n)': False, 'contains(numerical methods as an integrated part of physics education\\n  during the last decade we have witnessed an impressive development in\\nso-called interpreted languages and computational environments such as maple,\\nmathematica, idl, matlab etc. problems which until recently were typically\\nsolved on mainframe machines and written in computing languages such as fortran\\nor c/c++, can now easily be solved on standard pcs with the bonus of immediate\\nvisualizations of the results.\\n  in our undergraduate programs an often posed question is how to incorporate\\nand exploit efficiently these advances in the standard physics and mathematics\\ncurriculum, without detracting the attention from the classical and basic\\ntheoretical and experimental topics to be covered. furthermore, if students are\\ntrained to use such tools at early stages in their education, do such tools\\nreally enhance and improve the learning environment? and, perhaps even more\\nimportant, does it lead to a better physics understanding?\\n  here we present one possible approach, where computational topics are\\ngradually baked into our undergraduate curriculum in mathematics and physics,\\nastronomy and meteorology. we focus on training our students to use general\\nprogramming tools in solving physics problems, in addition to the classical\\nanalytic problems. by this approach, the students gain an expertise that they\\ncan build upon in their future studies and careers. we use mainly java, matlab\\nand maple as computational environments. our students are now capable of\\nhandling at an early stage in their education more realistic physics problems\\nthan before. we believe firmly that, in addition to educating modern\\nscientists, this promotes a better physics understanding for a majority of the\\nstudents.\\n)': False, \"contains(algorithms for estimating trends in global temperature volatility\\n  trends in terrestrial temperature variability are perhaps more relevant for\\nspecies viability than trends in mean temperature. in this paper, we develop\\nmethodology for estimating such trends using multi-resolution climate data from\\npolar orbiting weather satellites. we derive two novel algorithms for\\ncomputation that are tailored for dense, gridded observations over both space\\nand time. we evaluate our methods with a simulation that mimics these data's\\nfeatures and on a large, publicly available, global temperature dataset with\\nthe eventual goal of tracking trends in cloud reflectance temperature\\nvariability.\\n)\": False, 'contains(wind estimation using quadcopter motion: a machine learning approach\\n  in this article, we study the well known problem of wind estimation in\\natmospheric turbulence using small unmanned aerial systems (suas). we present a\\nmachine learning approach to wind velocity estimation based on quadcopter state\\nmeasurements without a wind sensor. we accomplish this by training a long\\nshort-term memory (lstm) neural network (nn) on roll and pitch angles and\\nquadcopter position inputs with forcing wind velocities as the targets. the\\ndatasets are generated using a simulated quadcopter in turbulent wind fields.\\nthe trained neural network is deployed to estimate the turbulent winds as\\ngenerated by the dryden gust model as well as a realistic large eddy simulation\\n(les) of a near-neutral atmospheric boundary layer (abl) over flat terrain. the\\nresulting nn predictions are compared to a wind triangle approach that uses\\ntilt angle as an approximation of airspeed. results from this study indicate\\nthat the lstm-nn based approach predicts lower errors in both the mean and\\nvariance of the local wind field as compared to the wind triangle approach. the\\nwork reported in this article demonstrates the potential of machine learning\\nfor sensor-less wind estimation and has strong implications to large-scale\\nlow-altitude atmospheric sensing using suas for environmental and autonomous\\nnavigation applications.\\n)': False, \"contains(ecological non-linear state space model selection via adaptive particle\\n  markov chain monte carlo (adpmcmc)\\n  we develop a novel advanced particle markov chain monte carlo algorithm that\\nis capable of sampling from the posterior distribution of non-linear state\\nspace models for both the unobserved latent states and the unknown model\\nparameters. we apply this novel methodology to five population growth models,\\nincluding models with strong and weak allee effects, and test if it can\\nefficiently sample from the complex likelihood surface that is often associated\\nwith these models. utilising real and also synthetically generated data sets we\\nexamine the extent to which observation noise and process error may frustrate\\nefforts to choose between these models. our novel algorithm involves an\\nadaptive metropolis proposal combined with an sir particle mcmc algorithm\\n(adpmcmc). we show that the adpmcmc algorithm samples complex, high-dimensional\\nspaces efficiently, and is therefore superior to standard gibbs or metropolis\\nhastings algorithms that are known to converge very slowly when applied to the\\nnon-linear state space ecological models considered in this paper.\\nadditionally, we show how the adpmcmc algorithm can be used to recursively\\nestimate the bayesian cram\\\\'er-rao lower bound of tichavsk\\\\'y (1998). we derive\\nexpressions for these cram\\\\'er-rao bounds and estimate them for the models\\nconsidered. our results demonstrate a number of important features of common\\npopulation growth models, most notably their multi-modal posterior surfaces and\\ndependence between the static and dynamic parameters. we conclude by sampling\\nfrom the posterior distribution of each of the models, and use bayes factors to\\nhighlight how observation noise significantly diminishes our ability to select\\namong some of the models, particularly those that are designed to reproduce an\\nallee effect.\\n)\": False, 'contains(combining physically-based modeling and deep learning for fusing grace\\n  satellite data: can we learn from mismatch?\\n  global hydrological and land surface models are increasingly used for\\ntracking terrestrial total water storage (tws) dynamics, but the utility of\\nexisting models is hampered by conceptual and/or data uncertainties related to\\nvarious underrepresented and unrepresented processes, such as groundwater\\nstorage. the gravity recovery and climate experiment (grace) satellite mission\\nprovided a valuable independent data source for tracking tws at regional and\\ncontinental scales. strong interests exist in fusing grace data into global\\nhydrological models to improve their predictive performance. here we develop\\nand apply deep convolutional neural network (cnn) models to learn the\\nspatiotemporal patterns of mismatch between tws anomalies (twsa) derived from\\ngrace and those simulated by noah, a widely used land surface model. once\\ntrained, our cnn models can be used to correct the noah simulated twsa without\\nrequiring grace data, potentially filling the data gap between grace and its\\nfollow-on mission, grace-fo. our methodology is demonstrated over india, which\\nhas experienced significant groundwater depletion in recent decades that is\\nnevertheless not being captured by the noah model. results show that the cnn\\nmodels significantly improve the match with grace twsa, achieving a\\ncountry-average correlation coefficient of 0.94 and nash-sutcliff efficient of\\n0.87, or 14\\\\% and 52\\\\% improvement respectively over the original noah twsa. at\\nthe local scale, the learned mismatch pattern correlates well with the observed\\nin situ groundwater storage anomaly data for most parts of india, suggesting\\nthat deep learning models effectively compensate for the missing groundwater\\ncomponent in noah for this study region.\\n)': False, 'contains(the stability of the first neumann laplacian eigenfunction under domain\\n  deformations and applications\\n  the robustness of manifold learning methods is often predicated on the\\nstability of the neumann laplacian eigenfunctions under deformations of the\\nassumed underlying domain. indeed, many manifold learning methods are based on\\napproximating the neumann laplacian eigenfunctions on a manifold that is\\nassumed to underlie data, which is viewed through a source of distortion. in\\nthis paper, we study the stability of the first neumann laplacian eigenfunction\\nwith respect to deformations of a domain by a diffeomorphism. in particular, we\\nare interested in the stability of the first eigenfunction on tall thin domains\\nwhere, intuitively, the first neumann laplacian eigenfunction should only\\ndepend on the length along the domain. we prove a rigorous version of this\\nstatement and apply it to a machine learning problem in geophysical\\ninterpretation.\\n)': False, 'contains(strictly proper kernel scores and characteristic kernels on compact\\n  spaces\\n  strictly proper kernel scores are well-known tool in probabilistic\\nforecasting, while characteristic kernels have been extensively investigated in\\nthe machine learning literature. we first show that both notions coincide, so\\nthat insights from one part of the literature can be used in the other. we then\\nshow that the metric induced by a characteristic kernel cannot reliably\\ndistinguish between distributions that are far apart in the total variation\\nnorm as soon as the underlying space of measures is infinite dimensional. in\\naddition, we provide a characterization of characteristic kernels in terms of\\neigenvalues and -functions and apply this characterization to the case of\\ncontinuous kernels on (locally) compact spaces. in the compact case we further\\nshow that characteristic kernels exist if and only if the space is metrizable.\\nas special cases of our general theory we investigate translation-invariant\\nkernels on compact abelian groups and isotropic kernels on spheres. the latter\\nare of particular interest for forecast evaluation of probabilistic predictions\\non spherical domains as frequently encountered in meteorology and climatology.\\n)': False, \"contains(extensions of morse-smale regression with application to actuarial\\n  science\\n  the problem of subgroups is ubiquitous in scientific research (ex. disease\\nheterogeneity, spatial distributions in ecology...), and piecewise regression\\nis one way to deal with this phenomenon. morse-smale regression offers a way to\\npartition the regression function based on level sets of a defined function and\\nthat function's basins of attraction. this topologically-based piecewise\\nregression algorithm has shown promise in its initial applications, but the\\ncurrent implementation in the literature has been limited to elastic net and\\ngeneralized linear regression. it is possible that nonparametric methods, such\\nas random forest or conditional inference trees, may provide better prediction\\nand insight through modeling interaction terms and other nonlinear\\nrelationships between predictors and a given outcome.\\n  this study explores the use of several machine learning algorithms within a\\nmorse-smale piecewise regression framework, including boosted regression with\\nlinear baselearners, homotopy-based lasso, conditional inference trees, random\\nforest, and a wide neural network framework called extreme learning machines.\\nsimulations on tweedie regression problems with varying tweedie parameter and\\ndispersion suggest that many machine learning approaches to morse-smale\\npiecewise regression improve the original algorithm's performance, particularly\\nfor outcomes with lower dispersion and linear or a mix of linear and nonlinear\\npredictor relationships. on a real actuarial problem, several of these new\\nalgorithms perform as good as or better than the original morse-smale\\nregression algorithm, and most provide information on the nature of predictor\\nrelationships within each partition to provide insight into differences between\\ndataset partitions.\\n)\": False, 'contains(surrogate-assisted bayesian inversion for landscape and basin evolution\\n  models\\n  the complex and computationally expensive features of the forward landscape\\nand sedimentary basin evolution models pose a major challenge in the\\ndevelopment of efficient inference and optimization methods. bayesian inference\\nprovides a methodology for estimation and uncertainty quantification of free\\nmodel parameters. in our previous work, parallel tempering bayeslands was\\ndeveloped as a framework for parameter estimation and uncertainty\\nquantification for the landscape and basin evolution modelling software\\nbadlands. parallel tempering bayeslands features high-performance computing\\nwith dozens of processing cores running in parallel to enhance computational\\nefficiency. although parallel computing is used, the procedure remains\\ncomputationally challenging since thousands of samples need to be drawn and\\nevaluated. in large-scale landscape and basin evolution problems, a single\\nmodel evaluation can take from several minutes to hours, and in certain cases,\\neven days. surrogate-assisted optimization has been with successfully applied\\nto a number of engineering problems. this motivates its use in optimisation and\\ninference methods suited for complex models in geology and geophysics.\\nsurrogates can speed up parallel tempering bayeslands by developing\\ncomputationally inexpensive surrogates to mimic expensive models. in this\\npaper, we present an application of surrogate-assisted parallel tempering where\\nthat surrogate mimics a landscape evolution model including erosion, sediment\\ntransport and deposition, by estimating the likelihood function that is given\\nby the model. we employ a machine learning model as a surrogate that learns\\nfrom the samples generated by the parallel tempering algorithm. the results\\nshow that the methodology is effective in lowering the overall computational\\ncost significantly while retaining the quality of solutions.\\n)': False, 'contains(a computational framework for modelling and analyzing ice storms\\n  ice storms are extreme weather events that can have devastating implications\\nfor the sustainability of natural ecosystems as well as man made\\ninfrastructure. ice storms are caused by a complex mix of atmospheric\\nconditions and are among the least understood of severe weather events. our\\nability to model ice storms and characterize storm features will go a long way\\ntowards both enabling support systems that offset storm impacts and increasing\\nour understanding of ice storms. in this paper, we present a holistic\\ncomputational framework to answer key questions of interest about ice storms.\\nwe model ice storms as a function of relevant surface and atmospheric\\nvariables. we learn these models by adapting and applying supervised and\\nunsupervised machine learning algorithms on data with missing or incorrect\\nlabels. we also include a knowledge representation module that reasons with\\ndomain knowledge to revise the output of the learned models. our models are\\ntrained using reanalysis data and historical records of storm events. we\\nevaluate these models on reanalyis data as well as global climate model (gcm)\\ndata for historical and future climate change scenarios. furthermore, we\\ndiscuss the use of appropriate bias correction approaches to run such modeling\\nframeworks with gcm data.\\n)': False, 'contains(exemplar-based synthesis of geology using kernel discrepancies and\\n  generative neural networks\\n  we propose a framework for synthesis of geological images based on an\\nexemplar image. we synthesize new realizations such that the discrepancy in the\\npatch distribution between the realizations and the exemplar image is\\nminimized. such discrepancy is quantified using a kernel method for two-sample\\ntest called maximum mean discrepancy. to enable fast synthesis, we train a\\ngenerative neural network in an offline phase to sample realizations\\nefficiently during deployment, while also providing a parametrization of the\\nsynthesis process. we assess the framework on a classical binary image\\nrepresenting channelized subsurface reservoirs, finding that the method\\nreproduces the visual patterns and spatial statistics (image histogram and\\ntwo-point probability functions) of the exemplar image.\\n)': False, 'contains(short-term predictability of photovoltaic production over italy\\n  photovoltaic (pv) power production increased drastically in europe throughout\\nthe last years. about the 6% of electricity in italy comes from pv and for an\\nefficient management of the power grid an accurate and reliable forecasting of\\nproduction would be needed. starting from a dataset of electricity production\\nof 65 italian solar plants for the years 2011-2012 we investigate the\\npossibility to forecast daily production from one to ten days of lead time\\nwithout using on site measurements. our study is divided in two parts: an\\nassessment of the predictability of meteorological variables using weather\\nforecasts and an analysis on the application of data-driven modelling in\\npredicting solar power production. we calibrate a svm model using available\\nobservations and then we force the same model with the predicted variables from\\nweather forecasts with a lead time from one to ten days. as expected, solar\\npower production is strongly influenced by cloudiness and clear sky, in fact we\\nobserve that while during summer we obtain a general error under the 10%\\n(slightly lower in south italy), during winter the error is abundantly above\\nthe 20%.\\n)': False, 'contains(supervised classification for object identification in urban areas using\\n  satellite imagery\\n  this paper presents a useful method to achieve classification in satellite\\nimagery. the approach is based on pixel level study employing various features\\nsuch as correlation, homogeneity, energy and contrast. in this study gray-scale\\nimages are used for training the classification model. for supervised\\nclassification, two classification techniques are employed namely the support\\nvector machine (svm) and the naive bayes. with textural features used for\\ngray-scale images, naive bayes performs better with an overall accuracy of 76%\\ncompared to 68% achieved by svm. the computational time is evaluated while\\nperforming the experiment with two different window sizes i.e., 50x50 and\\n70x70. the required computational time on a single image is found to be 27\\nseconds for a window size of 70x70 and 45 seconds for a window size of 50x50.\\n)': False, 'contains(earthquake catalog-based machine learning identification of laboratory\\n  fault states and the effects of magnitude of completeness\\n  machine learning regression can predict macroscopic fault properties such as\\nshear stress, friction, and time to failure using continuous records of fault\\nzone acoustic emissions. here we show that a similar approach is successful\\nusing event catalogs derived from the continuous data. our methods are\\napplicable to catalogs of arbitrary scale and magnitude of completeness. we\\ninvestigate how machine learning regression from an event catalog of laboratory\\nearthquakes performs as a function of the catalog magnitude of completeness. we\\nfind that strong model performance requires a sufficiently low magnitude of\\ncompleteness, and below this magnitude of completeness model performance\\nsaturates.\\n)': False, 'contains(predicting aircraft trajectories: a deep generative convolutional\\n  recurrent neural networks approach\\n  reliable 4d aircraft trajectory prediction, whether in a real-time setting or\\nfor analysis of counterfactuals, is important to the efficiency of the aviation\\nsystem. toward this end, we first propose a highly generalizable efficient\\ntree-based matching algorithm to construct image-like feature maps from\\nhigh-fidelity meteorological datasets - wind, temperature and convective\\nweather. we then model the track points on trajectories as conditional gaussian\\nmixtures with parameters to be learned from our proposed deep generative model,\\nwhich is an end-to-end convolutional recurrent neural network that consists of\\na long short-term memory (lstm) encoder network and a mixture density lstm\\ndecoder network. the encoder network embeds last-filed flight plan information\\ninto fixed-size hidden state variables and feeds the decoder network, which\\nfurther learns the spatiotemporal correlations from the historical flight\\ntracks and outputs the parameters of gaussian mixtures. convolutional layers\\nare integrated into the pipeline to learn representations from the\\nhigh-dimension weather features. during the inference process, beam search,\\nadaptive kalman filter, and rauch-tung-striebel smoother algorithms are used to\\nprune the variance of generated trajectories.\\n)': False, 'contains(network of nano-landers for in-situ characterization of asteroid impact\\n  studies\\n  exploration of asteroids and comets can give insight into the origins of the\\nsolar system and can be instrumental in planetary defence and in-situ resource\\nutilization (isru). asteroids, due to their low gravity are a challenging\\ntarget for surface exploration. current missions envision performing\\ntouch-and-go operations over an asteroid surface. in this work, we analyse the\\nfeasibility of sending scores of nano-landers, each 1 kg in mass and volume of\\n1u, or 1000 cm3. these landers would hop, roll and fly over the asteroid\\nsurface. the landers would include science instruments such as stereo cameras,\\nhand-lens imagers and spectrometers to characterize rock composition. a network\\nof nano-landers situated on the surface of an asteroid can provide unique and\\nvery detailed measurements of a spacecraft impacting onto an asteroid surface.\\na full-scale, artificial impact experiment onto an asteroid can help\\ncharacterize its composition and geology and help in the development of\\nasteroid deflection techniques intended for planetary defence. scores of\\nnano-landers could provide multiple complementary views of the impact,\\nresultant seismic activity and trajectory of the ejecta. the nano-landers can\\nanalyse the pristine, unearthed regolith shielded from effects of uv and cosmic\\nrays and that may be millions of years old. our approach to formulating this\\nmission concepts utilizes automated machine learning techniques in the planning\\nand design of space systems. we use a form of darwinian selection to select and\\nidentify suitable number of nano-landers, the on-board instruments and control\\nsystem to explore and navigate the asteroid environment. scenarios are\\ngenerated in simulation and evaluated against quantifiable mission goals such\\nas area explored on the asteroid and amount of data recorded from the impact\\nevent.\\n)': False, 'contains(eigenvalues of random graphs with cycles\\n  networks and graphs are often studied using the eigenvalues of their\\nadjacency matrix, an powerful mathematical tool with applications on fields as\\ndiverse as systems engineering, ecology, machine learning and neuroscience. as\\nin those applications the exact graph structure is not known, researchers\\nresort to random graphs to obtain eigenvalue properties from known structural\\nfeatures. however, this theory is not intuitive and only few results are known.\\nin this paper we tackle this problem through the cycles in a graph. we start by\\nderiving a simple relation between eigenvalues and cycle weights and we apply\\nit to two structural features: the spectral radius of circulant graphs and the\\neigenvalue distribution of random graphs with motifs. during this study we\\nempirically uncover to surprising phenomena: first, circulant directed networks\\nhave eigenvalues distributed in concentric circles around the origin. second,\\nthe eigenvalues of a network with abundance of short cycles are confined to the\\ninterior of a $\\\\tau$-ellipse --an ellipse with $\\\\tau$ foci-- in the complex\\nplane, where $\\\\tau$ is the length of the cycles. our approach offers an\\nintuitive way to study eigenvalues on graphs and in doing so reveals surprising\\nconnections between random matrix theory and planar geometry.\\n)': False, 'contains(are adversarial perturbations a showstopper for ml-based cad? a case\\n  study on cnn-based lithographic hotspot detection\\n  there is substantial interest in the use of machine learning (ml) based\\ntechniques throughout the electronic computer-aided design (cad) flow,\\nparticularly those based on deep learning. however, while deep learning methods\\nhave surpassed state-of-the-art performance in several applications, they have\\nexhibited intrinsic susceptibility to adversarial perturbations --- small but\\ndeliberate alterations to the input of a neural network, precipitating\\nincorrect predictions. in this paper, we seek to investigate whether\\nadversarial perturbations pose risks to ml-based cad tools, and if so, how\\nthese risks can be mitigated. to this end, we use a motivating case study of\\nlithographic hotspot detection, for which convolutional neural networks (cnn)\\nhave shown great promise. in this context, we show the first adversarial\\nperturbation attacks on state-of-the-art cnn-based hotspot detectors;\\nspecifically, we show that small (on average 0.5% modified area), functionality\\npreserving and design-constraint satisfying changes to a layout can nonetheless\\ntrick a cnn-based hotspot detector into predicting the modified layout as\\nhotspot free (with up to 99.7% success). we propose an adversarial retraining\\nstrategy to improve the robustness of cnn-based hotspot detection and show that\\nthis strategy significantly improves robustness (by a factor of ~3) against\\nadversarial attacks without compromising classification accuracy.\\n)': False, \"contains(using phone sensors and an artificial neural network to detect gait\\n  changes during drinking episodes in the natural environment\\n  phone sensors could be useful in assessing changes in gait that occur with\\nalcohol consumption. this study determined (1) feasibility of collecting\\ngait-related data during drinking occasions in the natural environment, and (2)\\nhow gait-related features measured by phone sensors relate to estimated blood\\nalcohol concentration (ebac). ten young adult heavy drinkers were prompted to\\ncomplete a 5-step gait task every hour from 8pm to 12am over four consecutive\\nweekends. we collected 3-xis accelerometer, gyroscope, and magnetometer data\\nfrom phone sensors, and computed 24 gait-related features using a sliding\\nwindow technique. ebac levels were calculated at each time point based on\\necological momentary assessment (ema) of alcohol use. we used an artificial\\nneural network model to analyze associations between sensor features and ebacs\\nin training (70% of the data) and validation and test (30% of the data)\\ndatasets. we analyzed 128 data points where both ebac and gait-related sensor\\ndata was captured, either when not drinking (n=60), while ebac was ascending\\n(n=55) or ebac was descending (n=13). 21 data points were captured at times\\nwhen the ebac was greater than the legal limit (0.08 mg/dl). using a bayesian\\nregularized neural network, gait-related phone sensor features showed a high\\ncorrelation with ebac (pearson's r > 0.9), and >95% of estimated ebac would\\nfall between -0.012 and +0.012 of actual ebac. it is feasible to collect\\ngait-related data from smartphone sensors during drinking occasions in the\\nnatural environment. sensor-based features can be used to infer gait changes\\nassociated with elevated blood alcohol content.\\n)\": False, 'contains(integration of lidar and multispectral images for exposure and\\n  earthquake vulnerability estimation. application in lorca, spain\\n  we present a procedure for assessing the urban exposure and seismic\\nvulnerability that integrates lidar data with aerial and satellite images. it\\ncomprises three phases: first, we segment the satellite image to divide the\\nstudy area into different urban patterns. second, we extract building\\nfootprints and attributes that represent the type of building of each urban\\npattern. finally, we assign the seismic vulnerability to each building using\\ndifferent machine-learning techniques: decision trees, svm, logistic regression\\nand bayesian networks. we apply the procedure to 826 buildings in the city of\\nlorca (se spain), where we count on a vulnerability database that we use as\\nground truth for the validation of results. the outcomes show that the machine\\nlearning techniques have similar performance, yielding vulnerability\\nclassification results with an accuracy of 77% - 80% (f1-score). the procedure\\nis scalable and can be replicated in different areas. it is especially\\ninteresting as a complement to conventional data gathering approaches for\\ndisaster risk applications in areas where field surveys need to be restricted\\nto certain areas, dates or budget. keywords lidar, satellite image, orthophoto,\\nimage segmentation, machine learning, earthquake vulnerability.\\n)': False, 'contains(random mesh projectors for inverse problems\\n  we propose a new learning-based approach to solve ill-posed inverse problems\\nin imaging. we address the case where ground truth training samples are rare\\nand the problem is severely ill-posed - both because of the underlying physics\\nand because we can only get few measurements. this setting is common in\\ngeophysical imaging and remote sensing. we show that in this case the common\\napproach to directly learn the mapping from the measured data to the\\nreconstruction becomes unstable. instead, we propose to first learn an ensemble\\nof simpler mappings from the data to projections of the unknown image into\\nrandom piecewise-constant subspaces. we then combine the projections to form a\\nfinal reconstruction by solving a deconvolution-like problem. we show\\nexperimentally that the proposed method is more robust to measurement noise and\\ncorruptions not seen during training than a directly learned inverse.\\n)': False, \"contains(optimal transport for multi-source domain adaptation under target shift\\n  in this paper, we propose to tackle the problem of reducing discrepancies\\nbetween multiple domains referred to as multi-source domain adaptation and\\nconsider it under the target shift assumption: in all domains we aim to solve a\\nclassification problem with the same output classes, but with labels'\\nproportions differing across them. this problem, generally ignored in the vast\\nmajority papers on domain adaptation papers, is nevertheless critical in\\nreal-world applications, and we theoretically show its impact on the adaptation\\nsuccess. to address this issue, we design a method based on optimal transport,\\na theory that has been successfully used to tackle adaptation problems in\\nmachine learning. our method performs multi-source adaptation and target shift\\ncorrection simultaneously by learning the class probabilities of the unlabeled\\ntarget sample and the coupling allowing to align two (or more) probability\\ndistributions. experiments on both synthetic and real-world data related to\\nsatellite image segmentation task show the superiority of the proposed method\\nover the state-of-the-art.\\n)\": False, \"contains(an unsupervised clustering-based short-term solar forecasting\\n  methodology using multi-model machine learning blending\\n  solar forecasting accuracy is affected by weather conditions, and weather\\nawareness forecasting models are expected to improve the performance. however,\\nit may not be available and reliable to classify different forecasting tasks by\\nusing only meteorological weather categorization. in this paper, an\\nunsupervised clustering-based (uc-based) solar forecasting methodology is\\ndeveloped for short-term (1-hour-ahead) global horizontal irradiance (ghi)\\nforecasting. this methodology consists of three parts: ghi time series\\nunsupervised clustering, pattern recognition, and uc-based forecasting. the\\ndaily ghi time series is first clustered by an optimized cross-validated\\nclustering (occur) method, which determines the optimal number of clusters and\\nbest clustering results. then, support vector machine pattern recognition\\n(svm-pr) is adopted to recognize the category of a certain day using the first\\nfew hours' data in the forecasting stage. ghi forecasts are generated by the\\nmost suitable models in different clusters, which are built by a two-layer\\nmachine learning based multi-model (m3) forecasting framework. the developed\\nuc-based methodology is validated by using 1-year of data with six solar\\nfeatures. numerical results show that (i) uc-based models outperform non-uc\\n(all-in-one) models with the same m3 architecture by approximately 20%; (ii)\\nm3-based models also outperform the single-algorithm machine learning (saml)\\nmodels by approximately 20%.\\n)\": False, 'contains(geometric methods for robust data analysis in high dimension\\n  machine learning and data analysis now finds both scientific and industrial\\napplication in biology, chemistry, geology, medicine, and physics. these\\napplications rely on large quantities of data gathered from automated sensors\\nand user input. furthermore, the dimensionality of many datasets is extreme:\\nmore details are being gathered about single user interactions or sensor\\nreadings. all of these applications encounter problems with a common theme: use\\nobserved data to make inferences about the world. our work obtains the first\\nprovably efficient algorithms for independent component analysis (ica) in the\\npresence of heavy-tailed data. the main tool in this result is the centroid\\nbody (a well-known topic in convex geometry), along with optimization and\\nrandom walks for sampling from a convex body. this is the first algorithmic use\\nof the centroid body and it is of independent theoretical interest, since it\\neffectively replaces the estimation of covariance from samples, and is more\\ngenerally accessible.\\n  this reduction relies on a non-linear transformation of samples from such an\\nintersection of halfspaces (i.e. a simplex) to samples which are approximately\\nfrom a linearly transformed product distribution. through this transformation\\nof samples, which can be done efficiently, one can then use an ica algorithm to\\nrecover the vertices of the intersection of halfspaces.\\n  finally, we again use ica as an algorithmic primitive to construct an\\nefficient solution to the widely-studied problem of learning the parameters of\\na gaussian mixture model. our algorithm again transforms samples from a\\ngaussian mixture model into samples which fit into the ica model and, when\\nprocessed by an ica algorithm, result in recovery of the mixture parameters.\\nour algorithm is effective even when the number of gaussians in the mixture\\ngrows polynomially with the ambient dimension\\n)': False, 'contains(towards global remote discharge estimation: using the few to estimate\\n  the many\\n  learning hydrologic models for accurate riverine flood prediction at scale is\\na challenge of great importance. one of the key difficulties is the need to\\nrely on in-situ river discharge measurements, which can be quite scarce and\\nunreliable, particularly in regions where floods cause the most damage every\\nyear. accordingly, in this work we tackle the problem of river discharge\\nestimation at different river locations. a core characteristic of the data at\\nhand (e.g. satellite measurements) is that we have few measurements for many\\nlocations, all sharing the same physics that underlie the water discharge. we\\ncapture this scenario in a simple but powerful common mechanism regression\\n(cmr) model with a local component as well as a shared one which captures the\\nglobal discharge mechanism. the resulting learning objective is non-convex, but\\nwe show that we can find its global optimum by leveraging the power of joining\\nlocal measurements across sites. in particular, using a spectral initialization\\nwith provable near-optimal accuracy, we can find the optimum using standard\\ndescent methods. we demonstrate the efficacy of our approach for the problem of\\ndischarge estimation using simulations.\\n)': False, 'contains(recovering thermodynamics from spectral profiles observed by iris: a\\n  machine and deep learning approach\\n  inversion codes allow reconstructing a model atmosphere from observations.\\nwith the inclusion of optically thick lines that form in the solar\\nchromosphere, such modelling is computationally very expensive because a\\nnon-lte evaluation of the radiation field is required. in this study, we\\ncombine the results provided by these traditional methods with machine and deep\\nlearning techniques to obtain similar-quality results in an easy-touse, much\\nfaster way. we have applied these new methods to mg ii h&k lines observed by\\niris. as a result, we are able to reconstruct the thermodynamic state\\n(temperature, line-of-sight velocity, non-thermal velocities, electron density,\\netc.) in the chromosphere and upper photosphere of an area equivalent to an\\nactive region in a few cpu minutes, speeding up the process by a factor of\\n$10^5$-$10^6$. the open-source code accompanying this paper will allow the\\ncommunity to use iris observations to open a new window to a host of solar\\nphenomena.\\n)': False, \"contains(improving orbit prediction accuracy through supervised machine learning\\n  due to the lack of information such as the space environment condition and\\nresident space objects' (rsos') body characteristics, current orbit predictions\\nthat are solely grounded on physics-based models may fail to achieve required\\naccuracy for collision avoidance and have led to satellite collisions already.\\nthis paper presents a methodology to predict rsos' trajectories with higher\\naccuracy than that of the current methods. inspired by the machine learning\\n(ml) theory through which the models are learned based on large amounts of\\nobserved data and the prediction is conducted without explicitly modeling space\\nobjects and space environment, the proposed ml approach integrates\\nphysics-based orbit prediction algorithms with a learning-based process that\\nfocuses on reducing the prediction errors. using a simulation-based space\\ncatalog environment as the test bed, the paper demonstrates three types of\\ngeneralization capability for the proposed ml approach: 1) the ml model can be\\nused to improve the same rso's orbit information that is not available during\\nthe learning process but shares the same time interval as the training data; 2)\\nthe ml model can be used to improve predictions of the same rso at future\\nepochs; and 3) the ml model based on a rso can be applied to other rsos that\\nshare some common features.\\n)\": False, 'contains(systems of global governance in the era of human-machine convergence\\n  technology is increasingly shaping our social structures and is becoming a\\ndriving force in altering human biology. besides, human activities already\\nproved to have a significant impact on the earth system which in turn generates\\ncomplex feedback loops between social and ecological systems. furthermore,\\nsince our species evolved relatively fast from small groups of hunter-gatherers\\nto large and technology-intensive urban agglomerations, it is not a surprise\\nthat the major institutions of human society are no longer fit to cope with the\\npresent complexity. in this note we draw foundational parallelisms between\\nneurophysiological systems and ict-enabled social systems, discussing how\\nframeworks rooted in biology and physics could provide heuristic value in the\\ndesign of evolutionary systems relevant to politics and economics. in this\\nregard we highlight how the governance of emerging technology (i.e.\\nnanotechnology, biotechnology, information technology, and cognitive science),\\nand the one of climate change both presently confront us with a number of\\nconnected challenges. in particular: historically high level of inequality; the\\nco-existence of growing multipolar cultural systems in an unprecedentedly\\nconnected world; the unlikely reaching of the institutional agreements required\\nto deviate abnormal trajectories of development. we argue that wise general\\nsolutions to such interrelated issues should embed the deep understanding of\\nhow to elicit mutual incentives in the socio-economic subsystems of earth\\nsystem in order to jointly concur to a global utility function (e.g. avoiding\\nthe reach of planetary boundaries and widespread social unrest). we leave some\\nopen questions on how techno-social systems can effectively learn and adapt\\nwith respect to our understanding of geopolitical complexity.\\n)': False, 'contains(parametric generation of conditional geological realizations using\\n  generative neural networks\\n  deep learning techniques are increasingly being considered for geological\\napplications where -- much like in computer vision -- the challenges are\\ncharacterized by high-dimensional spatial data dominated by multipoint\\nstatistics. in particular, a novel technique called generative adversarial\\nnetworks has been recently studied for geological parametrization and\\nsynthesis, obtaining very impressive results that are at least qualitatively\\ncompetitive with previous methods. the method obtains a neural network\\nparametrization of the geology -- so-called a generator -- that is capable of\\nreproducing very complex geological patterns with dimensionality reduction of\\nseveral orders of magnitude. subsequent works have addressed the conditioning\\ntask, i.e. using the generator to generate realizations honoring spatial\\nobservations (hard data). the current approaches, however, do not provide a\\nparametrization of the conditional generation process. in this work, we propose\\na method to obtain a parametrization for direct generation of conditional\\nrealizations. the main idea is to simply extend the existing generator network\\nby stacking a second inference network that learns to perform the conditioning.\\nthis inference network is a neural network trained to sample a posterior\\ndistribution derived using a bayesian formulation of the conditioning task. the\\nresulting extended neural network thus provides the conditional\\nparametrization. our method is assessed on a benchmark image of binary\\nchannelized subsurface, obtaining very promising results for a wide variety of\\nconditioning configurations.\\n)': False, 'contains(generating a training dataset for land cover classification to advance\\n  global development\\n  semantic segmentation of land cover classes is fundamental for agricultural\\nand economic development work, from sustainable forestry to urban planning, yet\\nexisting training datasets have significant limitations. to generate an open\\nand comprehensive training library of high resolution earth imagery and high\\nquality land cover classifications, public sentinel-2 data at 10 m spatial\\nresolution was matched with accurate globeland30 labels from 2010, which were\\nfiltered by agreement with an intermediary sentinel-2 classification at 20 m\\nproduced during atmospheric correction. scene-level classifications were\\npredicted by random forests trained on valid reflectance data and the filtered\\nlabels, and achieved over 80% model accuracy for a variety of locations.\\nfurther work is required to aggregate individual scene classifications for\\nannual labels and to test the approach in more locations, before crowdsourcing\\nhuman validation. the goal is to create a sustained community-wide effort to\\ngenerate image labels not only for land cover, but also very specific images\\nfor major agriculture crops across the world and other thematic categories of\\ninterest to the global development community.\\n)': False, 'contains(a machine learning approach for underwater gas leakage detection\\n  underwater gas reservoirs are used in many situations. in particular, carbon\\ncapture and storage (ccs) facilities that are currently being developed intend\\nto store greenhouse gases inside geological formations in the deep sea. in\\nthese formations, however, the gas might percolate, leaking back to the water\\nand eventually to the atmosphere. the early detection of such leaks is\\ntherefore tantamount to any underwater ccs project. in this work, we propose to\\nuse passive acoustic monitoring (pam) and a machine learning approach to design\\nefficient detectors that can signal the presence of a leakage. we use data\\nobtained from simulation experiments off the brazilian shore, and show that the\\ndetection based on classification algorithms achieve good performance. we also\\npropose a smoothing strategy based on hidden markov models in order to\\nincorporate previous knowledge about the probabilities of leakage occurrences.\\n)': False, 'contains(remote sensing image classification with large scale gaussian processes\\n  current remote sensing image classification problems have to deal with an\\nunprecedented amount of heterogeneous and complex data sources. upcoming\\nmissions will soon provide large data streams that will make land cover/use\\nclassification difficult. machine learning classifiers can help at this, and\\nmany methods are currently available. a popular kernel classifier is the\\ngaussian process classifier (gpc), since it approaches the classification\\nproblem with a solid probabilistic treatment, thus yielding confidence\\nintervals for the predictions as well as very competitive results to\\nstate-of-the-art neural networks and support vector machines. however, its\\ncomputational cost is prohibitive for large scale applications, and constitutes\\nthe main obstacle precluding wide adoption. this paper tackles this problem by\\nintroducing two novel efficient methodologies for gaussian process (gp)\\nclassification. we first include the standard random fourier features\\napproximation into gpc, which largely decreases its computational cost and\\npermits large scale remote sensing image classification. in addition, we\\npropose a model which avoids randomly sampling a number of fourier frequencies,\\nand alternatively learns the optimal ones within a variational bayes approach.\\nthe performance of the proposed methods is illustrated in complex problems of\\ncloud detection from multispectral imagery and infrared sounding data.\\nexcellent empirical results support the proposal in both computational cost and\\naccuracy.\\n)': False, 'contains(prediction of soil moisture content based on satellite data and\\n  sequence-to-sequence networks\\n  the main objective of this study is to combine remote sensing and machine\\nlearning to detect soil moisture content. growing population and food\\nconsumption has led to the need to improve agricultural yield and to reduce\\nwastage of natural resources. in this paper, we propose a neural network\\narchitecture, based on recent work by the research community, that can make a\\nstrong social impact and aid united nations sustainable development goal of\\nzero hunger. the main aims here are to: improve efficiency of water usage;\\nreduce dependence on irrigation; increase overall crop yield; minimise risk of\\ncrop loss due to drought and extreme weather conditions. we achieve this by\\napplying satellite imagery, crop segmentation, soil classification and ndvi and\\nsoil moisture prediction on satellite data, ground truth and climate data\\nrecords. by applying machine learning to sensor data and ground data, farm\\nmanagement systems can evolve into a real time ai enabled platform that can\\nprovide actionable recommendations and decision support tools to the farmers.\\n)': False, 'contains(a new spectral clustering algorithm\\n  we present a new clustering algorithm that is based on searching for natural\\ngaps in the components of the lowest energy eigenvectors of the laplacian of a\\ngraph. in comparing the performance of the proposed method with a set of other\\npopular methods (kmeans, spectral-kmeans, and an agglomerative method) in the\\ncontext of the lancichinetti-fortunato-radicchi (lfr) benchmark for undirected\\nweighted overlapping networks, we find that the new method outperforms the\\nother spectral methods considered in certain parameter regimes. finally, in an\\napplication to climate data involving one of the most important modes of\\ninterannual climate variability, the el nino southern oscillation phenomenon,\\nwe demonstrate the ability of the new algorithm to readily identify different\\nflavors of the phenomenon.\\n)': False, \"contains(ensemble model aggregation using a computationally lightweight\\n  machine-learning model to forecast ocean waves\\n  this study investigated an approach to improve the accuracy of\\ncomputationally lightweight surrogate models by updating forecasts based on\\nhistorical accuracy relative to sparse observation data. using a lightweight,\\nocean-wave forecasting model, we created a large number of model ensembles,\\nwith perturbed inputs, for a two-year study period. forecasts were aggregated\\nusing a machine-learning algorithm that combined forecasts from multiple,\\nindependent models into a single 'best-estimate' prediction of the true state,\\nbased on historical performance relative to observations. the framework was\\napplied to a case-study site in monterey bay, california.\\na~learning-aggregation technique used historical observations and model\\nforecasts to calculate a weight for each ensemble member. weighted ensemble\\npredictions were compared to measured wave conditions to evaluate performance\\nagainst present state-of-the-art. finally, we discuss how this framework, which\\nintegrates ensemble aggregations and surrogate models, can be used to improve\\nforecasting systems and scientific process studies.\\n)\": False, 'contains(exploratory data analysis of the kelvinhelmholtz instability in jets\\n  the kelvinhelmholtz (kh) instability is a fundamental wave instability that\\nis frequently observed in all kinds of shear layer (jets, wakes, atmospheric\\nair currents etc). the study of kh-instability, coherent flow structures has a\\nmajor impact in understanding the fundamentals of fluid dynamics. therefore\\nthere is a need for methods that can identify and analyse these structures. in\\nthis final assignment, we use machine-learning methods such as proper\\northogonal decomposition (pod) and dynamic mode decomposition (dmd) to analyse\\nthe coherent flow structures. we used a 2d co-axial jet as our data, with\\nreynolds number corresponding to re: 10,000. results for pod modes and dmd\\nmodes are discussed and compared.\\n)': False, 'contains(semi-supervised multitask learning on multispectral satellite images\\n  using wasserstein generative adversarial networks (gans) for predicting\\n  poverty\\n  obtaining reliable data describing local poverty metrics at a granularity\\nthat is informative to policy-makers requires expensive and logistically\\ndifficult surveys, particularly in the developing world. not surprisingly, the\\npoverty stricken regions are also the ones which have a high probability of\\nbeing a war zone, have poor infrastructure and sometimes have governments that\\ndo not cooperate with internationally funded development efforts. we train a\\ncnn on free and publicly available daytime satellite images of the african\\ncontinent from landsat 7 to build a model for predicting local economic\\nlivelihoods. only 5% of the satellite images can be associated with labels\\n(which are obtained from dhs surveys) and thus a semi-supervised approach using\\na gan (similar to the approach of salimans, et al. (2016)), albeit with a more\\nstable-to-train flavor of gans called the wasserstein gan regularized with\\ngradient penalty(gulrajani, et al. (2017)) is used. the method of multitask\\nlearning is employed to regularize the network and also create an end-to-end\\nmodel for the prediction of multiple poverty metrics.\\n)': False, 'contains(random forest ensemble of support vector regression models for solar\\n  power forecasting\\n  to mitigate the uncertainty of variable renewable resources, two\\noff-the-shelf machine learning tools are deployed to forecast the solar power\\noutput of a solar photovoltaic system. the support vector machines generate the\\nforecasts and the random forest acts as an ensemble learning method to combine\\nthe forecasts. the common ensemble technique in wind and solar power\\nforecasting is the blending of meteorological data from several sources. in\\nthis study though, the present and the past solar power forecasts from several\\nmodels, as well as the associated meteorological data, are incorporated into\\nthe random forest to combine and improve the accuracy of the day-ahead solar\\npower forecasts. the performance of the combined model is evaluated over the\\nentire year and compared with other combining techniques.\\n)': False, \"contains(satellite image-based localization via learned embeddings\\n  we propose a vision-based method that localizes a ground vehicle using\\npublicly available satellite imagery as the only prior knowledge of the\\nenvironment. our approach takes as input a sequence of ground-level images\\nacquired by the vehicle as it navigates, and outputs an estimate of the\\nvehicle's pose relative to a georeferenced satellite image. we overcome the\\nsignificant viewpoint and appearance variations between the images through a\\nneural multi-view model that learns location-discriminative embeddings in which\\nground-level images are matched with their corresponding satellite view of the\\nscene. we use this learned function as an observation model in a filtering\\nframework to maintain a distribution over the vehicle's pose. we evaluate our\\nmethod on different benchmark datasets and demonstrate its ability localize\\nground-level images in environments novel relative to training, despite the\\nchallenges of significant viewpoint and appearance variations.\\n)\": False, 'contains(shallow learning for fluid flow reconstruction with limited sensors and\\n  limited data\\n  in many applications, it is important to reconstruct a fluid flow field, or\\nsome other high-dimensional state, from limited measurements and limited data.\\nin this work, we propose a shallow neural network-based learning methodology\\nfor such fluid flow reconstruction. our approach learns an end-to-end mapping\\nbetween the sensor measurements and the high-dimensional fluid flow field,\\nwithout any heavy preprocessing on the raw data. no prior knowledge is assumed\\nto be available, and the estimation method is purely data-driven. we\\ndemonstrate the performance on three examples in fluid mechanics and\\noceanography, showing that this modern data-driven approach outperforms\\ntraditional modal approximation techniques which are commonly used for flow\\nreconstruction. not only does the proposed method show superior performance\\ncharacteristics, it can also produce a comparable level of performance with\\ntraditional methods in the area, using significantly fewer sensors. thus, the\\nmathematical architecture is ideal for emerging global monitoring technologies\\nwhere measurement data are often limited.\\n)': False, 'contains(open-loop tomography with artificial neural networks on canary: on-sky\\n  results\\n  we present recent results from the initial testing of an artificial neural\\nnetwork (ann) based tomographic reconstructor complex atmospheric reconstructor\\nbased on machine learning (carmen) on canary, an adaptive optics demonstrator\\noperated on the 4.2m william herschel telescope, la palma. the reconstructor\\nwas compared with contemporaneous data using the learn and apply (l&a)\\ntomographic reconstructor. we find that the fully optimised l&a tomographic\\nreconstructor outperforms carmen by approximately 5% in strehl ratio or 15nm\\nrms in wavefront error. we also present results for canary in ground layer\\nadaptive optics mode to show that the reconstructors are tomographic. the\\nresults are comparable and this small deficit is attributed to limitations in\\nthe training data used to build the ann. laboratory bench tests show that the\\nann can out perform l&a under certain conditions, e.g. if the higher layer of a\\nmodel two layer atmosphere was to change in altitude by ~300~m (equivalent to a\\nshift of approximately one tenth of a subaperture).\\n)': False, 'contains(optimizing deep video representation to match brain activity\\n  the comparison of observed brain activity with the statistics generated by\\nartificial intelligence systems is useful to probe brain functional\\norganization under ecological conditions. here we study fmri activity in ten\\nsubjects watching color natural movies and compute deep representations of\\nthese movies with an architecture that relies on optical flow and image\\ncontent. the association of activity in visual areas with the different layers\\nof the deep architecture displays complexity-related contrasts across visual\\nareas and reveals a striking foveal/peripheral dichotomy.\\n)': False, 'contains(spacenet: a remote sensing dataset and challenge series\\n  foundational mapping remains a challenge in many parts of the world,\\nparticularly in dynamic scenarios such as natural disasters when timely updates\\nare critical. updating maps is currently a highly manual process requiring a\\nlarge number of human labelers to either create features or rigorously validate\\nautomated outputs. we propose that the frequent revisits of earth imaging\\nsatellite constellations may accelerate existing efforts to quickly update\\nfoundational maps when combined with advanced machine learning techniques.\\naccordingly, the spacenet partners (cosmiq works, radiant solutions, and\\nnvidia), released a large corpus of labeled satellite imagery on amazon web\\nservices (aws) called spacenet. the spacenet partners also launched a series of\\npublic prize competitions to encourage improvement of remote sensing machine\\nlearning algorithms. the first two of these competitions focused on automated\\nbuilding footprint extraction, and the most recent challenge focused on road\\nnetwork extraction. in this paper we discuss the spacenet imagery, labels,\\nevaluation metrics, prize challenge results to date, and future plans for the\\nspacenet challenge series.\\n)': False, 'contains(landslide geohazard assessment with convolutional neural networks using\\n  sentinel-2 imagery data\\n  in this paper, the authors aim to combine the latest state of the art models\\nin image recognition with the best publicly available satellite images to\\ncreate a system for landslide risk mitigation. we focus first on landslide\\ndetection and further propose a similar system to be used for prediction. such\\nmodels are valuable as they could easily be scaled up to provide data for\\nhazard evaluation, as satellite imagery becomes increasingly available. the\\ngoal is to use satellite images and correlated data to enrich the public\\nrepository of data and guide disaster relief efforts for locating precise areas\\nwhere landslides have occurred. different image augmentation methods are used\\nto increase diversity in the chosen dataset and create more robust\\nclassification. the resulting outputs are then fed into variants of 3-d\\nconvolutional neural networks. a review of the current literature indicates\\nthere is no research using cnns (convolutional neural networks) and freely\\navailable satellite imagery for classifying landslide risk. the model has shown\\nto be ultimately able to achieve a significantly better than baseline accuracy.\\n)': False, 'contains(countdown regression: sharp and calibrated survival predictions\\n  probabilistic survival predictions from models trained with maximum\\nlikelihood estimation (mle) can have high, and sometimes unacceptably high\\nvariance. the field of meteorology, where the paradigm of maximizing sharpness\\nsubject to calibration is popular, has addressed this problem by using scoring\\nrules beyond mle, such as the continuous ranked probability score (crps). in\\nthis paper we present the \\\\emph{survival-crps}, a generalization of the crps to\\nthe survival prediction setting, with right-censored and interval-censored\\nvariants. we evaluate our ideas on the mortality prediction task using two\\ndifferent electronic health record (ehr) data sets (starr and mimic-iii)\\ncovering millions of patients, with suitable deep neural network architectures:\\na recurrent neural network (rnn) for starr and a fully connected network (fcn)\\nfor mimic-iii. we compare results between the two scoring rules while keeping\\nthe network architecture and data fixed, and show that models trained with\\nsurvival-crps result in sharper predictive distributions compared to those\\ntrained by mle, while still maintaining calibration.\\n)': False, 'contains(semi-supervised learning for structured regression on partially observed\\n  attributed graphs\\n  conditional probabilistic graphical models provide a powerful framework for\\nstructured regression in spatio-temporal datasets with complex correlation\\npatterns. however, in real-life applications a large fraction of observations\\nis often missing, which can severely limit the representational power of these\\nmodels. in this paper we propose a marginalized gaussian conditional random\\nfields (m-gcrf) structured regression model for dealing with missing labels in\\npartially observed temporal attributed graphs. this method is aimed at learning\\nwith both labeled and unlabeled parts and effectively predicting future values\\nin a graph. the method is even capable of learning from nodes for which the\\nresponse variable is never observed in history, which poses problems for many\\nstate-of-the-art models that can handle missing data. the proposed model is\\ncharacterized for various missingness mechanisms on 500 synthetic graphs. the\\nbenefits of the new method are also demonstrated on a challenging application\\nfor predicting precipitation based on partial observations of climate variables\\nin a temporal graph that spans the entire continental us. we also show that the\\nmethod can be useful for optimizing the costs of data collection in climate\\napplications via active reduction of the number of weather stations to\\nconsider. in experiments on these real-world and synthetic datasets we show\\nthat the proposed model is consistently more accurate than alternative\\nsemi-supervised structured models, as well as models that either use imputation\\nto deal with missing values or simply ignore them altogether.\\n)': False, \"contains(the first comparison between swarm-c accelerometer-derived thermospheric\\n  densities and physical and empirical model estimates\\n  the first systematic comparison between swarm-c accelerometer-derived\\nthermospheric density and both empirical and physics-based model results using\\nmultiple model performance metrics is presented. this comparison is performed\\nat the satellite's high temporal 10-s resolution, which provides a meaningful\\nevaluation of the models' fidelity for orbit prediction and other space weather\\nforecasting applications. the comparison against the physical model is\\ninfluenced by the specification of the lower atmospheric forcing, the\\nhigh-latitude ionospheric plasma convection, and solar activity. some insights\\ninto the model response to thermosphere-driving mechanisms are obtained through\\na machine learning exercise. the results of this analysis show that the\\nshort-timescale variations observed by swarm-c during periods of high solar and\\ngeomagnetic activity were better captured by the physics-based model than the\\nempirical models. it is concluded that swarm-c data agree well with the\\nclimatologies inherent within the models and are, therefore, a useful data set\\nfor further model validation and scientific research.\\n)\": False, 'contains(deep multi-species embedding\\n  understanding how species are distributed across landscapes over time is a\\nfundamental question in biodiversity research. unfortunately, most species\\ndistribution models only target a single species at a time, despite strong\\necological evidence that species are not independently distributed. we propose\\ndeep multi-species embedding (dmse), which jointly embeds vectors corresponding\\nto multiple species as well as vectors representing environmental covariates\\ninto a common high-dimensional feature space via a deep neural network. applied\\nto bird observational data from the citizen science project \\\\textit{ebird}, we\\ndemonstrate how the dmse model discovers inter-species relationships to\\noutperform single-species distribution models (random forests and svms) as well\\nas competing multi-label models. additionally, we demonstrate the benefit of\\nusing a deep neural network to extract features within the embedding and show\\nhow they improve the predictive performance of species distribution modelling.\\nan important domain contribution of the dmse model is the ability to discover\\nand describe species interactions while simultaneously learning the shared\\nhabitat preferences among species. as an additional contribution, we provide a\\ngraphical embedding of hundreds of bird species in the northeast us.\\n)': False, 'contains(a multi-variable stacked long-short term memory network for wind speed\\n  forecasting\\n  precisely forecasting wind speed is essential for wind power producers and\\ngrid operators. however, this task is challenging due to the stochasticity of\\nwind speed. to accurately predict short-term wind speed under uncertainties,\\nthis paper proposed a multi-variable stacked lstms model (mslstm). the proposed\\nmethod utilizes multiple historical meteorological variables, such as wind\\nspeed, temperature, humidity, pressure, dew point and solar radiation to\\naccurately predict wind speeds. the prediction performance is extensively\\nassessed using real data collected in west texas, usa. the experimental results\\nshow that the proposed mslstm can preferably capture and learn uncertainties\\nwhile output competitive performance.\\n)': False, 'contains(optimal clustering framework for hyperspectral band selection\\n  band selection, by choosing a set of representative bands in hyperspectral\\nimage (hsi), is an effective method to reduce the redundant information without\\ncompromising the original contents. recently, various unsupervised band\\nselection methods have been proposed, but most of them are based on\\napproximation algorithms which can only obtain suboptimal solutions toward a\\nspecific objective function. this paper focuses on clustering-based band\\nselection, and proposes a new framework to solve the above dilemma, claiming\\nthe following contributions: 1) an optimal clustering framework (ocf), which\\ncan obtain the optimal clustering result for a particular form of objective\\nfunction under a reasonable constraint. 2) a rank on clusters strategy (rcs),\\nwhich provides an effective criterion to select bands on existing clustering\\nstructure. 3) an automatic method to determine the number of the required\\nbands, which can better evaluate the distinctive information produced by\\ncertain number of bands. in experiments, the proposed algorithm is compared to\\nsome state-of-the-art competitors. according to the experimental results, the\\nproposed algorithm is robust and significantly outperform the other methods on\\nvarious data sets.\\n)': False, 'contains(prolongation of smap to spatio-temporally seamless coverage of\\n  continental us using a deep learning neural network\\n  the soil moisture active passive (smap) mission has delivered valuable\\nsensing of surface soil moisture since 2015. however, it has a short time span\\nand irregular revisit schedule. utilizing a state-of-the-art time-series deep\\nlearning neural network, long short-term memory (lstm), we created a system\\nthat predicts smap level-3 soil moisture data with atmospheric forcing,\\nmodel-simulated moisture, and static physiographic attributes as inputs. the\\nsystem removes most of the bias with model simulations and improves predicted\\nmoisture climatology, achieving small test root-mean-squared error (<0.035) and\\nhigh correlation coefficient >0.87 for over 75\\\\% of continental united states,\\nincluding the forested southeast. as the first application of lstm in\\nhydrology, we show the proposed network avoids overfitting and is robust for\\nboth temporal and spatial extrapolation tests. lstm generalizes well across\\nregions with distinct climates and physiography. with high fidelity to smap,\\nlstm shows great potential for hindcasting, data assimilation, and weather\\nforecasting.\\n)': False, \"contains(from context to concept: exploring semantic relationships in music with\\n  word2vec\\n  we explore the potential of a popular distributional semantics vector space\\nmodel, word2vec, for capturing meaningful relationships in ecological (complex\\npolyphonic) music. more precisely, the skip-gram version of word2vec is used to\\nmodel slices of music from a large corpus spanning eight musical genres. in\\nthis newly learned vector space, a metric based on cosine distance is able to\\ndistinguish between functional chord relationships, as well as harmonic\\nassociations in the music. evidence, based on cosine distance between\\nchord-pair vectors, suggests that an implicit circle-of-fifths exists in the\\nvector space. in addition, a comparison between pieces in different keys\\nreveals that key relationships are represented in word2vec space. these results\\nsuggest that the newly learned embedded vector representation does in fact\\ncapture tonal and harmonic characteristics of music, without receiving explicit\\ninformation about the musical content of the constituent slices. in order to\\ninvestigate whether proximity in the discovered space of embeddings is\\nindicative of `semantically-related' slices, we explore a music generation\\ntask, by automatically replacing existing slices from a given piece of music\\nwith new slices. we propose an algorithm to find substitute slices based on\\nspatial proximity and the pitch class distribution inferred in the chosen\\nsubspace. the results indicate that the size of the subspace used has a\\nsignificant effect on whether slices belonging to the same key are selected. in\\nsum, the proposed word2vec model is able to learn music-vector embeddings that\\ncapture meaningful tonal and harmonic relationships in music, thereby providing\\na useful tool for exploring musical properties and comparisons across pieces,\\nas a potential input representation for deep learning models, and as a music\\ngeneration device.\\n)\": False, 'contains(heterogeneous quantum computing for satellite constellation\\n  optimization: solving the weighted k-clique problem\\n  np-hard optimization problems scale very rapidly with problem size, becoming\\nunsolvable with brute force methods, even with supercomputing resources.\\ntypically, such problems have been approximated with heuristics. however, these\\nmethods still take a long time and are not guaranteed to find an optimal\\nsolution. quantum computing offers the possibility of producing significant\\nspeed-up and improved solution quality. current quantum annealing (qa) devices\\nare designed to solve difficult optimization problems, but they are limited by\\nhardware size and qubit connectivity restrictions. we present a novel\\nheterogeneous computing stack that combines qa and classical machine learning,\\nallowing the use of qa on problems larger than the hardware limits of the\\nquantum device. these results represent experiments on a real-world problem\\nrepresented by the weighted k-clique problem. through this experiment, we\\nprovide insight into the state of quantum machine learning.\\n)': False, 'contains(analytic expressions for stochastic distances between relaxed complex wishart distributions\\n\\n-the scaled complex wishart distribution is a widely used model for multilook full polarimetric sar data whose adequacy has been attested in the literature. 3 classification, segmentation, and image analysis techniques 1 which depend on this model have been devised, and many 0 of them employ some type of dissimilarity measure. in this 2 paper we derive analytic expressions for four stochastic rp distances between relaxed scaled complex wishart distributions in their most general form and in important aparticular cases. using these distances, inequalities are 9 obtained which lead to new ways of deriving the bartlett 1 and revised wishart distances. the expressiveness of the ] four analytic distances is assessed with respect to the lvariation of parameters. such distances are then used for deriving new tests statistics, which are proved to have .masymptotic chi-square distribution. adopting the test size t a as a comparison criterion, a sensitivity study is performed ts by means of monte carlo experiments suggesting that [ the bhattacharyya statistic outperforms all the others. the power of the tests is also assessed. applications to v1 actual data illustrate the discrimination and homogeneity 7 identification capabilities of these distances. 41 index terms-statistics, image analysis, information the5 ory, polarimetric radar, contrast measures. . 4\\n)': False, 'contains(precipitation nowcasting using a stochastic variational frame predictor\\n  with learned prior distribution\\n  we propose the use of a stochastic variational frame prediction deep neural\\nnetwork with a learned prior distribution trained on two-dimensional rain radar\\nreflectivity maps for precipitation nowcasting with lead times of up to 2 1/2\\nhours. we present a comparison to a standard convolutional lstm network and\\nassess the evolution of the structural similarity index for both methods. case\\nstudies are presented that illustrate that the novel methodology can yield\\nmeaningful forecasts without excessive blur for the time horizons of interest.\\n)': False, 'contains(artificial neural network surrogate modeling of oil reservoir: a case\\n  study\\n  we develop a data-driven model, introducing recent advances in machine\\nlearning to reservoir simulation. we use a conventional reservoir modeling tool\\nto generate training set and a special ensemble of artificial neural networks\\n(anns) to build a predictive model. the ann-based model allows to reproduce the\\ntime dependence of fluids and pressure distribution within the computational\\ncells of the reservoir model. we compare the performance of the ann-based model\\nwith conventional reservoir modeling and illustrate that ann-based model (1) is\\nable to capture all the output parameters of the conventional model with very\\nhigh accuracy and (2) demonstrate much higher computational performance. we\\nfinally elaborate on further options for research and developments within the\\narea of reservoir modeling.\\n)': False, 'contains(tools for higher-order network analysis\\n  networks are a fundamental model of complex systems throughout the sciences,\\nand network datasets are typically analyzed through lower-order connectivity\\npatterns described at the level of individual nodes and edges. however,\\nhigher-order connectivity patterns captured by small subgraphs, also called\\nnetwork motifs, describe the fundamental structures that control and mediate\\nthe behavior of many complex systems. we develop three tools for network\\nanalysis that use higher-order connectivity patterns to gain new insights into\\nnetwork datasets: (1) a framework to cluster nodes into modules based on joint\\nparticipation in network motifs; (2) a generalization of the clustering\\ncoefficient measurement to investigate higher-order closure patterns; and (3) a\\ndefinition of network motifs for temporal networks and fast algorithms for\\ncounting them. using these tools, we analyze data from biology, ecology,\\neconomics, neuroscience, online social networks, scientific collaborations,\\ntelecommunications, transportation, and the world wide web.\\n)': False, 'contains(state-space inference for non-linear latent force models with\\n  application to satellite orbit prediction\\n  latent force models (lfms) are flexible models that combine mechanistic\\nmodelling principles (i.e., physical models) with non-parametric data-driven\\ncomponents. several key applications of lfms need non-linearities, which\\nresults in analytically intractable inference. in this work we show how\\nnon-linear lfms can be represented as non-linear white noise driven state-space\\nmodels and present an efficient non-linear kalman filtering and smoothing based\\nmethod for approximate state and parameter inference. we illustrate the\\nperformance of the proposed methodology via two simulated examples, and apply\\nit to a real-world problem of long-term prediction of gps satellite orbits.\\n)': False, 'contains(constraining strongly-coupled new physics from cosmic rays with machine\\n  learning techniques\\n  cosmic rays interacting with the atmosphere allow for the probing of\\nfundamental interactions at ultra-high energies. we thus obtain limits on\\nstrongly-coupled new physics models via their imprints on cosmic ray air\\nshowers. using the monte carlo event generators herwig and herbvi, and the air\\nshower simulator corsika, to simulate such processes, we apply machine learning\\nalgorithms to the simulated observables to discriminate the events arising via\\nnew physics from the qcd background, before using the signal and background\\ndiscrimination performance to set potential limits on the cross sections of the\\nnew physics models.\\n)': False, 'contains(application of machine learning in rock facies classification with\\n  physics-motivated feature augmentation\\n  with recent progress in algorithms and the availability of massive amounts of\\ncomputation power, application of machine learning techniques is becoming a hot\\ntopic in the oil and gas industry. one of the most promising aspects to apply\\nmachine learning to the upstream field is the rock facies classification in\\nreservoir characterization, which is crucial in determining the net pay\\nthickness of reservoirs, thus a definitive factor in drilling decision making\\nprocess. for complex machine learning tasks like facies classification, feature\\nengineering is often critical. this paper shows the inclusion of\\nphysics-motivated feature interaction in feature augmentation can further\\nimprove the capability of machine learning in rock facies classification. we\\ndemonstrate this approach with the seg 2016 machine learning contest dataset\\nand the top winning algorithms. the improvement is roboust and can be $\\\\sim5\\\\%$\\nbetter than current existing best f-1 score, where f-1 is an evaluation metric\\nused to quantify average prediction accuracy.\\n)': False, \"contains(gear: geometry-aware r√©nyi information\\n  shannon's seminal theory of information has been of paramount importance in\\nthe development of modern machine learning techniques. however, standard\\ninformation measures deal with probability distributions over an alphabet\\nconsidered as a mere set of symbols and disregard further geometric structure,\\nwhich might be available in the form of a metric or similarity function. we\\nadvocate the use of a notion of entropy that reflects not only the relative\\nabundances of symbols but also the similarities between them, which was\\noriginally introduced in theoretical ecology to study the diversity of\\nbiological communities. echoing this idea, we propose a criterion for comparing\\ntwo probability distributions (possibly degenerate and with non-overlapping\\nsupports) that takes into account the geometry of the space in which the\\ndistributions are defined. our proposal exhibits performance on par with\\nstate-of-the-art methods based on entropy-regularized optimal transport, but\\nenjoys a closed-form expression and thus a lower computational cost. we\\ndemonstrate the versatility of our proposal via experiments on a broad range of\\ndomains: computing image barycenters, approximating densities with a collection\\nof (super-) samples; summarizing texts; assessing mode coverage; as well as\\ntraining generative models.\\n)\": False, 'contains(social network analysis of scientific articles published by food policy\\n\\nthe article analyses co-authorship and co-citation networks in food policy, which is the most important agricultural policy journal in the field of agricultural economics. the paper highlights the principal researchers in this field together with their authorship and citation networks on the basis of 714 articles written between 2006 and 2015. results suggest that the majority of the articles were written by a small number of researchers, indicating that groups and central authors play an important role in scientific advances. it also turns out that the number of articles and the central role played in the network are not related, contrary to expectations. results also suggest that groups cite themselves more often than average, thereby boosting the scientific advancement of their own members.\\n)': False, 'contains(deep recurrent neural networks for mapping winter vegetation quality\\n  coverage via multi-temporal sar sentinel-1\\n  mapping winter vegetation quality coverage is a challenge problem of remote\\nsensing. this is due to the cloud coverage in winter period, leading to use\\nradar rather than optical images. the objective of this paper is to provide a\\nbetter understanding of the capabilities of radar sentinel-1 and deep learning\\nconcerning about mapping winter vegetation quality coverage. the analysis\\npresented in this paper is carried out on multi-temporal sentinel-1 data over\\nthe site of la rochelle, france, during the campaign in december 2016. this\\ndataset were processed in order to produce an intensity radar data stack from\\noctober 2016 to february 2017. two deep recurrent neural network (rnn) based\\nclassifier methods were employed. we found that the results of rnns clearly\\noutperformed the classical machine learning approaches (support vector machine\\nand random forest). this study confirms that the time series radar sentinel-1\\nand rnns could be exploited for winter vegetation quality cover mapping.\\n)': False, 'contains(distinguishing cause from effect using observational data: methods and\\n  benchmarks\\n  the discovery of causal relationships from purely observational data is a\\nfundamental problem in science. the most elementary form of such a causal\\ndiscovery problem is to decide whether x causes y or, alternatively, y causes\\nx, given joint observations of two variables x, y. an example is to decide\\nwhether altitude causes temperature, or vice versa, given only joint\\nmeasurements of both variables. even under the simplifying assumptions of no\\nconfounding, no feedback loops, and no selection bias, such bivariate causal\\ndiscovery problems are challenging. nevertheless, several approaches for\\naddressing those problems have been proposed in recent years. we review two\\nfamilies of such methods: additive noise methods (anm) and information\\ngeometric causal inference (igci). we present the benchmark causeeffectpairs\\nthat consists of data for 100 different cause-effect pairs selected from 37\\ndatasets from various domains (e.g., meteorology, biology, medicine,\\nengineering, economy, etc.) and motivate our decisions regarding the \"ground\\ntruth\" causal directions of all pairs. we evaluate the performance of several\\nbivariate causal discovery methods on these real-world benchmark data and in\\naddition on artificially simulated data. our empirical results on real-world\\ndata indicate that certain methods are indeed able to distinguish cause from\\neffect using only purely observational data, although more benchmark data would\\nbe needed to obtain statistically significant conclusions. one of the best\\nperforming methods overall is the additive-noise method originally proposed by\\nhoyer et al. (2009), which obtains an accuracy of 63+-10 % and an auc of\\n0.74+-0.05 on the real-world benchmark. as the main theoretical contribution of\\nthis work we prove the consistency of that method.\\n)': False, 'contains(can learning from natural image denoising be used for seismic data\\n  interpolation?\\n  we propose a convolutional neural network (cnn) denoising based method for\\nseismic data interpolation. it provides a simple and efficient way to break\\nthough the lack problem of geophysical training labels that are often required\\nby deep learning methods. the new method consists of two steps: (1) train a set\\nof cnn denoisers from natural image clean-noisy pairs to learn denoising; (2)\\nintegrate the trained cnn denoisers into project onto convex set (pocs)\\nframework to perform seismic data interpolation. the method alleviates the\\ndemanding of seismic big data with similar features as applications of\\nend-to-end deep learning on seismic data interpolation. additionally, the\\nproposed method is flexible for many cases of traces missing because missing\\ncases are not involved in the training step, and thus it is of plug-and-play\\nnature. these indicate the high generalizability of our approach and the\\nreduction of the need of the problem-specific training. primary results on\\nsynthetic and field data show promising interpolation performances of the\\npresented cnn-pocs method in terms of signal-to-noise ratio, de-aliasing and\\nweak-feature reconstruction, in comparison with traditional $f$-$x$ prediction\\nfiltering and curvelet transform based pocs methods.\\n)': False, 'contains(bagged boosted trees for classification of ecological momentary\\n  assessment data\\n  ecological momentary assessment (ema) data is organized in multiple levels\\n(per-subject, per-day, etc.) and this particular structure should be taken into\\naccount in machine learning algorithms used in ema like decision trees and its\\nvariants. we propose a new algorithm called bbt (standing for bagged boosted\\ntrees) that is enhanced by a over/under sampling method and can provide better\\nestimates for the conditional class probability function. experimental results\\non a real-world dataset show that bbt can benefit ema data classification and\\nperformance.\\n)': False, 'contains(forecasting the colorado river discharge using an artificial neural\\n  network (ann) approach\\n  artificial neural network (ann) based model is a computational approach\\ncommonly used for modeling the complex relationships between input and output\\nparameters. prediction of the flow rate of a river is a requisite for any\\nsuccessful water resource management and river basin planning. in the current\\nsurvey, the effectiveness of an artificial neural network was examined to\\npredict the colorado river discharge. in this modeling process, an ann model\\nwas used to relate the discharge of the colorado river to such parameters as\\nthe amount of precipitation, ambient temperature and snowpack level at a\\nspecific time of the year. the model was able to precisely study the impact of\\nclimatic parameters on the flow rate of the colorado river.\\n)': False, 'contains(regression from dependent observations\\n  the standard linear and logistic regression models assume that the response\\nvariables are independent, but share the same linear relationship to their\\ncorresponding vectors of covariates. the assumption that the response variables\\nare independent is, however, too strong. in many applications, these responses\\nare collected on nodes of a network, or some spatial or temporal domain, and\\nare dependent. examples abound in financial and meteorological applications,\\nand dependencies naturally arise in social networks through peer effects.\\nregression with dependent responses has thus received a lot of attention in the\\nstatistics and economics literature, but there are no strong consistency\\nresults unless multiple independent samples of the vectors of dependent\\nresponses can be collected from these models. we present computationally and\\nstatistically efficient methods for linear and logistic regression models when\\nthe response variables are dependent on a network. given one sample from a\\nnetworked linear or logistic regression model and under mild assumptions, we\\nprove strong consistency results for recovering the vector of coefficients and\\nthe strength of the dependencies, recovering the rates of standard regression\\nunder independent observations. we use projected gradient descent on the\\nnegative log-likelihood, or negative log-pseudolikelihood, and establish their\\nstrong convexity and consistency using concentration of measure for dependent\\nrandom variables.\\n)': False, 'contains(eurosat: a novel dataset and deep learning benchmark for land use and\\n  land cover classification\\n  in this paper, we address the challenge of land use and land cover\\nclassification using sentinel-2 satellite images. the sentinel-2 satellite\\nimages are openly and freely accessible provided in the earth observation\\nprogram copernicus. we present a novel dataset based on sentinel-2 satellite\\nimages covering 13 spectral bands and consisting out of 10 classes with in\\ntotal 27,000 labeled and geo-referenced images. we provide benchmarks for this\\nnovel dataset with its spectral bands using state-of-the-art deep convolutional\\nneural network (cnns). with the proposed novel dataset, we achieved an overall\\nclassification accuracy of 98.57%. the resulting classification system opens a\\ngate towards a number of earth observation applications. we demonstrate how\\nthis classification system can be used for detecting land use and land cover\\nchanges and how it can assist in improving geographical maps. the\\ngeo-referenced dataset eurosat is made publicly available at\\nhttps://github.com/phelber/eurosat.\\n)': False, 'contains(stochastic reconstruction of an oolitic limestone by generative\\n  adversarial networks\\n  stochastic image reconstruction is a key part of modern digital rock physics\\nand materials analysis that aims to create numerous representative samples of\\nmaterial micro-structures for upscaling, numerical computation of effective\\nproperties and uncertainty quantification. we present a method of\\nthree-dimensional stochastic image reconstruction based on generative\\nadversarial neural networks (gans). gans represent a framework of unsupervised\\nlearning methods that require no a priori inference of the probability\\ndistribution associated with the training data. using a fully convolutional\\nneural network allows fast sampling of large volumetric images.we apply a gan\\nbased workflow of network training and image generation to an oolitic ketton\\nlimestone micro-ct dataset. minkowski functionals, effective permeability as\\nwell as velocity distributions of simulated flow within the acquired images are\\ncompared with the synthetic reconstructions generated by the deep neural\\nnetwork. while our results show that gans allow a fast and accurate\\nreconstruction of the evaluated image dataset, we address a number of open\\nquestions and challenges involved in the evaluation of generative network-based\\nmethods.\\n)': False, 'contains(gaussian process kernels for pattern discovery and extrapolation\\n  gaussian processes are rich distributions over functions, which provide a\\nbayesian nonparametric approach to smoothing and interpolation. we introduce\\nsimple closed form kernels that can be used with gaussian processes to discover\\npatterns and enable extrapolation. these kernels are derived by modelling a\\nspectral density -- the fourier transform of a kernel -- with a gaussian\\nmixture. the proposed kernels support a broad class of stationary covariances,\\nbut gaussian process inference remains simple and analytic. we demonstrate the\\nproposed kernels by discovering patterns and performing long range\\nextrapolation on synthetic examples, as well as atmospheric co2 trends and\\nairline passenger data. we also show that we can reconstruct standard\\ncovariances within our framework.\\n)': False, 'contains(online learning as a way to tackle instabilities and biases in neural\\n  network parameterizations\\n  over the last couple of years, machine learning parameterizations have\\nemerged as a potential way to improve the representation of sub-grid processes\\nin atmospheric models. all previous studies created a training dataset from a\\nhigh-resolution simulation, fitted a machine learning algorithms to that\\ndataset, and then plugged the trained algorithm into an atmospheric model. the\\nresulting online simulations were frequently plagued by instabilities and\\nbiases. here, i propose online learning as a way to combat these issues. in\\nonline learning, the pretrained machine learning parameterization, specifically\\na neural network, is run in parallel with a high-resolution simulation which is\\nkept in sync with the neural network-driven atmospheric model through constant\\nforcing. this enables the neural network to learn from the tendencies that the\\nhigh-resolution simulation would produce if it experienced the atmospheric\\nstates the neural network creates. the concept is illustrated using the lorenz\\n96 model, where online learning is able to recover the \"true\"\\nparameterizations. then i present detailed algorithms for implementing online\\nlearning in the 3d cloud-resolving model and super-parameterization frameworks.\\nfinally, i discuss outstanding challenges and issues not solved by this\\napproach.\\n)': False, 'contains(an image-driven machine learning approach to kinetic modeling of a\\n  discontinuous precipitation reaction\\n  micrograph quantification is an essential component of several materials\\nscience studies. machine learning methods, in particular convolutional neural\\nnetworks, have previously demonstrated performance in image recognition tasks\\nacross several disciplines (e.g. materials science, medical imaging, facial\\nrecognition). here, we apply these well-established methods to develop an\\napproach to microstructure quantification for kinetic modeling of a\\ndiscontinuous precipitation reaction in a case study on the uranium-molybdenum\\nsystem. prediction of material processing history based on image data\\n(classification), calculation of area fraction of phases present in the\\nmicrographs (segmentation), and kinetic modeling from segmentation results were\\nperformed. results indicate that convolutional neural networks represent\\nmicrostructure image data well, and segmentation using the k-means clustering\\nalgorithm yields results that agree well with manually annotated images.\\nclassification accuracies of original and segmented images are both 94\\\\% for a\\n5-class classification problem. kinetic modeling results agree well with\\npreviously reported data using manual thresholding. the image quantification\\nand kinetic modeling approach developed and presented here aims to reduce\\nresearcher bias introduced into the characterization process, and allows for\\nleveraging information in limited image data sets.\\n)': False, 'contains(aid: a benchmark dataset for performance evaluation of aerial scene classi cation\\n\\naerial scene classi cation, which aims to automatically label an aerial image with a speci c semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. in recent years, it has become an active task in remote sensing area and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. however, the existing datasets for aerial scene classi cation like uc-merced dataset and whu-rs19 are with relatively small sizes, and the results on them are already saturated. this largely limits the development of scene classi cation algorithms. this paper describes the aerial image dataset (aid): a large-scale dataset for aerial scene classi cation. the goal of aid is to advance the state-of-the-arts in scene classi cation of remote sensing images. for creating aid, we collect and annotate more than ten thousands aerial scene images. in addition, a comprehensive review of the existing aerial scene classi cation techniques as well as recent widely-used deep learning methods is given. finally, we provide a performance analysis of typical aerial scene classi cation and deep learning approaches on aid, which can be served as the baseline results on this benchmark.\\n)': False, 'contains(automatically identifying, counting, and describing wild animals in\\n  camera-trap images with deep learning\\n  having accurate, detailed, and up-to-date information about the location and\\nbehavior of animals in the wild would revolutionize our ability to study and\\nconserve ecosystems. we investigate the ability to automatically, accurately,\\nand inexpensively collect such data, which could transform many fields of\\nbiology, ecology, and zoology into \"big data\" sciences. motion sensor \"camera\\ntraps\" enable collecting wildlife pictures inexpensively, unobtrusively, and\\nfrequently. however, extracting information from these pictures remains an\\nexpensive, time-consuming, manual task. we demonstrate that such information\\ncan be automatically extracted by deep learning, a cutting-edge type of\\nartificial intelligence. we train deep convolutional neural networks to\\nidentify, count, and describe the behaviors of 48 species in the\\n3.2-million-image snapshot serengeti dataset. our deep neural networks\\nautomatically identify animals with over 93.8% accuracy, and we expect that\\nnumber to improve rapidly in years to come. more importantly, if our system\\nclassifies only images it is confident about, our system can automate animal\\nidentification for 99.3% of the data while still performing at the same 96.6%\\naccuracy as that of crowdsourced teams of human volunteers, saving more than\\n8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000\\nhours) on this 3.2-million-image dataset. those efficiency gains immediately\\nhighlight the importance of using deep neural networks to automate data\\nextraction from camera-trap images. our results suggest that this technology\\ncould enable the inexpensive, unobtrusive, high-volume, and even real-time\\ncollection of a wealth of information about vast numbers of animals in the\\nwild.\\n)': False, \"contains(slack channels ecology in enterprises: how employees collaborate through\\n  group chat\\n  despite the long history of studying instant messaging usage in\\norganizations, we know very little about how today's people participate in\\ngroup chat channels and interact with others. in this short note, we aim to\\nupdate the existing knowledge on how group chat is used in the context of\\ntoday's organizations. we have the privilege of collecting a total of 4300\\npublicly available group chat channels in slack from an r\\\\&d department in a\\nmultinational it company. through qualitative coding of 100 channels, we\\nidentified 9 channel categories such as project based channels and event\\nchannels. we further defined a feature metric with 21 features to depict the\\ngroup communication style for these group chat channels, with which we\\nsuccessfully trained a machine learning model that can automatically classify a\\ngiven group channel into one of the 9 categories. in addition, we illustrated\\nhow these communication metrics could be used for analyzing teams'\\ncollaboration activities. we focused on 117 project teams as we have their\\nperformance data, and further collected 54 out of the 117 teams' slack group\\ndata and generated the communication style metrics for each of them. with these\\ndata, we are able to build a regression model to reveal the relationship\\nbetween these group communication styles and one indicator of the project team\\nperformance.\\n)\": False, \"contains(recover missing sensor data with iterative imputing network\\n  sensor data has been playing an important role in machine learning tasks,\\ncomplementary to the human-annotated data that is usually rather costly.\\nhowever, due to systematic or accidental mis-operations, sensor data comes very\\noften with a variety of missing values, resulting in considerable difficulties\\nin the follow-up analysis and visualization. previous work imputes the missing\\nvalues by interpolating in the observational feature space, without consulting\\nany latent (hidden) dynamics. in contrast, our model captures the latent\\ncomplex temporal dynamics by summarizing each observation's context with a\\nnovel iterative imputing network, thus significantly outperforms previous work\\non the benchmark beijing air quality and meteorological dataset. our model also\\nyields consistent superiority over other methods in cases of different missing\\nrates.\\n)\": False, \"contains(poverty prediction with public landsat 7 satellite imagery and machine\\n  learning\\n  obtaining detailed and reliable data about local economic livelihoods in\\ndeveloping countries is expensive, and data are consequently scarce. previous\\nwork has shown that it is possible to measure local-level economic livelihoods\\nusing high-resolution satellite imagery. however, such imagery is relatively\\nexpensive to acquire, often not updated frequently, and is mainly available for\\nrecent years. we train cnn models on free and publicly available multispectral\\ndaytime satellite images of the african continent from the landsat 7 satellite,\\nwhich has collected imagery with global coverage for almost two decades. we\\nshow that despite these images' lower resolution, we can achieve accuracies\\nthat exceed previous benchmarks.\\n)\": False, \"contains(impact of atmospheric chromatic effects on weak lensing measurements\\n  current and future imaging surveys will measure cosmic shear with statistical\\nprecision that demands a deeper understanding of potential systematic biases in\\ngalaxy shape measurements than has been achieved to date. we use analytic and\\ncomputational techniques to study the impact on shape measurements of two\\natmospheric chromatic effects for ground-based surveys such as the dark energy\\nsurvey and the large synoptic survey telescope (lsst): (i) atmospheric\\ndifferential chromatic refraction and (ii) wavelength dependence of seeing. we\\ninvestigate the effects of using the point spread function (psf) measured with\\nstars to determine the shapes of galaxies that have different spectral energy\\ndistributions than the stars. we find that both chromatic effects lead to\\nsignificant biases in galaxy shape measurements for current and future surveys,\\nif not corrected. using simulated galaxy images, we find a form of chromatic\\n`model bias' that arises when fitting a galaxy image with a model that has been\\nconvolved with a stellar, instead of galactic, point spread function. we show\\nthat both forms of atmospheric chromatic biases can be predicted (and\\ncorrected) with minimal model bias by applying an ordered set of perturbative\\npsf-level corrections based on machine-learning techniques applied to six-band\\nphotometry. catalog-level corrections do not address the model bias. we\\nconclude that achieving the ultimate precision for weak lensing from current\\nand future ground-based imaging surveys requires a detailed understanding of\\nthe wavelength dependence of the psf from the atmosphere, and from other\\nsources such as optics and sensors. the source code for this analysis is\\navailable at https://github.com/darkenergysciencecollaboration/chroma .\\n)\": False, 'contains(a n d s e qu en c e- to - s equ en c e n et wo r ks\\n\\nthis article explores the concepts of ocean wave multivariate multistep forecasting, reconstruction and feature selection. we introduce recurrent neural network frameworks, integrated with bayesian hyperparameter optimization and elastic net methods. we consider both short- and long-term forecasts and reconstruction, for significant wave height and output power of the ocean waves. sequenceto-sequence neural networks are being developed for the first time to reconstruct the missing characteristics of ocean waves based on information from nearby wave sensors. our results indicate that the adam and amsgrad optimization algorithms are the most robust ones to optimize the sequence-tosequence network. for the case of significant wave height reconstruction, we compare the proposed methods with alternatives on a well-studied dataset. we show the superiority of the proposed methods considering several error metrics. we design a new case study based on measurement stations along the east coast of the united states and investigate the feature selection concept. comparisons substantiate the benefit of utilizing elastic net. moreover, case study results indicate that when the number of features is considerable, having deeper structures improves the performance.\\n)': False, 'contains(assigning a grade: accurate measurement of road quality using satellite\\n  imagery\\n  roads are critically important infrastructure to societal and economic\\ndevelopment, with huge investments made by governments every year. however,\\nmethods for monitoring those investments tend to be time-consuming, laborious,\\nand expensive, placing them out of reach for many developing regions. in this\\nwork, we develop a model for monitoring the quality of road infrastructure\\nusing satellite imagery. for this task, we harness two trends: the increasing\\navailability of high-resolution, often-updated satellite imagery, and the\\nenormous improvement in speed and accuracy of convolutional neural\\nnetwork-based methods for performing computer vision tasks. we employ a unique\\ndataset of road quality information on 7000km of roads in kenya combined with\\n50cm resolution satellite imagery. we create models for a binary classification\\ntask as well as a comprehensive 5-category classification task, with accuracy\\nscores of 88 and 73 percent respectively. we also provide evidence of the\\nrobustness of our methods with challenging held-out scenarios, though we note\\nsome improvement is still required for confident analysis of a never before\\nseen road. we believe these results are well-positioned to have substantial\\nimpact on a broad set of transport applications.\\n)': False, 'contains(machine learning on images using a string-distance\\n  we present a new method for image feature-extraction which is based on\\nrepresenting an image by a finite-dimensional vector of distances that measure\\nhow different the image is from a set of image prototypes. we use the recently\\nintroduced universal image distance (uid) \\\\cite{ratsabychesterieee2012} to\\ncompare the similarity between an image and a prototype image. the advantage in\\nusing the uid is the fact that no domain knowledge nor any image analysis need\\nto be done. each image is represented by a finite dimensional feature vector\\nwhose components are the uid values between the image and a finite set of image\\nprototypes from each of the feature categories. the method is automatic since\\nonce the user selects the prototype images, the feature vectors are\\nautomatically calculated without the need to do any image analysis. the\\nprototype images can be of different size, in particular, different than the\\nimage size. based on a collection of such cases any supervised or unsupervised\\nlearning algorithm can be used to train and produce an image classifier or\\nimage cluster analysis. in this paper we present the image feature-extraction\\nmethod and use it on several supervised and unsupervised learning experiments\\nfor satellite image data.\\n)': False, 'contains(probabilistic design of a molybdenum-base alloy using a neural network\\n  an artificial intelligence tool is exploited to discover and characterize a\\nnew molybdenum-base alloy that is the most likely to simultaneously satisfy\\ntargets of cost, phase stability, precipitate content, yield stress, and\\nhardness. experimental testing demonstrates that the proposed alloy fulfils the\\ncomputational predictions, and furthermore the physical properties exceed those\\nof other commercially available mo-base alloys for forging-die applications.\\n)': False, 'contains(bin-ct: urban waste collection based on predicting the container fill level\\n\\nthe fast demographic growth, together with the population concentration in cities and the increasing amount of daily waste, are factors that are pushing to the limit the ability of waste assimilation by nature. therefore, we need technological means to optimally manage of the waste collection process, which represents 70% of the operational cost in waste treatment. in this article, we present a free intelligent software system called bin-ct (bin for the city), based on computational learning algorithms, which plans the 9best routes for waste collection supported by past (historical) and future (predictions) data. 1the objective of the system is to reduction the cost of the waste collection service minimizing the distance traveled by a truck to 0collect the waste from a container, thereby reducing the fuel consumption. at the same time the quality of service for the citizen 2 is increased, avoiding the annoying overflows of containers thanks to the accurate fill-level predictions given by bin-ct. in this r particle we show the features of our software system, illustrating its operation with a real case study of a spanish city. we conclude that the use of bin-ct avoids unnecessary trips to containers, reduces the distance traveled to collect a container by 20%, and generates routes 33.2% shorter than the routes used by the company. therefore it enables a considerable reduction of total costs a 2and harmful emissions thrown up into the atmosphere. 2\\n)': False, 'contains(ubl: an r package for utility-based learning\\n  this document describes the r package ubl that allows the use of several\\nmethods for handling utility-based learning problems. classification and\\nregression problems that assume non-uniform costs and/or benefits pose serious\\nchallenges to predictive analytic tasks. in the context of meteorology,\\nfinance, medicine, ecology, among many other, specific domain information\\nconcerning the preference bias of the users must be taken into account to\\nenhance the models predictive performance. to deal with this problem, a large\\nnumber of techniques was proposed by the research community for both\\nclassification and regression tasks. the main goal of ubl package is to\\nfacilitate the utility-based predictive analytic task by providing a set of\\nmethods to deal with this type of problems in the r environment. it is a\\nversatile tool that provides mechanisms to handle both regression and\\nclassification (binary and multiclass) tasks. moreover, ubl package allows the\\nuser to specify his domain preferences, but it also provides some automatic\\nmethods that try to infer those preference bias from the domain, considering\\nsome common known settings.\\n)': False, 'contains(graph regularized low rank representation for aerosol optical depth\\n  retrieval\\n  in this paper, we propose a novel data-driven regression model for aerosol\\noptical depth (aod) retrieval. first, we adopt a low rank representation (lrr)\\nmodel to learn a powerful representation of the spectral response. then, graph\\nregularization is incorporated into the lrr model to capture the local\\nstructure information and the nonlinear property of the remote-sensing data.\\nsince it is easy to acquire the rich satellite-retrieval results, we use them\\nas a baseline to construct the graph. finally, the learned feature\\nrepresentation is feeded into support vector machine (svm) to retrieve aod.\\nexperiments are conducted on two widely used data sets acquired by different\\nsensors, and the experimental results show that the proposed method can achieve\\nsuperior performance compared to the physical models and other state-of-the-art\\nempirical models.\\n)': False, 'contains(netherlands dataset: a new public dataset for machine learning in\\n  seismic interpretation\\n  machine learning and, more specifically, deep learning algorithms have seen\\nremarkable growth in their popularity and usefulness in the last years. this is\\narguably due to three main factors: powerful computers, new techniques to train\\ndeeper networks and larger datasets. although the first two are readily\\navailable in modern computers and ml libraries, the last one remains a\\nchallenge for many domains. it is a fact that big data is a reality in almost\\nall fields nowadays, and geosciences are not an exception. however, to achieve\\nthe success of general-purpose applications such as imagenet - for which there\\nare +14 million labeled images for 1000 target classes - we not only need more\\ndata, we need more high-quality labeled data. when it comes to the oil&gas\\nindustry, confidentiality issues hamper even more the sharing of datasets. in\\nthis work, we present the netherlands interpretation dataset, a contribution to\\nthe development of machine learning in seismic interpretation. the netherlands\\nf3 dataset acquisition was carried out in the north sea, netherlands offshore.\\nthe data is publicly available and contains pos-stack data, 8 horizons and well\\nlogs of 4 wells. for the purposes of our machine learning tasks, the original\\ndataset was reinterpreted, generating 9 horizons separating different seismic\\nfacies intervals. the interpreted horizons were used to generate approximatelly\\n190,000 labeled images for inlines and crosslines. finally, we present two deep\\nlearning applications in which the proposed dataset was employed and produced\\ncompelling results.\\n)': False, 'contains(sequential geophysical and flow inversion to characterize fracture\\n  networks in subsurface systems\\n  subsurface applications including geothermal, geological carbon\\nsequestration, oil and gas, etc., typically involve maximizing either the\\nextraction of energy or the storage of fluids. characterizing the subsurface is\\nextremely complex due to heterogeneity and anisotropy. due to this complexity,\\nthere are uncertainties in the subsurface parameters, which need to be\\nestimated from multiple diverse as well as fragmented data streams. in this\\npaper, we present a non-intrusive sequential inversion framework, for\\nintegrating data from geophysical and flow sources to constraint subsurface\\ndiscrete fracture networks (dfn). in this approach, we first estimate bounds on\\nthe statistics for the dfn fracture orientations using microseismic data. these\\nbounds are estimated through a combination of a focal mechanism (physics-based\\napproach) and clustering analysis (statistical approach) of seismic data. then,\\nthe fracture lengths are constrained based on the flow data. the efficacy of\\nthis multi-physics based sequential inversion is demonstrated through a\\nrepresentative synthetic example.\\n)': False, 'contains(some neural network applications in environmental sciences. part ii: advancing computational efficiency of environmental numerical models\\n\\na new generic neural network (nn) application-improving computational efficiency of certain processes in numerical environmental models-is considered. this approach can be used to accelerate the calculations and improve the accuracy of the parameterizations of several types of physical processes which generally require computations involving complex mathematical expressions, including differential and integral equations, rules, restrictions and highly nonlinear empirical relations based on physical or statistical models. it is shown that, from a mathematical point of view, such parameterizations can usually be considered as continuous mappings (continuous dependencies between two vectors) and, therefore, nns can be used to replace primary parameterization algorithms. in addition to fast and accurate approximation of the primary parameterization, nn also provides the entire jacobian for very little computation cost. four particular real-life applications of the nn approach are presented here: for oceanic numerical models, a nn approximation of the unesco equation of state of the sea water (nn for the density of the seawater) and an inversion of this equation (nn for the salinity of the seawater); for atmospheric numerical models, a nn approximation for long wave radiative transfer code; and for wave models, a nn approximation for the nonlinear wave - wave interaction. in all considered applications a significant acceleration of numerical computations has been achieved. the first two of these nn applications have already been implemented in the multi-scale ocean forecast system at ncep. the nn approach introduced in this paper can provide numerically efficient solutions to a wide range of problems in numerical models where lengthy, complicated calculations, which describe physical, chemical and/or biological processes, must be repeated frequently. q 2003 elsevier science ltd. all rights reserved.\\n)': False, \"contains(a semi-supervised spatial spectral regularized manifold local scaling\\n  cut with hgf for dimensionality reduction of hyperspectral images\\n  hyperspectral images (hsi) contain a wealth of information over hundreds of\\ncontiguous spectral bands, making it possible to classify materials through\\nsubtle spectral discrepancies. however, the classification of this rich\\nspectral information is accompanied by the challenges like high dimensionality,\\nsingularity, limited training samples, lack of labeled data samples,\\nheteroscedasticity and nonlinearity. to address these challenges, we propose a\\nsemi-supervised graph based dimensionality reduction method named\\n`semi-supervised spatial spectral regularized manifold local scaling cut'\\n(s3rmlsc). the underlying idea of the proposed method is to exploit the limited\\nlabeled information from both the spectral and spatial domains along with the\\nabundant unlabeled samples to facilitate the classification task by retaining\\nthe original distribution of the data. in s3rmlsc, a hierarchical guided filter\\n(hgf) is initially used to smoothen the pixels of the hsi data to preserve the\\nspatial pixel consistency. this step is followed by the construction of linear\\npatches from the nonlinear manifold by using the maximal linear patch (mlp)\\ncriterion. then the inter-patch and intra-patch dissimilarity matrices are\\nconstructed in both spectral and spatial domains by regularized manifold local\\nscaling cut (rmlsc) and neighboring pixel manifold local scaling cut (npmlsc)\\nrespectively. finally, we obtain the projection matrix by optimizing the\\nupdated semi-supervised spatial-spectral between-patch and total-patch\\ndissimilarity. the effectiveness of the proposed dr algorithm is illustrated\\nwith publicly available real-world hsi datasets.\\n)\": False, \"contains(deepgeo: photo localization with deep neural network\\n  in this paper we address the task of determining the geographical location of\\nan image, a pertinent problem in learning and computer vision. this research\\nwas inspired from playing geoguessr, a game that tests a humans' ability to\\nlocalize themselves using just images of their surroundings. in particular, we\\nwish to investigate how geographical, ecological and man-made features\\ngeneralize for random location prediction. this is framed as a classification\\nproblem: given images sampled from the usa, the most-probable state among 50 is\\npredicted. previous work uses models extensively trained on large, unfiltered\\nonline datasets that are primed towards specific locations. to this end, we\\ncreate (and open-source) the 50states10k dataset - with 0.5 million google\\nstreet view images of the country. a deep neural network based on the resnet\\narchitecture is trained, and four different strategies of incorporating\\nlow-level cardinality information are presented. this model achieves an\\naccuracy 20 times better than chance on a test dataset, which rises to 71.87%\\nwhen taking the best of top-5 guesses. the network also beats human subjects in\\n4 out of 5 rounds of geoguessr.\\n)\": False, 'contains(including physics in deep learning -- an example from 4d seismic\\n  pressure saturation inversion\\n  geoscience data often have to rely on strong priors in the face of\\nuncertainty. additionally, we often try to detect or model anomalous sparse\\ndata that can appear as an outlier in machine learning models. these are\\nclassic examples of imbalanced learning. approaching these problems can benefit\\nfrom including prior information from physics models or transforming data to a\\nbeneficial domain. we show an example of including physical information in the\\narchitecture of a neural network as prior information. we go on to present\\nnoise injection at training time to successfully transfer the network from\\nsynthetic data to field data.\\n)': False, 'contains(workflow-driven distributed machine learning in chase-ci: a cognitive\\n  hardware and software ecosystem community infrastructure\\n  the advances in data, computing and networking over the last two decades led\\nto a shift in many application domains that includes machine learning on big\\ndata as a part of the scientific process, requiring new capabilities for\\nintegrated and distributed hardware and software infrastructure. this paper\\ncontributes a workflow-driven approach for dynamic data-driven application\\ndevelopment on top of a new kind of networked cyberinfrastructure called\\nchase-ci. in particular, we present: 1) the architecture for chase-ci, a\\nnetwork of distributed fast gpu appliances for machine learning and storage\\nmanaged through kubernetes on the high-speed (10-100gbps) pacific research\\nplatform (prp); 2) a machine learning software containerization approach and\\nlibraries required for turning such a network into a distributed computer for\\nbig data analysis; 3) an atmospheric science case study that can only be made\\nscalable with an infrastructure like chase-ci; 4) capabilities for virtual\\ncluster management for data communication and analysis in a dynamically\\nscalable fashion, and visualization across the network in specialized\\nvisualization facilities in near real-time; and, 5) a step-by-step workflow and\\nperformance measurement approach that enables taking advantage of the dynamic\\narchitecture of the chase-ci network and container management infrastructure.\\n)': False, 'contains(data-driven forecast of dengue outbreaks in brazil: a critical\\n  assessment of climate conditions for different capitals\\n  local climate conditions play a major role in the development of the mosquito\\npopulation responsible for transmitting dengue fever. since the {\\\\em aedes\\naegypti} mosquito is also a primary vector for the recent zika and chikungunya\\nepidemics across the americas, a detailed monitoring of periods with favorable\\nclimate conditions for mosquito profusion may improve the timing of\\nvector-control efforts and other urgent public health strategies. we apply\\ndimensionality reduction techniques and machine-learning algorithms to climate\\ntime series data and analyze their connection to the occurrence of dengue\\noutbreaks for seven major cities in brazil. specifically, we have identified\\ntwo key variables and a period during the annual cycle that are highly\\npredictive of epidemic outbreaks. the key variables are the frequency of\\nprecipitation and temperature during an approximately two month window of the\\nwinter season preceding the outbreak. thus simple climate signatures may be\\ninfluencing dengue outbreaks even months before their occurrence. some of the\\nmore challenging datasets required usage of compressive-sensing procedures to\\nestimate missing entries for temperature and precipitation records. our results\\nindicate that each brazilian capital considered has a unique frequency of\\nprecipitation and temperature signature in the winter preceding a dengue\\noutbreak. such climate contributions on vector populations are key factors in\\ndengue dynamics which could lead to more accurate prediction models and early\\nwarning systems. finally, we show that critical temperature and precipitation\\nsignatures may vary significantly from city to city, suggesting that the\\ninterplay between climate variables and dengue outbreaks is more complex than\\ngenerally appreciated.\\n)': False, 'contains(reproducing kernel hilbert space based estimation of systems of ordinary\\n  differential equations\\n  non-linear systems of differential equations have attracted the interest in\\nfields like system biology, ecology or biochemistry, due to their flexibility\\nand their ability to describe dynamical systems. despite the importance of such\\nmodels in many branches of science they have not been the focus of systematic\\nstatistical analysis until recently. in this work we propose a general approach\\nto estimate the parameters of systems of differential equations measured with\\nnoise. our methodology is based on the maximization of the penalized likelihood\\nwhere the system of differential equations is used as a penalty. to do so, we\\nuse a reproducing kernel hilbert space approach that allows to formulate the\\nestimation problem as an unconstrained numeric maximization problem easy to\\nsolve. the proposed method is tested with synthetically simulated data and it\\nis used to estimate the unobserved transcription factor cdar in steptomyes\\ncoelicolor using gene expression data of the genes it regulates.\\n)': False, 'contains(hierarchical structure and the prediction of missing links in networks\\n  networks have in recent years emerged as an invaluable tool for describing\\nand quantifying complex systems in many branches of science. recent studies\\nsuggest that networks often exhibit hierarchical organization, where vertices\\ndivide into groups that further subdivide into groups of groups, and so forth\\nover multiple scales. in many cases these groups are found to correspond to\\nknown functional units, such as ecological niches in food webs, modules in\\nbiochemical networks (protein interaction networks, metabolic networks, or\\ngenetic regulatory networks), or communities in social networks. here we\\npresent a general technique for inferring hierarchical structure from network\\ndata and demonstrate that the existence of hierarchy can simultaneously explain\\nand quantitatively reproduce many commonly observed topological properties of\\nnetworks, such as right-skewed degree distributions, high clustering\\ncoefficients, and short path lengths. we further show that knowledge of\\nhierarchical structure can be used to predict missing connections in partially\\nknown networks with high accuracy, and for more general network structures than\\ncompeting techniques. taken together, our results suggest that hierarchy is a\\ncentral organizing principle of complex networks, capable of offering insight\\ninto many network phenomena.\\n)': False, \"contains(addressing the invisible: street address generation for developing\\n  countries with deep learning\\n  more than half of the world's roads lack adequate street addressing systems.\\nlack of addresses is even more visible in daily lives of people in developing\\ncountries. we would like to object to the assumption that having an address is\\na luxury, by proposing a generative address design that maps the world in\\naccordance with streets. the addressing scheme is designed considering several\\ntraditional street addressing methodologies employed in the urban development\\nscenarios around the world. our algorithm applies deep learning to extract\\nroads from satellite images, converts the road pixel confidences into a road\\nnetwork, partitions the road network to find neighborhoods, and labels the\\nregions, roads, and address units using graph- and proximity-based algorithms.\\nwe present our results on a sample us city, and several developing cities,\\ncompare travel times of users using current ad hoc and new complete addresses,\\nand contrast our addressing solution to current industrial and open geocoding\\nalternatives.\\n)\": False, \"contains(model selection for simulator-based statistical models: a kernel\\n  approach\\n  we propose a novel approach to model selection for simulator-based\\nstatistical models. the proposed approach defines a mixture of candidate\\nmodels, and then iteratively updates the weight coefficients for those models\\nas well as the parameters in each model simultaneously; this is done by\\nrecursively applying bayes' rule, using the recently proposed kernel recursive\\nabc algorithm. the practical advantage of the method is that it can be used\\neven when a modeler lacks appropriate prior knowledge about the parameters in\\neach model. we demonstrate the effectiveness of the proposed approach with a\\nnumber of experiments, including model selection for dynamical systems in\\necology and epidemiology.\\n)\": False, 'contains(using machine learning for discovery in synoptic survey imaging\\n  modern time-domain surveys continuously monitor large swaths of the sky to\\nlook for astronomical variability. astrophysical discovery in such data sets is\\ncomplicated by the fact that detections of real transient and variable sources\\nare highly outnumbered by bogus detections caused by imperfect subtractions,\\natmospheric effects and detector artefacts. in this work we present a machine\\nlearning (ml) framework for discovery of variability in time-domain imaging\\nsurveys. our ml methods provide probabilistic statements, in near real time,\\nabout the degree to which each newly observed source is astrophysically\\nrelevant source of variable brightness. we provide details about each of the\\nanalysis steps involved, including compilation of the training and testing\\nsets, construction of descriptive image-based and contextual features, and\\noptimization of the feature subset and model tuning parameters. using a\\nvalidation set of nearly 30,000 objects from the palomar transient factory, we\\ndemonstrate a missed detection rate of at most 7.7% at our chosen\\nfalse-positive rate of 1% for an optimized ml classifier of 23 features,\\nselected to avoid feature correlation and over-fitting from an initial library\\nof 42 attributes. importantly, we show that our classification methodology is\\ninsensitive to mis-labelled training data up to a contamination of nearly 10%,\\nmaking it easier to compile sufficient training sets for accurate performance\\nin future surveys. this ml framework, if so adopted, should enable the\\nmaximization of scientific gain from future synoptic survey and enable fast\\nfollow-up decisions on the vast amounts of streaming data produced by such\\nexperiments.\\n)': False, 'contains(extreme learning machine for reduced order modeling of turbulent\\n  geophysical flows\\n  we investigate the application of artificial neural networks to stabilize\\nproper orthogonal decomposition based reduced order models for quasi-stationary\\ngeophysical turbulent flows. an extreme learning machine concept is introduced\\nfor computing an eddy-viscosity closure dynamically to incorporate the effects\\nof the truncated modes. we consider a four-gyre wind-driven ocean circulation\\nproblem as our prototype setting to assess the performance of the proposed\\ndata-driven approach. our framework provides a significant reduction in\\ncomputational time and effectively retains the dynamics of the full-order model\\nduring the forward simulation period beyond the training data set. furthermore,\\nwe show that the method is robust for larger choices of time steps and can be\\nused as an efficient and reliable tool for long time integration of general\\ncirculation models.\\n)': False, 'contains(towards identification of relevant variables in the observed aerosol\\n  optical depth bias between modis and aeronet observations\\n  measurements made by satellite remote sensing, moderate resolution imaging\\nspectroradiometer (modis), and globally distributed aerosol robotic network\\n(aeronet) are compared. comparison of the two datasets measurements for aerosol\\noptical depth values show that there are biases between the two data products.\\nin this paper, we present a general framework towards identifying relevant set\\nof variables responsible for the observed bias. we present a general framework\\nto identify the possible factors influencing the bias, which might be\\nassociated with the measurement conditions such as the solar and sensor zenith\\nangles, the solar and sensor azimuth, scattering angles, and surface\\nreflectivity at the various measured wavelengths, etc. specifically, we\\nperformed analysis for remote sensing aqua-land data set, and used machine\\nlearning technique, neural network in this case, to perform multivariate\\nregression between the ground-truth and the training data sets. finally, we\\nused mutual information between the observed and the predicted values as the\\nmeasure of similarity to identify the most relevant set of variables. the\\nsearch is brute force method as we have to consider all possible combinations.\\nthe computations involves a huge number crunching exercise, and we implemented\\nit by writing a job-parallel program.\\n)': False, \"contains(a deep learning approach for forecasting air pollution in south korea\\n  using lstm\\n  tackling air pollution is an imperative problem in south korea, especially in\\nurban areas, over the last few years. more specially, south korea has joined\\nthe ranks of the world's most polluted countries alongside with other asian\\ncapitals, such as beijing or delhi. much research is being conducted in\\nenvironmental science to evaluate the dangerous impact of particulate matters\\non public health. besides that, deterministic models of air pollutant behavior\\nare also generated; however, this is both complex and often inaccurate. on the\\ncontrary, deep recurrent neural network reveals potent potential on forecasting\\nout-comes of time-series data and has become more prevalent. this paper uses\\nrecurrent neural network (rnn) with long short-term memory units as a framework\\nfor leveraging knowledge from time-series data of air pollution and\\nmeteorological information in daegu, seoul, beijing, and shenyang.\\nadditionally, we use encoder-decoder model, which is similar to machine\\ncomprehension problems, as a crucial part of our prediction machine. finally,\\nwe investigate the prediction accuracy of various configurations. our\\nexperiments prevent the efficiency of integrating multiple layers of rnn on\\nprediction model when forecasting far timesteps ahead. this research is a\\nsignificant motivation for not only continuing researching on urban air quality\\nbut also help the government leverage that insight to enact beneficial policies\\n)\": False, 'contains(detecting comma-shaped clouds for severe weather forecasting using shape\\n  and motion\\n  meteorologists use shapes and movements of clouds in satellite images as\\nindicators of several major types of severe storms. satellite imaginary data\\nare in increasingly higher resolution, both spatially and temporally, making it\\nimpossible for humans to fully leverage the data in their forecast. automatic\\nsatellite imagery analysis methods that can find storm-related cloud patterns\\nas soon as they are detectable are in demand. we propose a machine learning and\\npattern recognition based approach to detect \"comma-shaped\" clouds in satellite\\nimages, which are specific cloud distribution patterns strongly associated with\\nthe cyclone formulation. in order to detect regions with the targeted movement\\npatterns, our method is trained on manually annotated cloud examples\\nrepresented by both shape and motion-sensitive features. sliding windows in\\ndifferent scales are used to ensure that dense clouds will be captured, and we\\nimplement effective selection rules to shrink the region of interest among\\nthese sliding windows. finally, we evaluate the method on a hold-out annotated\\ncomma-shaped cloud dataset and cross-match the results with recorded storm\\nevents in the severe weather database. the validated utility and accuracy of\\nour method suggest a high potential for assisting meteorologists in weather\\nforecasting.\\n)': False, 'contains(remote sensor design for visual recognition with convolutional neural networks\\n\\n-while deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. in particular, remote sensing systems are usually constructed to optimize sensing cost-quality trade-offs with respect to human image 9 interpretability. while some recent studies have explored remote 1 sensing system design as a function of simple computer vision 0 algorithm performance, there has been little work relating this 2 design to the state-of-the-art in computer vision: deep learning with convolutional neural networks. we develop experimental n systems to conduct this analysis, showing results with modern u deep learning algorithms and recent overhead image data. our j results are compared to standard image quality measurements 4 based on human visual perception, and we conclude not only 2 that machine and human interpretability differ significantly, but that computer vision performance is largely self-consistent across ] a range of disparate conditions. this research is presented as a vcornerstone for a new generation of sensor design systems which i focus on computer algorithm performance instead of human . s visual perception. s e index terms-remote sensing, convolutional neural network e (cnn), deep learning, transfer learning, satellite imagery, image [ system design. 1\\n)': False, 'contains(deep learning for semantic segmentation of remote sensing images with\\n  rich spectral content\\n  with the rapid development of remote sensing acquisition techniques, there is\\na need to scale and improve processing tools to cope with the observed increase\\nof both data volume and richness. among popular techniques in remote sensing,\\ndeep learning gains increasing interest but depends on the quality of the\\ntraining data. therefore, this paper presents recent deep learning approaches\\nfor fine or coarse land cover semantic segmentation estimation. various 2d\\narchitectures are tested and a new 3d model is introduced in order to jointly\\nprocess the spatial and spectral dimensions of the data. such a set of networks\\nenables the comparison of the different spectral fusion schemes. besides, we\\nalso assess the use of a \" noisy ground truth \" (i.e. outdated and low spatial\\nresolution labels) for training and testing the networks.\\n)': False, 'contains(multi-target regression via random linear target combinations\\n  multi-target regression is concerned with the simultaneous prediction of\\nmultiple continuous target variables based on the same set of input variables.\\nit arises in several interesting industrial and environmental application\\ndomains, such as ecological modelling and energy forecasting. this paper\\npresents an ensemble method for multi-target regression that constructs new\\ntarget variables via random linear combinations of existing targets. we discuss\\nthe connection of our approach with multi-label classification algorithms, in\\nparticular ra$k$el, which originally inspired this work, and a family of recent\\nmulti-label classification algorithms that involve output coding. experimental\\nresults on 12 multi-target datasets show that it performs significantly better\\nthan a strong baseline that learns a single model for each target using\\ngradient boosting and compares favourably to multi-objective random forest\\napproach, which is a state-of-the-art approach. the experiments further show\\nthat our approach improves more when stronger unconditional dependencies exist\\namong the targets.\\n)': False, 'contains(development of a forecasting and warning system on the ecological\\n  life-cycle of sunn pest\\n  we provide a machine learning solution that replaces the traditional methods\\nfor deciding the pesticide application time of sunn pest. we correlate climate\\ndata with phases of sunn pest in its life-cycle and decide whether the fields\\nshould be sprayed. our solution includes two groups of prediction models. the\\nfirst group contains decision trees that predict migration time of sunn pest\\nfrom winter quarters to wheat fields. the second group contains random forest\\nmodels that predict the nymphal stage percentages of sunn pest which is a\\ncriterion for pesticide application. we trained our models on four years of\\nclimate data which was collected from kir\\\\c{s}ehir and aksaray. the experiments\\nshow that our promised solution make correct predictions with high accuracies.\\n)': False, 'contains(combining spatial and telemetric features for learning animal movement\\n  models\\n  we introduce a new graphical model for tracking radio-tagged animals and\\nlearning their movement patterns. the model provides a principled way to\\ncombine radio telemetry data with an arbitrary set of userdefined, spatial\\nfeatures. we describe an efficient stochastic gradient algorithm for fitting\\nmodel parameters to data and demonstrate its effectiveness via asymptotic\\nanalysis and synthetic experiments. we also apply our model to real datasets,\\nand show that it outperforms the most popular radio telemetry software package\\nused in ecology. we conclude that integration of different data sources under a\\nsingle statistical framework, coupled with appropriate parameter and state\\nestimation procedures, produces both accurate location estimates and an\\ninterpretable statistical model of animal movement.\\n)': False, 'contains(gamma/hadron segregation for a ground based imaging atmospheric\\n  cherenkov telescope using machine learning methods: random forest leads\\n  a detailed case study of $\\\\gamma$-hadron segregation for a ground based\\natmospheric cherenkov telescope is presented. we have evaluated and compared\\nvarious supervised machine learning methods such as the random forest method,\\nartificial neural network, linear discriminant method, naive bayes\\nclassifiers,support vector machines as well as the conventional dynamic\\nsupercut method by simulating triggering events with the monte carlo method and\\napplied the results to a cherenkov telescope. it is demonstrated that the\\nrandom forest method is the most sensitive machine learning method for\\n$\\\\gamma$-hadron segregation.\\n)': False, 'contains(kernel embedding of maps for sequential bayesian inference: the\\n  variational mapping particle filter\\n  in this work, a novel sequential monte carlo filter is introduced which aims\\nat efficient sampling of high-dimensional state spaces with a limited number of\\nparticles. particles are pushed forward from the prior to the posterior density\\nusing a sequence of mappings that minimizes the kullback-leibler divergence\\nbetween the posterior and the sequence of intermediate densities. the sequence\\nof mappings represents a gradient flow. a key ingredient of the mappings is\\nthat they are embedded in a reproducing kernel hilbert space, which allows for\\na practical and efficient algorithm. the embedding provides a direct means to\\ncalculate the gradient of the kullback-leibler divergence leading to quick\\nconvergence using well-known gradient-based stochastic optimization algorithms.\\nevaluation of the method is conducted in the chaotic lorenz-63 system, the\\nlorenz-96 system, which is a coarse prototype of atmospheric dynamics, and an\\nepidemic model that describes cholera dynamics. no resampling is required in\\nthe mapping particle filter even for long recursive sequences. the number of\\neffective particles remains close to the total number of particles in all the\\nexperiments.\\n)': False, 'contains(conditioning of three-dimensional generative adversarial networks for\\n  pore and reservoir-scale models\\n  geostatistical modeling of petrophysical properties is a key step in modern\\nintegrated oil and gas reservoir studies. recently, generative adversarial\\nnetworks (gan) have been shown to be a successful method for generating\\nunconditional simulations of pore- and reservoir-scale models. this\\ncontribution leverages the differentiable nature of neural networks to extend\\ngans to the conditional simulation of three-dimensional pore- and\\nreservoir-scale models. based on the previous work of yeh et al. (2016), we use\\na content loss to constrain to the conditioning data and a perceptual loss\\nobtained from the evaluation of the gan discriminator network. the technique is\\ntested on the generation of three-dimensional micro-ct images of a ketton\\nlimestone constrained by two-dimensional cross-sections, and on the simulation\\nof the maules creek alluvial aquifer constrained by one-dimensional sections.\\nour results show that gans represent a powerful method for sampling conditioned\\npore and reservoir samples for stochastic reservoir evaluation workflows.\\n)': False, 'contains(adoption of machine learning techniques in ecology and earth science\\n\\nbackground\\nthe natural sciences, such as ecology and earth science, study complex interactions\\nbetween biotic and abiotic systems in order to understand and make predictions.\\nmachinelearning-based methods have an advantage over traditional statistical methods in studying\\nthese systems because the former do not impose unrealistic assumptions (such as\\nlinearity), are capable of inferring missing data, and can reduce long-term expert\\nannotation burden. thus, a wider adoption of machine learning methods in ecology and\\nearth science has the potential to greatly accelerate the pace and quality of science.\\ndespite these advantages, the full potential of machine learning techniques in ecology and\\nearth science has not be fully realized.\\n)': False, 'contains(automatic classification of k2 pulsating stars using machine learning\\n  techniques\\n  the second mission of the nasa kepler satellite, k2, has collected hundreds\\nof thousands of lightcurves for stars close to the ecliptic plane. this new\\nsample could increase the number of known pulsating stars and then improve our\\nunderstanding of those stars. for the moment only a few stars have been\\nproperly classified and published. in this work, we present a method to\\nautomaticly classify k2 pulsating stars using a machine learning technique\\ncalled random forest. the objective is to sort out the stars in four classes:\\nred giant (rg), main-sequence solar-like stars (sl), classical pulsators (puls)\\nand other. to do this we use the effective temperatures and the luminosities of\\nthe stars as well as the fliper features, that measures the amount of power\\ncontained in the power spectral density. the classifier now retrieves the right\\nclassification for more than 80% of the stars.\\n)': False, 'contains(automated monitoring cropland using remote sensing data: challenges and\\n  opportunities for machine learning\\n  this paper provides an overview of how recent advances in machine learning\\nand the availability of data from earth observing satellites can dramatically\\nimprove our ability to automatically map croplands over long period and over\\nlarge regions. it discusses three applications in the domain of crop monitoring\\nwhere ml approaches are beginning to show great promise. for each application,\\nit highlights machine learning challenges, proposed approaches, and recent\\nresults. the paper concludes with discussion of major challenges that need to\\nbe addressed before ml approaches will reach their full potential for this\\nproblem of great societal relevance.\\n)': False, 'contains(meta-modeling game for deriving theoretical-consistent,\\n  micro-structural-based traction-separation laws via deep reinforcement\\n  learning\\n  this paper presents a new meta-modeling framework to employ deep\\nreinforcement learning (drl) to generate mechanical constitutive models for\\ninterfaces. the constitutive models are conceptualized as information flow in\\ndirected graphs. the process of writing constitutive models are simplified as a\\nsequence of forming graph edges with the goal of maximizing the model score (a\\nfunction of accuracy, robustness and forward prediction quality). thus\\nmeta-modeling can be formulated as a markov decision process with well-defined\\nstates, actions, rules, objective functions, and rewards. by using neural\\nnetworks to estimate policies and state values, the computer agent is able to\\nefficiently self-improve the constitutive model it generated through\\nself-playing, in the same way alphago zero (the algorithm that outplayed the\\nworld champion in the game of go)improves its gameplay. our numerical examples\\nshow that this automated meta-modeling framework not only produces models which\\noutperform existing cohesive models on benchmark traction-separation data but\\nis also capable of detecting hidden mechanisms among micro-structural features\\nand incorporating them in constitutive models to improve the forward prediction\\naccuracy, which are difficult tasks to do manually.\\n)': False, 'contains(parametrization of stochastic inputs using generative adversarial\\n  networks with application in geology\\n  we investigate artificial neural networks as a parametrization tool for\\nstochastic inputs in numerical simulations. we address parametrization from the\\npoint of view of emulating the data generating process, instead of explicitly\\nconstructing a parametric form to preserve predefined statistics of the data.\\nthis is done by training a neural network to generate samples from the data\\ndistribution using a recent deep learning technique called generative\\nadversarial networks. by emulating the data generating process, the relevant\\nstatistics of the data are replicated. the method is assessed in subsurface\\nflow problems, where effective parametrization of underground properties such\\nas permeability is important due to the high dimensionality and presence of\\nhigh spatial correlations. we experiment with realizations of binary\\nchannelized subsurface permeability and perform uncertainty quantification and\\nparameter estimation. results show that the parametrization using generative\\nadversarial networks is very effective in preserving visual realism as well as\\nhigh order statistics of the flow responses, while achieving a dimensionality\\nreduction of two orders of magnitude.\\n)': False, 'contains(expanding the horizon of automated metamaterials discovery via quantum\\n  annealing\\n  complexity of materials designed by machine learning is currently limited by\\nthe inefficiency of classical computers. we show how quantum annealing can be\\nincorporated into automated materials discovery and conduct a\\nproof-of-principle study on designing complex thermofunctional metamaterials\\nconsisting of sio2, sic, and poly(methyl methacrylate). empirical computing\\ntime of our quantum-classical hybrid algorithm involving a factorization\\nmachine, a rigorous coupled wave analysis, and a d-wave 2000q quantum annealer\\nwas insensitive to the problem size, while a classical counterpart experienced\\nrapid increase. our method was used to design complex structures of wavelength\\nselective radiators showing much better concordance with the thermal\\natmospheric transparency window in comparison to existing human-designed\\nalternatives. our result shows that quantum annealing provides scientists\\ngigantic computational power that may change how materials are designed.\\n)': False, 'contains(deep learning approach in automatic iceberg - ship detection with sar\\n  remote sensing data\\n  deep learning is gaining traction with geophysics community to understand\\nsubsurface structures, such as fault detection or salt body in seismic data.\\nthis study describes using deep learning method for iceberg or ship recognition\\nwith synthetic aperture radar (sar) data. drifting icebergs pose a potential\\nthreat to activities offshore around the arctic, including for both ship\\nnavigation and oil rigs. advancement of satellite imagery using\\nweather-independent cross-polarized radar has enabled us to monitor and\\ndelineate icebergs and ships, however a human component is needed to classify\\nthe images. here we present transfer learning, a convolutional neural network\\n(cnn) designed to work with a limited training data and features, while\\ndemonstrating its effectiveness in this problem. key aspect of the approach is\\ndata augmentation and stacking of multiple outputs, resulted in a significant\\nboost in accuracy (logarithmic score of 0.1463). this algorithm has been tested\\nthrough participation at the statoil/c-core kaggle competition.\\n)': False, \"contains(optimizing the human-machine partnership with zooniverse\\n  over the past decade, citizen science has become a proven method of\\ndistributed data analysis, enabling research teams from diverse domains to\\nsolve problems involving large quantities of data with complexity levels which\\nrequire human pattern recognition capabilities. with over 120 projects built\\nreaching nearly 1.7 million volunteers, the zooniverse.org platform has led the\\nway in the application of citizen science as a method for closing the big data\\nanalysis gap. since the launch in 2007 of the galaxy zoo project, the\\nzooniverse platform has enabled significant contributions across many\\ndisciplines; e.g., in ecology, humanities, and astronomy. citizen science as an\\napproach to big data combines the twin advantages of the ability to scale\\nanalysis to the size of modern datasets with the ability of humans to make\\nserendipitous discoveries. to cope with the larger datasets looming on the\\nhorizon such as astronomy's large synoptic survey telescope (lsst) or the 100's\\nof tb from ecology projects annually, zooniverse has been researching a system\\ndesign that is optimized for efficiency in task assignment and incorporating\\nhuman and machine classifiers into the classification engine. by making\\nefficient use of smart task assignment and the combination of human and machine\\nclassifiers, we can achieve greater accuracy and flexibility than has been\\npossible to date. we note that creating the most efficient system must consider\\nhow best to engage and retain volunteers as well as make the most efficient use\\nof their classifications. our work thus focuses on understanding the factors\\nthat optimize efficiency of the combined human-machine system. this paper\\nsummarizes some of our research to date on integration of machine learning with\\nzooniverse, while also describing new infrastructure developed on the\\nzooniverse platform to carry out this research.\\n)\": False, 'contains(dictionary learning of sound speed profiles\\n  to provide constraints on their inversion, ocean sound speed profiles (ssps)\\noften are modeled using empirical orthogonal functions (eofs). however, this\\nregularization, which uses the leading order eofs with a minimum-energy\\nconstraint on their coefficients, often yields low resolution ssp estimates. in\\nthis paper, it is shown that dictionary learning, a form of unsupervised\\nmachine learning, can improve ssp resolution by generating a dictionary of\\nshape functions for sparse processing (e.g. compressive sensing) that optimally\\ncompress ssps; both minimizing the reconstruction error and the number of\\ncoefficients. these learned dictionaries (lds) are not constrained to be\\northogonal and thus, fit the given signals such that each signal example is\\napproximated using few ld entries. here, lds describing ssp observations from\\nthe hf-97 experiment and the south china sea are generated using the k-svd\\nalgorithm. these lds better explain ssp variability and require fewer\\ncoefficients than eofs, describing much of the variability with one\\ncoefficient. thus, lds improve the resolution of ssp estimates with negligible\\ncomputational burden.\\n)': False, 'contains(identifying typical mg ii flare spectra using machine learning\\n  iris performs solar observations over a large range of atmospheric heights,\\nincluding the chromosphere where the majority of flare energy is dissipated.\\nthe strong mg ii h&k spectral lines are capable of providing excellent\\natmospheric diagnostics, but have not been fully utilized for flaring\\natmospheres. we aim to investigate whether the physics of the chromosphere is\\nidentical for all flare observations by analyzing if there are certain spectra\\nthat occur in all flares. to achieve this, we automatically analyze hundreds of\\nthousands of mg ii h&k line profiles from a set of 33 flares, and use a machine\\nlearning technique which we call supervised hierarchical k-means, to cluster\\nall profile shapes. we identify a single peaked mg ii profile, in contrast to\\nthe double-peaked quiet sun profiles, appearing in every flare. additionally,\\nwe find extremely broad profiles with characteristic blue shifted central\\nreversals appearing at the front of fast-moving flare ribbons. these profiles\\noccur during the impulsive phase of the flare, and we present results of their\\ntemporal and spatial correlation with non-thermal hard x-ray signatures,\\nsuggesting that flare-accelerated electrons play an important role in the\\nformation of these profiles. the ratio of the integrated mg ii h&k lines can\\nalso serve as an opacity diagnostic, and we find higher opacities during each\\nflare maximum. our study shows that machine learning is a powerful tool for\\nlarge scale statistical solar analyses.\\n)': False, 'contains(earth system modeling 2.0: a blueprint for models that learn from\\n  observations and targeted high-resolution simulations\\n  climate projections continue to be marred by large uncertainties, which\\noriginate in processes that need to be parameterized, such as clouds,\\nconvection, and ecosystems. but rapid progress is now within reach. new\\ncomputational tools and methods from data assimilation and machine learning\\nmake it possible to integrate global observations and local high-resolution\\nsimulations in an earth system model (esm) that systematically learns from\\nboth. here we propose a blueprint for such an esm. we outline how\\nparameterization schemes can learn from global observations and targeted\\nhigh-resolution simulations, for example, of clouds and convection, through\\nmatching low-order statistics between esms, observations, and high-resolution\\nsimulations. we illustrate learning algorithms for esms with a simple dynamical\\nsystem that shares characteristics of the climate system; and we discuss the\\nopportunities the proposed framework presents and the challenges that remain to\\nrealize it.\\n)': False, 'contains(bin-ct: urban waste collection based in predicting the container fill\\n  level\\n  the fast demographic growth, together with the concentration of the\\npopulation in cities and the increasing amount of daily waste, are factors that\\npush to the limit the ability of waste assimilation by nature. therefore, we\\nneed technological means to make an optimal management of the waste collection\\nprocess, which represents 70% of the operational cost in waste treatment. in\\nthis article, we present a free intelligent software system, based on\\ncomputational learning algorithms, which plans the best routes for waste\\ncollection supported by past (historical) and future (predictions) data.\\n  the objective of the system is the cost reduction of the waste collection\\nservice by means of the minimization in distance traveled by any truck to\\ncollect a container, hence the fuel consumption. at the same time the quality\\nof service to the citizen is increased avoiding the annoying overflows of\\ncontainers thanks to the accurate fill level predictions performed by bin-ct.\\nin this article we show the features of our software system, illustrating it\\noperation with a real case study of a spanish city. we conclude that the use of\\nbin-ct avoids unnecessary visits to containers, reduces the distance traveled\\nto collect a container and therefore we obtain a reduction of total costs and\\nharmful emissions thrown to the atmosphere.\\n)': False, 'contains(predicting property damage from tornadoes with zero-inflated neural\\n  networks\\n  tornadoes are the most violent of all atmospheric storms. in a typical year,\\nthe united states experiences hundreds of tornadoes with associated damages on\\nthe order of one billion dollars. community preparation and resilience would\\nbenefit from accurate predictions of these economic losses, particularly as\\npopulations in tornado-prone areas increase in density and extent. here, we use\\na zero-inflated modeling approach and artificial neural networks to predict\\ntornado-induced property damage using publicly available data. we developed a\\nneural network that predicts whether a tornado will cause property damage\\n(out-of-sample accuracy = 0.821 and area under the receiver operating\\ncharacteristic curve, auroc, = 0.872). conditional on a tornado causing damage,\\nanother neural network predicts the amount of damage (out-of-sample mean\\nsquared error = 0.0918 and r2 = 0.432). when used together, these two models\\nfunction as a zero-inflated log-normal regression with hidden layers. from the\\nbest-performing models, we provide static and interactive gridded maps of\\nmonthly predicted probabilities of damage and property damages for the year\\n2019. two primary weaknesses include (1) model fitting requires log-scale data\\nwhich leads to large natural-scale residuals and (2) beginning tornado\\ncoordinates were utilized rather than tornado paths. ultimately, this is the\\nfirst known study to directly model tornado-induced property damages, and all\\ndata, code, and tools are publicly available. the predictive capacity of this\\nmodel along with an interactive interface may provide an opportunity for\\nscience-informed tornado disaster planning.\\n)': False, 'contains(multi-resolution neural networks for tracking seismic horizons from few\\n  training images\\n  detecting a specific horizon in seismic images is a valuable tool for\\ngeological interpretation. because hand-picking the locations of the horizon is\\na time-consuming process, automated computational methods were developed\\nstarting three decades ago. older techniques for such picking include\\ninterpolation of control points however, in recent years neural networks have\\nbeen used for this task. until now, most networks trained on small patches from\\nlarger images. this limits the networks ability to learn from large-scale\\ngeologic structures. moreover, currently available networks and training\\nstrategies require label patches that have full and continuous annotations,\\nwhich are also time-consuming to generate.\\n  we propose a projected loss-function for training convolutional networks with\\na multi-resolution structure, including variants of the u-net. our networks\\nlearn from a small number of large seismic images without creating patches. the\\nprojected loss-function enables training on labels with just a few annotated\\npixels and has no issue with the other unknown label pixels. training uses all\\ndata without reserving some for validation. only the labels are split into\\ntraining/testing. contrary to other work on horizon tracking, we train the\\nnetwork to perform non-linear regression, and not classification. as such, we\\npropose labels as the convolution of a gaussian kernel and the known horizon\\nlocations that indicate uncertainty in the labels. the network output is the\\nprobability of the horizon location. we demonstrate the proposed computational\\ningredients on two different datasets, for horizon extrapolation and\\ninterpolation. we show that the predictions of our methodology are accurate\\neven in areas far from known horizon locations because our learning strategy\\nexploits all data in large seismic images.\\n)': False, \"contains(reproducible floating-point aggregation in rdbmss\\n  industry-grade database systems are expected to produce the same result if\\nthe same query is repeatedly run on the same input. however, the numerous\\nsources of non-determinism in modern systems make reproducible results\\ndifficult to achieve. this is particularly true if floating-point numbers are\\ninvolved, where the order of the operations affects the final result.\\n  as part of a larger effort to extend database engines with data\\nrepresentations more suitable for machine learning and scientific applications,\\nin this paper we explore the problem of making relational groupby over\\nfloating-point formats bit-reproducible, i.e., ensuring any execution of the\\noperator produces the same result up to every single bit. to that aim, we first\\npropose a numeric data type that can be used as drop-in replacement for other\\nnumber formats and is---unlike standard floating-point formats---associative.\\nwe use this data type to make state-of-the-art groupby operators reproducible,\\nbut this approach incurs a slowdown between 4x and 12x compared to the same\\noperator using conventional database number formats. we thus explore how to\\nmodify existing groupby algorithms to make them bit-reproducible and efficient.\\nby using vectorized summation on batches and carefully balancing batch size,\\ncache footprint, and preprocessing costs, we are able to reduce the slowdown\\ndue to reproducibility to a factor between 1.9x and 2.4x of aggregation in\\nisolation and to a mere 2.7% of end-to-end query performance even on\\naggregation-intensive queries in monetdb. we thereby provide a solid basis for\\nsupporting more reproducible operations directly in relational engines.\\n  this document is an extended version of an article currently in print for the\\nproceedings of icde'18 with the same title and by the same authors. the main\\nadditions are more implementation details and experiments.\\n)\": False, 'contains(band selection from hyperspectral images using attention-based\\n  convolutional neural networks\\n  this paper introduces new attention-based convolutional neural networks for\\nselecting bands from hyperspectral images. the proposed approach re-uses\\nconvolutional activations at different depths, identifying the most informative\\nregions of the spectrum with the help of gating mechanisms. our attention\\ntechniques are modular and easy to implement, and they can be seamlessly\\ntrained end-to-end using gradient descent. our rigorous experiments showed that\\ndeep models equipped with the attention mechanism deliver high-quality\\nclassification, and repeatedly identify significant bands in the training data,\\npermitting the creation of refined and extremely compact sets that retain the\\nmost meaningful features.\\n)': False, 'contains(multi-view kernels for low-dimensional modeling of seismic events\\n  the problem of learning from seismic recordings has been studied for years.\\nthere is a growing interest in developing automatic mechanisms for identifying\\nthe properties of a seismic event. one main motivation is the ability have a\\nreliable identification of man-made explosions. the availability of multiple\\nhigh-dimensional observations has increased the use of machine learning\\ntechniques in a variety of fields. in this work, we propose to use a\\nkernel-fusion based dimensionality reduction framework for generating\\nmeaningful seismic representations from raw data. the proposed method is tested\\non 2023 events that were recorded in israel and in jordan. the method achieves\\npromising results in classification of event type as well as in estimating the\\nlocation of the event. the proposed fusion and dimensionality reduction tools\\nmay be applied to other types of geophysical data.\\n)': False, 'contains(rediscovery of good-turing estimators via bayesian nonparametrics\\n  the problem of estimating discovery probabilities originated in the context\\nof statistical ecology, and in recent years it has become popular due to its\\nfrequent appearance in challenging applications arising in genetics,\\nbioinformatics, linguistics, designs of experiments, machine learning, etc. a\\nfull range of statistical approaches, parametric and nonparametric as well as\\nfrequentist and bayesian, has been proposed for estimating discovery\\nprobabilities. in this paper we investigate the relationships between the\\ncelebrated good-turing approach, which is a frequentist nonparametric approach\\ndeveloped in the 1940s, and a bayesian nonparametric approach recently\\nintroduced in the literature. specifically, under the assumption of a two\\nparameter poisson-dirichlet prior, we show that bayesian nonparametric\\nestimators of discovery probabilities are asymptotically equivalent, for a\\nlarge sample size, to suitably smoothed good-turing estimators. as a by-product\\nof this result, we introduce and investigate a methodology for deriving exact\\nand asymptotic credible intervals to be associated with the bayesian\\nnonparametric estimators of discovery probabilities. the proposed methodology\\nis illustrated through a comprehensive simulation study and the analysis of\\nexpressed sequence tags data generated by sequencing a benchmark complementary\\ndna library.\\n)': False, 'contains(learning to interpret satellite images in global scale using wikipedia\\n  despite recent progress in computer vision, finegrained interpretation of\\nsatellite images remains challenging because of a lack of labeled training\\ndata. to overcome this limitation, we construct a novel dataset called\\nwikisatnet by pairing georeferenced wikipedia articles with satellite imagery\\nof their corresponding locations. we then propose two strategies to learn\\nrepresentations of satellite images by predicting properties of the\\ncorresponding articles from the images. leveraging this new multi-modal\\ndataset, we can drastically reduce the quantity of human-annotated labels and\\ntime required for downstream tasks. on the recently released fmow dataset, our\\npre-training strategies can boost the performance of a model pre-trained on\\nimagenet by up to 4:5% in f1 score.\\n)': False, \"contains(decentralized flood forecasting using deep neural networks\\n  predicting flood for any location at times of extreme storms is a\\nlongstanding problem that has utmost importance in emergency management.\\nconventional methods that aim to predict water levels in streams use advanced\\nhydrological models still lack of giving accurate forecasts everywhere. this\\nstudy aims to explore artificial deep neural networks' performance on flood\\nprediction. while providing models that can be used in forecasting stream\\nstage, this paper presents a dataset that focuses on the connectivity of data\\npoints on river networks. it also shows that neural networks can be very\\nhelpful in time-series forecasting as in flood events, and support improving\\nexisting models through data assimilation.\\n)\": False, 'contains(development and analysis of a bayesian water balance model for large\\n  lake systems\\n  water balance models (wbms) are often employed to understand regional\\nhydrologic cycles over various time scales. most wbms, however, are\\nphysically-based, and few employ state-of-the-art statistical methods to\\nreconcile independent input measurement uncertainty and bias. further, few wbms\\nexist for large lakes, and most large lake wbms perform additive accounting,\\nwith minimal consideration towards input data uncertainty. here, we introduce a\\nframework for improving a previously developed large lake statistical water\\nbalance model (l2swbm). focusing on the water balances of lakes superior and\\nmichigan-huron, we demonstrate our new analytical framework, identifying\\nl2swbms from 26 alternatives that adequately close the water balance of the\\nlakes with satisfactory computation times compared with the prototype model. we\\nexpect our new framework will be used to develop water balance models for other\\nlakes around the world.\\n)': False, 'contains(bayesian learning of conditional kernel mean embeddings for automatic\\n  likelihood-free inference\\n  in likelihood-free settings where likelihood evaluations are intractable,\\napproximate bayesian computation (abc) addresses the formidable inference task\\nto discover plausible parameters of simulation programs that explain the\\nobservations. however, they demand large quantities of simulation calls.\\ncritically, hyperparameters that determine measures of simulation discrepancy\\ncrucially balance inference accuracy and sample efficiency, yet are difficult\\nto tune. in this paper, we present kernel embedding likelihood-free inference\\n(kelfi), a holistic framework that automatically learns model hyperparameters\\nto improve inference accuracy given limited simulation budget. by leveraging\\nlikelihood smoothness with conditional mean embeddings, we nonparametrically\\napproximate likelihoods and posteriors as surrogate densities and sample from\\nclosed-form posterior mean embeddings, whose hyperparameters are learned under\\nits approximate marginal likelihood. our modular framework demonstrates\\nimproved accuracy and efficiency on challenging inference problems in ecology.\\n)': False, 'contains(efficient statistical classification of satellite measurements\\n  supervised statistical classification is a vital tool for satellite image\\nprocessing. it is useful not only when a discrete result, such as feature\\nextraction or surface type, is required, but also for continuum retrievals by\\ndividing the quantity of interest into discrete ranges. because of the high\\nresolution of modern satellite instruments and because of the requirement for\\nreal-time processing, any algorithm has to be fast to be useful. here we\\ndescribe an algorithm based on kernel estimation called adaptive gaussian\\nfiltering that incorporates several innovations to produce superior efficiency\\nas compared to three other popular methods: k-nearest-neighbour (knn), learning\\nvector quantization (lvq) and support vector machines (svm). this efficiency is\\ngained with no compromises: accuracy is maintained, while estimates of the\\nconditional probabilities are returned. these are useful not only to gauge the\\naccuracy of an estimate in the absence of its true value, but also to\\nre-calibrate a retrieved image and as a proxy for a discretized continuum\\nvariable. the algorithm is demonstrated and compared with the other three on a\\npair of synthetic test classes and to map the waterways of the netherlands.\\nsoftware may be found at: http://libagf.sourceforge.net.\\n)': False, 'contains(application of different simulated spectral data and machine learning to\\n  estimate the chlorophyll $a$ concentration of several inland waters\\n  water quality is of great importance for humans and for the environment and\\nhas to be monitored continuously. it is determinable through proxies such as\\nthe chlorophyll $a$ concentration, which can be monitored by remote sensing\\ntechniques. this study focuses on the trade-off between the spatial and the\\nspectral resolution of six simulated satellite-based data sets when estimating\\nthe chlorophyll $a$ concentration with supervised machine learning models. the\\ninitial dataset for the spectral simulation of the satellite missions contains\\nspectrometer data and measured chlorophyll $a$ concentration of 13 different\\ninland waters. focusing on the regression performance, it appears that the\\nmachine learning models achieve almost as good results with the simulated\\nsentinel data as with the simulated hyperspectral data. regarding the\\napplicability, the sentinel 2 mission is the best choice for small inland\\nwaters due to its high spatial and temporal resolution in combination with a\\nsuitable spectral resolution.\\n)': False, 'contains(forecasting high-dimensional dynamics exploiting suboptimal embeddings\\n  delay embedding---a method for reconstructing dynamical systems by delay\\ncoordinates---is widely used to forecast nonlinear time series as a model-free\\napproach. when multivariate time series are observed, several existing\\nframeworks can be applied to yield a single forecast combining multiple\\nforecasts derived from various embeddings. however, the performance of these\\nframeworks is not always satisfactory because they randomly select embeddings\\nor use brute force and do not consider the diversity of the embeddings to\\ncombine. herein, we develop a forecasting framework that overcomes these\\nexisting problems. the framework exploits various \"suboptimal embeddings\"\\nobtained by minimizing the in-sample error via combinatorial optimization. the\\nframework achieves the best results among existing frameworks for sample toy\\ndatasets and a real-world flood dataset. we show that the framework is\\napplicable to a wide range of data lengths and dimensions. therefore, the\\nframework can be applied to various fields such as neuroscience, ecology,\\nfinance, fluid dynamics, weather, and disaster prevention.\\n)': False, \"contains(svm-based sea-surface small target detection: a\\n  false-alarm-rate-controllable approach\\n  in this letter, we consider the varying detection environments to address the\\nproblem of detecting small targets within sea clutter. we first extract three\\nsimple yet practically discriminative features from the returned signals in the\\ntime and frequency domains and then fuse them into a 3-d feature space. based\\non the constructed space, we then adopt and elegantly modify the support vector\\nmachine (svm) to design a learning-based detector that enfolds the false alarm\\nrate (far). most importantly, our proposed detector can flexibly control the\\nfar by simply adjusting two introduced parameters, which facilitates to\\nregulate detector's sensitivity to the outliers incurred by the sea spikes and\\nto fairly evaluate the performance of different detection algorithms.\\nexperimental results demonstrate that our proposed detector significantly\\nimproves the detection probability over several existing classical detectors in\\nboth low signal to clutter ratio (scr) (up to 58%) and low far (up to 40%)\\ncases.\\n)\": False, 'contains(a machine learns to predict the stability of tightly packed planetary\\n  systems\\n  the requirement that planetary systems be dynamically stable is often used to\\nvet new discoveries or set limits on unconstrained masses or orbital elements.\\nthis is typically carried out via computationally expensive n-body simulations.\\nwe show that characterizing the complicated and multi-dimensional stability\\nboundary of tightly packed systems is amenable to machine learning methods. we\\nfind that training an xgboost machine learning algorithm on physically\\nmotivated features yields an accurate classifier of stability in packed\\nsystems. on the stability timescale investigated ($10^7$ orbits), it is 3\\norders of magnitude faster than direct n-body simulations. optimized machine\\nlearning classifiers for dynamical stability may thus prove useful across the\\ndiscipline, e.g., to characterize the exoplanet sample discovered by the\\nupcoming transiting exoplanet survey satellite (tess). this proof of concept\\nmotivates investing computational resources to train algorithms capable of\\npredicting stability over longer timescales and over broader regions of phase\\nspace.\\n)': False, 'contains(a data-driven approach for accurate rainfall prediction\\n  in recent years, there has been growing interest in using precipitable water\\nvapor (pwv) derived from global positioning system (gps) signal delays to\\npredict rainfall. however, the occurrence of rainfall is dependent on a myriad\\nof atmospheric parameters. this paper proposes a systematic approach to analyze\\nvarious parameters that affect precipitation in the atmosphere. different\\nground-based weather features like temperature, relative humidity, dew point,\\nsolar radiation, pwv along with seasonal and diurnal variables are identified,\\nand a detailed feature correlation study is presented. while all features play\\na significant role in rainfall classification, only a few of them, such as pwv,\\nsolar radiation, seasonal and diurnal features, stand out for rainfall\\nprediction. based on these findings, an optimum set of features are used in a\\ndata-driven machine learning algorithm for rainfall prediction. the\\nexperimental evaluation using a four-year (2012-2015) database shows a true\\ndetection rate of 80.4%, a false alarm rate of 20.3%, and an overall accuracy\\nof 79.6%. compared to the existing literature, our method significantly reduces\\nthe false alarm rates.\\n)': False, 'contains(validating hyperspectral image segmentation\\n  hyperspectral satellite imaging attracts enormous research attention in the\\nremote sensing community, hence automated approaches for precise segmentation\\nof such imagery are being rapidly developed. in this letter, we share our\\nobservations on the strategy for validating hyperspectral image segmentation\\nalgorithms currently followed in the literature, and show that it can lead to\\nover-optimistic experimental insights. we introduce a new routine for\\ngenerating segmentation benchmarks, and use it to elaborate ready-to-use\\nhyperspectral training-test data partitions. they can be utilized for fair\\nvalidation of new and existing algorithms without any training-test data\\nleakage.\\n)': False, 'contains(deep learning for scientific inference from geophysical data: the\\n  madden-julian oscillation as a test case\\n  deep learning can recognize complex geophysical phenomena by inferring which\\nvariables are important for their identification and understanding their\\nspatial characteristics.\\n  we use a particular mode of multi-scale tropical atmospheric variability, the\\nmadden-julian oscillation (mjo), to study the capabilities of deep learning, a\\nform of artificial intelligence and machine learning, in identifying spatial\\ngeophysical phenomena. the mjo is characterized by its spatial and temporal\\nevolution of cloud patterns, and an extensive body of literature has examined\\nits defining characteristics. by applying a convolutional neural network (cnn),\\na type of deep learning model, to the task of identifying the state of the mjo,\\nwe show that deep learning can correctly identify geophysical phenomena by\\n\"learning\" the variables and spatial patterns important to their evolution. in\\na broader sense, these findings suggest that deep learning models are\\ninterpretable and viable for scientific inference in geoscientific\\napplications.\\n)': False, 'contains(a data-driven approach to detecting precipitation from meteorological\\n  sensor data\\n  precipitation is dependent on a myriad of atmospheric conditions. in this\\npaper, we study how certain atmospheric parameters impact the occurrence of\\nrainfall. we propose a data-driven, machine-learning based methodology to\\ndetect precipitation using various meteorological sensor data. our approach\\nachieves a true detection rate of 87.4% and a moderately low false alarm rate\\nof 32.2%.\\n)': False, 'contains(matrix completion with variational graph autoencoders: application in\\n  hyperlocal air quality inference\\n  inferring air quality from a limited number of observations is an essential\\ntask for monitoring and controlling air pollution. existing inference methods\\ntypically use low spatial resolution data collected by fixed monitoring\\nstations and infer the concentration of air pollutants using additional types\\nof data, e.g., meteorological and traffic information. in this work, we focus\\non street-level air quality inference by utilizing data collected by mobile\\nstations. we formulate air quality inference in this setting as a graph-based\\nmatrix completion problem and propose a novel variational model based on graph\\nconvolutional autoencoders. our model captures effectively the spatio-temporal\\ncorrelation of the measurements and does not depend on the availability of\\nadditional information apart from the street-network topology. experiments on a\\nreal air quality dataset, collected with mobile stations, shows that the\\nproposed model outperforms state-of-the-art approaches.\\n)': False, 'contains(land cover classification via multi-temporal spatial data by recurrent\\n  neural networks\\n  nowadays, modern earth observation programs produce huge volumes of satellite\\nimages time series (sits) that can be useful to monitor geographical areas\\nthrough time. how to efficiently analyze such kind of information is still an\\nopen question in the remote sensing field. recently, deep learning methods\\nproved suitable to deal with remote sensing data mainly for scene\\nclassification (i.e. convolutional neural networks - cnns - on single images)\\nwhile only very few studies exist involving temporal deep learning approaches\\n(i.e recurrent neural networks - rnns) to deal with remote sensing time series.\\nin this letter we evaluate the ability of recurrent neural networks, in\\nparticular the long-short term memory (lstm) model, to perform land cover\\nclassification considering multi-temporal spatial data derived from a time\\nseries of satellite images. we carried out experiments on two different\\ndatasets considering both pixel-based and object-based classification. the\\nobtained results show that recurrent neural networks are competitive compared\\nto state-of-the-art classifiers, and may outperform classical approaches in\\npresence of low represented and/or highly mixed classes. we also show that\\nusing the alternative feature representation generated by lstm can improve the\\nperformances of standard classifiers.\\n)': False, \"contains(from satellite imagery to disaster insights\\n  the use of satellite imagery has become increasingly popular for disaster\\nmonitoring and response. after a disaster, it is important to prioritize rescue\\noperations, disaster response and coordinate relief efforts. these have to be\\ncarried out in a fast and efficient manner since resources are often limited in\\ndisaster-affected areas and it's extremely important to identify the areas of\\nmaximum damage. however, most of the existing disaster mapping efforts are\\nmanual which is time-consuming and often leads to erroneous results. in order\\nto address these issues, we propose a framework for change detection using\\nconvolutional neural networks (cnn) on satellite images which can then be\\nthresholded and clustered together into grids to find areas which have been\\nmost severely affected by a disaster. we also present a novel metric called\\ndisaster impact index (dii) and use it to quantify the impact of two natural\\ndisasters - the hurricane harvey flood and the santa rosa fire. our framework\\nachieves a top f1 score of 81.2% on the gridded flood dataset and 83.5% on the\\ngridded fire dataset.\\n)\": False, 'contains(soil texture classification with 1d convolutional neural networks based\\n  on hyperspectral data\\n  soil texture is important for many environmental processes. in this paper, we\\nstudy the classification of soil texture based on hyperspectral data. we\\ndevelop and implement three 1-dimensional (1d) convolutional neural networks\\n(cnn): the lucascnn, the lucasresnet which contains an identity block as\\nresidual network, and the lucascoordconv with an additional coordinates layer.\\nfurthermore, we modify two existing 1d cnn approaches for the presented\\nclassification task. the code of all five cnn approaches is available on github\\n(riese, 2019). we evaluate the performance of the cnn approaches and compare\\nthem to a random forest classifier. thereby, we rely on the freely available\\nlucas topsoil dataset. the cnn approach with the least depth turns out to be\\nthe best performing classifier. the lucascoordconv achieves the best\\nperformance regarding the average accuracy. in future work, we can further\\nenhance the introduced lucascnn, lucasresnet and lucascoordconv and include\\nadditional variables of the rich lucas dataset.\\n)': False, 'contains(a deep learning approach for population estimation from satellite\\n  imagery\\n  knowing where people live is a fundamental component of many decision making\\nprocesses such as urban development, infectious disease containment, evacuation\\nplanning, risk management, conservation planning, and more. while bottom-up,\\nsurvey driven censuses can provide a comprehensive view into the population\\nlandscape of a country, they are expensive to realize, are infrequently\\nperformed, and only provide population counts over broad areas. population\\ndisaggregation techniques and population projection methods individually\\naddress these shortcomings, but also have shortcomings of their own. to jointly\\nanswer the questions of \"where do people live\" and \"how many people live\\nthere,\" we propose a deep learning model for creating high-resolution\\npopulation estimations from satellite imagery. specifically, we train\\nconvolutional neural networks to predict population in the usa at a\\n$0.01^{\\\\circ} \\\\times 0.01^{\\\\circ}$ resolution grid from 1-year composite\\nlandsat imagery. we validate these models in two ways: quantitatively, by\\ncomparing our model\\'s grid cell estimates aggregated at a county-level to\\nseveral us census county-level population projections, and qualitatively, by\\ndirectly interpreting the model\\'s predictions in terms of the satellite image\\ninputs. we find that aggregating our model\\'s estimates gives comparable results\\nto the census county-level population projections and that the predictions made\\nby our model can be directly interpreted, which give it advantages over\\ntraditional population disaggregation methods. in general, our model is an\\nexample of how machine learning techniques can be an effective tool for\\nextracting information from inherently unstructured, remotely sensed data to\\nprovide effective solutions to social problems.\\n)': False, 'contains(machine learning for the geosciences: challenges and opportunities\\n  geosciences is a field of great societal relevance that requires solutions to\\nseveral urgent problems facing our humanity and the planet. as geosciences\\nenters the era of big data, machine learning (ml) -- that has been widely\\nsuccessful in commercial domains -- offers immense potential to contribute to\\nproblems in geosciences. however, problems in geosciences have several unique\\nchallenges that are seldom found in traditional applications, requiring novel\\nproblem formulations and methodologies in machine learning. this article\\nintroduces researchers in the machine learning (ml) community to these\\nchallenges offered by geoscience problems and the opportunities that exist for\\nadvancing both machine learning and geosciences. we first highlight typical\\nsources of geoscience data and describe their properties that make it\\nchallenging to use traditional machine learning techniques. we then describe\\nsome of the common categories of geoscience problems where machine learning can\\nplay a role, and discuss some of the existing efforts and promising directions\\nfor methodological development in machine learning. we conclude by discussing\\nsome of the emerging research themes in machine learning that are applicable\\nacross all problems in the geosciences, and the importance of a deep\\ncollaboration between machine learning and geosciences for synergistic\\nadvancements in both disciplines.\\n)': False, 'contains(estimating chlorophyll a concentrations of several inland waters with\\n  hyperspectral data and machine learning models\\n  water is a key component of life, the natural environment and human health.\\nfor monitoring the conditions of a water body, the chlorophyll a concentration\\ncan serve as a proxy for nutrients and oxygen supply. in situ measurements of\\nwater quality parameters are often time-consuming, expensive and limited in\\nareal validity. therefore, we apply remote sensing techniques. during field\\ncampaigns, we collected hyperspectral data with a spectrometer and in situ\\nmeasured chlorophyll a concentrations of 13 inland water bodies with different\\nspectral characteristics. one objective of this study is to estimate\\nchlorophyll a concentrations of these inland waters by applying three machine\\nlearning regression models: random forest, support vector machine and an\\nartificial neural network. additionally, we simulate four different\\nhyperspectral resolutions of the spectrometer data to investigate the effects\\non the estimation performance. furthermore, the application of first order\\nderivatives of the spectra is evaluated in turn to the regression performance.\\nthis study reveals the potential of combining machine learning approaches and\\nremote sensing data for inland waters. each machine learning model achieves an\\nr2-score between 80 % to 90 % for the regression on chlorophyll a\\nconcentrations. the random forest model benefits clearly from the applied\\nderivatives of the spectra. in further studies, we will focus on the\\napplication of machine learning models on spectral satellite data to enhance\\nthe area-wide estimation of chlorophyll a concentration for inland waters.\\n)': False, 'contains(convolutional lstm network: a machine learning approach for\\n  precipitation nowcasting\\n  the goal of precipitation nowcasting is to predict the future rainfall\\nintensity in a local region over a relatively short period of time. very few\\nprevious studies have examined this crucial and challenging weather forecasting\\nproblem from the machine learning perspective. in this paper, we formulate\\nprecipitation nowcasting as a spatiotemporal sequence forecasting problem in\\nwhich both the input and the prediction target are spatiotemporal sequences. by\\nextending the fully connected lstm (fc-lstm) to have convolutional structures\\nin both the input-to-state and state-to-state transitions, we propose the\\nconvolutional lstm (convlstm) and use it to build an end-to-end trainable model\\nfor the precipitation nowcasting problem. experiments show that our convlstm\\nnetwork captures spatiotemporal correlations better and consistently\\noutperforms fc-lstm and the state-of-the-art operational rover algorithm for\\nprecipitation nowcasting.\\n)': False, 'contains(complex-valued neural networks for machine learning on non-stationary\\n  physical data\\n  deep learning has become an area of interest in most scientific areas,\\nincluding physical sciences. modern networks apply real-valued transformations\\non the data. particularly, convolutions in convolutional neural networks\\ndiscard phase information entirely. many deterministic signals, such as seismic\\ndata or electrical signals, contain significant information in the phase of the\\nsignal. we explore complex-valued deep convolutional networks to leverage\\nnon-linear feature maps. seismic data commonly has a lowcut filter applied, to\\nattenuate noise from ocean waves and similar long wavelength contributions.\\ndiscarding the phase information leads to low-frequency aliasing analogous to\\nthe nyquist-shannon theorem for high frequencies. in non-stationary data, the\\nphase content can stabilize training and improve the generalizability of neural\\nnetworks. while it has been shown that phase content can be restored in deep\\nneural networks, we show how including phase information in feature maps\\nimproves both training and inference from deterministic physical data.\\nfurthermore, we show that the reduction of parameters in a complex network\\nresults in training on a smaller dataset without overfitting, in comparison to\\na real-valued network with the same performance.\\n)': False}\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in documents)\n",
    "word_features = list(all_words)[:1000]\n",
    "print(document_features(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
